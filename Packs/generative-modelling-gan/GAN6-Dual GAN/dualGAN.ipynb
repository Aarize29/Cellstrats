{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aG8au6SZ2GNN",
   "metadata": {
    "id": "aG8au6SZ2GNN"
   },
   "source": [
    "# CellStrat Hub Pack - Generative Adversarial Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8uryVs4825ls",
   "metadata": {
    "id": "8uryVs4825ls"
   },
   "source": [
    "#### GAN6 - Unsupervised Dual Learning for Image-to-Image Translation(dualGAN)\n",
    "\n",
    "Note: use Python 3 kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded31892-1ff0-4abd-96eb-849efa39f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# If any library needs to be installed, install with following command :-\n",
    "# pip install <library-name>\n",
    "# This pip command should be in an independent cell with no other code or comments in this cell.\n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb1424-1899-45f4-802b-160a8726d9b7",
   "metadata": {},
   "source": [
    "##### Install all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60215a79-29c9-49e5-a81d-386c9e0b235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git (from -r requirements.txt (line 2))\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-6v7cokd7\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-6v7cokd7\n",
      "Requirement already satisfied: keras in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.3.4)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.6.2)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (8.2.0)\n",
      "Requirement already satisfied: scikit-image in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 3)) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 9)) (2.5)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 9)) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 9)) (2020.10.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from scikit-image->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from networkx>=2.0->scikit-image->-r requirements.txt (line 9)) (5.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501d9f57-c7f6-4d4b-a218-0a4f579f1fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[K     |███████████████████████████     | 385.2 MB 84.4 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 458.4 MB 20 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.0-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 92.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 37.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: keras~=2.6 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 70.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 79.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.18.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 60.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 35.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=280f89a6c8fb7009458b8dd0c65f6604fa5d31b6b139efdb1083ae14dbaf732c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=610ad5cc11dafa41e5f8177d5e87b944882380bba9ee7ffb3cf8e82bbd671c7c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.14.0 astunparse-1.6.3 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.0 h5py-3.1.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.18.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b465-4ac6-409e-894e-05f6f30b095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-01 09:06:32.978093: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-01 09:06:32.978128: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-10-01 09:06:34.723502: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-01 09:06:34.723538: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-01 09:06:34.723563: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-3-5): /proc/driver/nvidia/version does not exist\n",
      "2021-10-01 09:06:34.723792: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-01 09:06:40.686874: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "0 [D1 loss: 0.000080] [D2 loss: 0.000082] [G loss: 194.399155]\n",
      "1 [D1 loss: 0.000093] [D2 loss: 0.000091] [G loss: 194.985077]\n",
      "2 [D1 loss: 0.000095] [D2 loss: 0.000092] [G loss: 194.253860]\n",
      "3 [D1 loss: 0.000094] [D2 loss: 0.000090] [G loss: 193.489868]\n",
      "4 [D1 loss: 0.000092] [D2 loss: 0.000086] [G loss: 193.290604]\n",
      "5 [D1 loss: 0.000088] [D2 loss: 0.000082] [G loss: 193.882431]\n",
      "6 [D1 loss: 0.000084] [D2 loss: 0.000077] [G loss: 192.701843]\n",
      "2021-10-01 09:06:47.803317: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "7 [D1 loss: 0.000079] [D2 loss: 0.000072] [G loss: 192.852692]\n",
      "8 [D1 loss: 0.000075] [D2 loss: 0.000068] [G loss: 191.460938]\n",
      "9 [D1 loss: 0.000071] [D2 loss: 0.000064] [G loss: 191.220078]\n",
      "10 [D1 loss: 0.000067] [D2 loss: 0.000060] [G loss: 191.191559]\n",
      "11 [D1 loss: 0.000064] [D2 loss: 0.000057] [G loss: 190.429535]\n",
      "12 [D1 loss: 0.000060] [D2 loss: 0.000055] [G loss: 190.872574]\n",
      "13 [D1 loss: 0.000058] [D2 loss: 0.000053] [G loss: 190.144028]\n",
      "14 [D1 loss: 0.000055] [D2 loss: 0.000051] [G loss: 190.463272]\n",
      "15 [D1 loss: 0.000053] [D2 loss: 0.000050] [G loss: 189.393433]\n",
      "16 [D1 loss: 0.000052] [D2 loss: 0.000049] [G loss: 189.065735]\n",
      "17 [D1 loss: 0.000050] [D2 loss: 0.000048] [G loss: 188.782867]\n",
      "18 [D1 loss: 0.000049] [D2 loss: 0.000048] [G loss: 187.398071]\n",
      "19 [D1 loss: 0.000049] [D2 loss: 0.000047] [G loss: 188.059555]\n",
      "20 [D1 loss: 0.000048] [D2 loss: 0.000047] [G loss: 186.517715]\n",
      "21 [D1 loss: 0.000047] [D2 loss: 0.000047] [G loss: 185.910736]\n",
      "22 [D1 loss: 0.000047] [D2 loss: 0.000046] [G loss: 186.039230]\n",
      "23 [D1 loss: 0.000047] [D2 loss: 0.000046] [G loss: 183.938629]\n",
      "24 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 184.513245]\n",
      "25 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 184.097565]\n",
      "26 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 182.151016]\n",
      "27 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 182.939453]\n",
      "28 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 181.021851]\n",
      "29 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 179.598541]\n",
      "30 [D1 loss: 0.000046] [D2 loss: 0.000046] [G loss: 180.465775]\n",
      "31 [D1 loss: 0.000045] [D2 loss: 0.000046] [G loss: 178.322678]\n",
      "32 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 176.851959]\n",
      "33 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 177.033966]\n",
      "34 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 172.707031]\n",
      "35 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 174.434906]\n",
      "36 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 174.995972]\n",
      "37 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 173.423203]\n",
      "38 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 169.036957]\n",
      "39 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 164.818665]\n",
      "40 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 165.271851]\n",
      "41 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 164.942459]\n",
      "42 [D1 loss: 0.000045] [D2 loss: 0.000045] [G loss: 161.261169]\n",
      "43 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 162.613647]\n",
      "44 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 156.802734]\n",
      "45 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 157.837753]\n",
      "46 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 156.061874]\n",
      "47 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 154.728516]\n",
      "48 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 153.682632]\n",
      "49 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 148.409393]\n",
      "50 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 146.818497]\n",
      "51 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 148.375214]\n",
      "52 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 143.832092]\n",
      "53 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 139.655640]\n",
      "54 [D1 loss: 0.000044] [D2 loss: 0.000044] [G loss: 135.997406]\n",
      "55 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 134.547516]\n",
      "56 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 132.346558]\n",
      "57 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 128.380966]\n",
      "58 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 129.439240]\n",
      "59 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 130.031876]\n",
      "60 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 128.610123]\n",
      "61 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 121.402847]\n",
      "62 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 119.454552]\n",
      "63 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 118.565773]\n",
      "64 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 117.161942]\n",
      "65 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 120.431015]\n",
      "66 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 118.983223]\n",
      "67 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 117.796669]\n",
      "68 [D1 loss: 0.000043] [D2 loss: 0.000043] [G loss: 115.487144]\n",
      "69 [D1 loss: 0.000042] [D2 loss: 0.000043] [G loss: 114.591522]\n",
      "70 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 121.898315]\n",
      "71 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 119.484306]\n",
      "72 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 114.144218]\n",
      "73 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 116.879028]\n",
      "74 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 107.678238]\n",
      "75 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 108.524773]\n",
      "76 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 116.701180]\n",
      "77 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 112.310020]\n",
      "78 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 94.807487]\n",
      "79 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 110.116951]\n",
      "80 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 97.172386]\n",
      "81 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 104.886604]\n",
      "82 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 113.315147]\n",
      "83 [D1 loss: 0.000042] [D2 loss: 0.000042] [G loss: 103.253258]\n",
      "84 [D1 loss: 0.000041] [D2 loss: 0.000042] [G loss: 99.835869]\n",
      "85 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 96.711296]\n",
      "86 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 100.299568]\n",
      "87 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 97.538109]\n",
      "88 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 95.420204]\n",
      "89 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 92.410698]\n",
      "90 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 90.908478]\n",
      "91 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 119.121391]\n",
      "92 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 87.785988]\n",
      "93 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 92.702797]\n",
      "94 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 101.104881]\n",
      "95 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 89.319839]\n",
      "96 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 100.730911]\n",
      "97 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 90.931396]\n",
      "98 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 99.468842]\n",
      "99 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 99.800247]\n",
      "100 [D1 loss: 0.000041] [D2 loss: 0.000041] [G loss: 93.873177]\n",
      "101 [D1 loss: 0.000040] [D2 loss: 0.000041] [G loss: 93.816895]\n",
      "102 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 89.414268]\n",
      "103 [D1 loss: 0.000041] [D2 loss: 0.000040] [G loss: 102.008514]\n",
      "104 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 91.882973]\n",
      "105 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 85.609970]\n",
      "106 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 91.547966]\n",
      "107 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 95.419464]\n",
      "108 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 87.658371]\n",
      "109 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 85.562798]\n",
      "110 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 87.217384]\n",
      "111 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 85.911476]\n",
      "112 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 86.951035]\n",
      "113 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 98.716904]\n",
      "114 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 84.197388]\n",
      "115 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 86.076843]\n",
      "116 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 81.098503]\n",
      "117 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 82.460922]\n",
      "118 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 102.740814]\n",
      "119 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 91.810760]\n",
      "120 [D1 loss: 0.000040] [D2 loss: 0.000040] [G loss: 83.941628]\n",
      "121 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.999100]\n",
      "122 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.697067]\n",
      "2021-10-01 09:07:38.927482: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "123 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 93.871399]\n",
      "124 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 89.534683]\n",
      "125 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 82.399384]\n",
      "126 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 86.507324]\n",
      "127 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 90.916847]\n",
      "128 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 81.095695]\n",
      "129 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 91.850014]\n",
      "130 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 86.353973]\n",
      "131 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 82.927658]\n",
      "132 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 81.679337]\n",
      "133 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.424332]\n",
      "134 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 95.192383]\n",
      "135 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 84.046951]\n",
      "136 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.501595]\n",
      "137 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 90.958221]\n",
      "138 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 90.552048]\n",
      "139 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.452782]\n",
      "140 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 87.921051]\n",
      "141 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.521576]\n",
      "142 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 83.274155]\n",
      "143 [D1 loss: 0.000039] [D2 loss: 0.000039] [G loss: 74.519409]\n",
      "144 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.412064]\n",
      "145 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 83.380119]\n",
      "146 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.748795]\n",
      "147 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 74.086655]\n",
      "148 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 80.232910]\n",
      "149 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 83.866592]\n",
      "150 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 79.153214]\n",
      "151 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 80.563454]\n",
      "152 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 83.300797]\n",
      "153 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 69.319527]\n",
      "154 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 74.041267]\n",
      "155 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 83.712158]\n",
      "156 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 77.335052]\n",
      "157 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 85.285736]\n",
      "158 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.016205]\n",
      "159 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 71.807953]\n",
      "160 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 92.020432]\n",
      "161 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 76.478195]\n",
      "162 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 86.414558]\n",
      "163 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 76.151535]\n",
      "164 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.727364]\n",
      "165 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 79.805237]\n",
      "166 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 77.714142]\n",
      "167 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.707497]\n",
      "168 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 81.572227]\n",
      "169 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 73.769028]\n",
      "170 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 74.709679]\n",
      "171 [D1 loss: 0.000038] [D2 loss: 0.000038] [G loss: 75.610069]\n",
      "172 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 82.964836]\n",
      "173 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 82.551025]\n",
      "174 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 73.796967]\n",
      "175 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 78.665138]\n",
      "176 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 84.677414]\n",
      "177 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 70.120323]\n",
      "178 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 80.082794]\n",
      "179 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 72.937103]\n",
      "180 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 79.151024]\n",
      "181 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.530540]\n",
      "182 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 73.792870]\n",
      "183 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 76.076424]\n",
      "184 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 71.598633]\n",
      "185 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 75.801582]\n",
      "186 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 70.019165]\n",
      "187 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 71.855537]\n",
      "188 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 67.168472]\n",
      "189 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 82.845940]\n",
      "190 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 74.701027]\n",
      "191 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 79.024017]\n",
      "192 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.526329]\n",
      "193 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 74.870842]\n",
      "194 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 80.807121]\n",
      "195 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 75.903481]\n",
      "196 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 80.443588]\n",
      "197 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 77.135353]\n",
      "198 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 74.335487]\n",
      "199 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 71.119759]\n",
      "200 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 68.282684]\n",
      "201 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 72.563377]\n",
      "202 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 68.973717]\n",
      "203 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 70.323814]\n",
      "204 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 67.018044]\n",
      "205 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 74.372078]\n",
      "206 [D1 loss: 0.000037] [D2 loss: 0.000037] [G loss: 69.136093]\n",
      "207 [D1 loss: 0.000036] [D2 loss: 0.000037] [G loss: 73.892143]\n",
      "208 [D1 loss: 0.000036] [D2 loss: 0.000037] [G loss: 69.206894]\n",
      "209 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 70.632202]\n",
      "210 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 69.388977]\n",
      "211 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 72.950729]\n",
      "212 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.730110]\n",
      "213 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 70.149887]\n",
      "214 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 83.967346]\n",
      "215 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.128113]\n",
      "216 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.954285]\n",
      "217 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 77.852692]\n",
      "218 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.585121]\n",
      "219 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 75.222588]\n",
      "220 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.568672]\n",
      "221 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 69.646828]\n",
      "222 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.529251]\n",
      "223 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 72.789383]\n",
      "224 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 62.848770]\n",
      "225 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.670769]\n",
      "226 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 72.411415]\n",
      "227 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 59.039406]\n",
      "228 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.981956]\n",
      "229 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 75.282997]\n",
      "230 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 79.286247]\n",
      "231 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 75.730675]\n",
      "232 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 65.576912]\n",
      "233 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 67.357903]\n",
      "234 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 68.176147]\n",
      "235 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 73.988525]\n",
      "236 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 63.975769]\n",
      "237 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 69.717964]\n",
      "238 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 70.852547]\n",
      "239 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 59.001865]\n",
      "240 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.077919]\n",
      "241 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 75.862823]\n",
      "242 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 78.150574]\n",
      "243 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.793304]\n",
      "244 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 65.516113]\n",
      "245 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 63.881863]\n",
      "246 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 64.099243]\n",
      "247 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 69.187798]\n",
      "248 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 65.696930]\n",
      "249 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 76.810074]\n",
      "250 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 71.029327]\n",
      "251 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 66.139626]\n",
      "252 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 65.107414]\n",
      "253 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 73.490334]\n",
      "254 [D1 loss: 0.000036] [D2 loss: 0.000036] [G loss: 62.837185]\n",
      "255 [D1 loss: 0.000035] [D2 loss: 0.000036] [G loss: 63.513424]\n",
      "256 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.316719]\n",
      "257 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.701981]\n",
      "258 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 70.665207]\n",
      "259 [D1 loss: 0.000036] [D2 loss: 0.000035] [G loss: 72.909012]\n",
      "260 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.192474]\n",
      "261 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.128220]\n",
      "262 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 69.378838]\n",
      "263 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 75.970078]\n",
      "264 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.519341]\n",
      "265 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 73.139610]\n",
      "266 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.599545]\n",
      "267 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.924179]\n",
      "268 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 74.606445]\n",
      "269 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.661552]\n",
      "270 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.594643]\n",
      "2021-10-01 09:08:44.797010: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "271 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.138931]\n",
      "272 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 76.727875]\n",
      "273 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 74.629974]\n",
      "274 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 72.285332]\n",
      "275 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.953491]\n",
      "276 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.407410]\n",
      "277 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.828522]\n",
      "278 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.367424]\n",
      "279 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 70.611870]\n",
      "280 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 71.016151]\n",
      "281 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 69.089157]\n",
      "282 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.971039]\n",
      "283 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.940712]\n",
      "284 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 74.100410]\n",
      "285 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.463661]\n",
      "286 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.145119]\n",
      "287 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.502987]\n",
      "288 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 67.050491]\n",
      "289 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 70.879105]\n",
      "290 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.546165]\n",
      "291 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.852776]\n",
      "2021-10-01 09:08:53.970560: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "292 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.664154]\n",
      "293 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 77.115372]\n",
      "294 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.364395]\n",
      "295 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.740799]\n",
      "296 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.440338]\n",
      "297 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 68.611420]\n",
      "298 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.115700]\n",
      "299 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.442894]\n",
      "300 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.181126]\n",
      "301 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 56.636944]\n",
      "302 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 55.947998]\n",
      "303 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.102707]\n",
      "304 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.071537]\n",
      "305 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 58.999836]\n",
      "306 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 71.586273]\n",
      "307 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.426918]\n",
      "308 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.107235]\n",
      "309 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.335575]\n",
      "310 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.344646]\n",
      "311 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.850674]\n",
      "312 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.459473]\n",
      "313 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.919254]\n",
      "314 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 54.170139]\n",
      "315 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.518490]\n",
      "2021-10-01 09:09:04.547981: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "316 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 55.737644]\n",
      "317 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 66.311592]\n",
      "318 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.714508]\n",
      "319 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.521740]\n",
      "320 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 64.690918]\n",
      "321 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.911377]\n",
      "322 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.000309]\n",
      "323 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 60.028259]\n",
      "324 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 58.580906]\n",
      "325 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 63.938435]\n",
      "326 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.433189]\n",
      "327 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 62.754639]\n",
      "328 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 59.010509]\n",
      "329 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 56.301201]\n",
      "330 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 58.748661]\n",
      "331 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 57.086838]\n",
      "332 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 61.155167]\n",
      "333 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 65.841301]\n",
      "334 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 59.494152]\n",
      "335 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.902710]\n",
      "336 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.464893]\n",
      "337 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 68.186127]\n",
      "338 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 59.834156]\n",
      "339 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 59.152912]\n",
      "340 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.103565]\n",
      "341 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 60.697823]\n",
      "342 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.558807]\n",
      "343 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 60.414581]\n",
      "344 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 65.052353]\n",
      "345 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 57.739853]\n",
      "346 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.849632]\n",
      "347 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.730030]\n",
      "348 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.671551]\n",
      "349 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.554806]\n",
      "350 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 53.861652]\n",
      "351 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 64.177597]\n",
      "352 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 52.390953]\n",
      "353 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 60.765362]\n",
      "354 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.430130]\n",
      "355 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.157906]\n",
      "356 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.969841]\n",
      "357 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.576996]\n",
      "358 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.556751]\n",
      "359 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.690742]\n",
      "360 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.311970]\n",
      "361 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.116894]\n",
      "362 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.537785]\n",
      "363 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.884007]\n",
      "364 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.104156]\n",
      "365 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.386280]\n",
      "366 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.009644]\n",
      "367 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.204876]\n",
      "368 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.130253]\n",
      "369 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.652557]\n",
      "370 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.624489]\n",
      "371 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.799557]\n",
      "2021-10-01 09:09:29.780869: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "372 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 64.564613]\n",
      "2021-10-01 09:09:30.335951: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:30.473868: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "373 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.797363]\n",
      "2021-10-01 09:09:30.724480: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:30.736591: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:30.747457: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:30.862508: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:30.873786: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:30.992399: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:31.166465: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:31.189659: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "374 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.628551]\n",
      "2021-10-01 09:09:31.526786: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:31.689263: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:31.828595: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:31.843481: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:31.869264: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "375 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.672760]\n",
      "2021-10-01 09:09:32.227720: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:32.244511: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:32.384954: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "376 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.268246]\n",
      "2021-10-01 09:09:32.826847: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:09:32.857267: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "377 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.752132]\n",
      "378 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.027515]\n",
      "379 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 68.759262]\n",
      "380 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.021202]\n",
      "381 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.241650]\n",
      "382 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.893993]\n",
      "383 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.343143]\n",
      "384 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.966637]\n",
      "385 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.476589]\n",
      "386 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.002068]\n",
      "387 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.348015]\n",
      "388 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.758110]\n",
      "389 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.716331]\n",
      "390 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 65.938652]\n",
      "391 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.019653]\n",
      "392 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.376842]\n",
      "393 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.486156]\n",
      "394 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.380630]\n",
      "395 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.117672]\n",
      "396 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.021236]\n",
      "397 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.410564]\n",
      "398 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.517323]\n",
      "399 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.173794]\n",
      "400 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.564041]\n",
      "401 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.794506]\n",
      "402 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.014008]\n",
      "403 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.728550]\n",
      "404 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.746403]\n",
      "405 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.430851]\n",
      "406 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.897774]\n",
      "407 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.221584]\n",
      "408 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.193203]\n",
      "409 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.056545]\n",
      "410 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.386124]\n",
      "411 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.519062]\n",
      "412 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.640152]\n",
      "413 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.171177]\n",
      "414 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.986286]\n",
      "415 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.467960]\n",
      "416 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.132935]\n",
      "417 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.170998]\n",
      "418 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.802612]\n",
      "419 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 66.765396]\n",
      "420 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.445847]\n",
      "421 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.886211]\n",
      "422 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.577164]\n",
      "423 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.057484]\n",
      "424 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.058994]\n",
      "425 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.564777]\n",
      "426 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.560040]\n",
      "427 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.636097]\n",
      "428 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.628105]\n",
      "429 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.979946]\n",
      "430 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.389435]\n",
      "431 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.272327]\n",
      "432 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.611687]\n",
      "433 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.949696]\n",
      "434 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.379303]\n",
      "435 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.843151]\n",
      "436 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.144588]\n",
      "437 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.726902]\n",
      "438 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.604431]\n",
      "439 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.003799]\n",
      "440 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.149349]\n",
      "2021-10-01 09:10:01.667794: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:01.707118: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:01.821874: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:01.839060: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:01.853686: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:01.867722: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:01.998198: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.028741: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.042339: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "441 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.730808]\n",
      "2021-10-01 09:10:02.252640: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.267041: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.294348: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.424550: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.437948: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.452342: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.472138: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.586727: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.600494: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.614522: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.628467: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.763139: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:02.791422: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "442 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.648331]\n",
      "2021-10-01 09:10:03.257456: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.272368: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.286503: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.403140: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.420329: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.434260: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.447819: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.599599: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.617711: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "443 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.021984]\n",
      "2021-10-01 09:10:03.862985: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.878449: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:03.892637: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:04.038312: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:04.053235: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:04.186050: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:10:04.341091: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "444 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.892735]\n",
      "445 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.844959]\n",
      "446 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.957802]\n",
      "447 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.195583]\n",
      "448 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.049282]\n",
      "449 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.788280]\n",
      "450 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.070606]\n",
      "451 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.084587]\n",
      "452 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.747013]\n",
      "453 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.472240]\n",
      "454 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.139988]\n",
      "455 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.024834]\n",
      "456 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.193626]\n",
      "457 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.121780]\n",
      "458 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.095032]\n",
      "459 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.751282]\n",
      "460 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.038406]\n",
      "461 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.287792]\n",
      "462 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.620232]\n",
      "463 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.065674]\n",
      "464 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.036392]\n",
      "465 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.772255]\n",
      "466 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.641598]\n",
      "467 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.403492]\n",
      "468 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.120380]\n",
      "469 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.762936]\n",
      "470 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.497711]\n",
      "471 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.924633]\n",
      "472 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.324474]\n",
      "473 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.646904]\n",
      "474 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.398045]\n",
      "475 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.857838]\n",
      "476 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.967373]\n",
      "477 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.747993]\n",
      "478 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.415707]\n",
      "479 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.694130]\n",
      "480 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.740509]\n",
      "481 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.487797]\n",
      "482 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.497791]\n",
      "483 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.434155]\n",
      "484 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.380455]\n",
      "485 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.153973]\n",
      "486 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.191502]\n",
      "487 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.269619]\n",
      "488 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.737480]\n",
      "489 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.161472]\n",
      "490 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.823692]\n",
      "491 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.796074]\n",
      "492 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.111580]\n",
      "493 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.377373]\n",
      "494 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.829784]\n",
      "495 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.181824]\n",
      "496 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.888287]\n",
      "497 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.187889]\n",
      "498 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.513813]\n",
      "499 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.258957]\n",
      "500 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.100662]\n",
      "501 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.601654]\n",
      "502 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.768738]\n",
      "503 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.630791]\n",
      "504 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.176353]\n",
      "505 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.296009]\n",
      "506 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.918377]\n",
      "507 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.494320]\n",
      "508 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.421726]\n",
      "509 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.952225]\n",
      "510 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.198654]\n",
      "511 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.987221]\n",
      "512 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.672043]\n",
      "513 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.254044]\n",
      "514 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.232456]\n",
      "515 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.687325]\n",
      "516 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.723545]\n",
      "517 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.416191]\n",
      "518 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.787083]\n",
      "519 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.817108]\n",
      "520 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.286671]\n",
      "521 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.355434]\n",
      "522 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.277699]\n",
      "523 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.149715]\n",
      "524 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.433804]\n",
      "525 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.630707]\n",
      "526 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.562054]\n",
      "527 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.416908]\n",
      "528 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.180817]\n",
      "529 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.562923]\n",
      "530 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.481117]\n",
      "531 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.699341]\n",
      "532 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.437210]\n",
      "533 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.721382]\n",
      "534 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.075550]\n",
      "535 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.654041]\n",
      "536 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.389664]\n",
      "537 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.168537]\n",
      "538 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.956882]\n",
      "539 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.243046]\n",
      "540 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.503674]\n",
      "541 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.467899]\n",
      "542 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.904011]\n",
      "543 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.792278]\n",
      "544 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.056648]\n",
      "545 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.408379]\n",
      "546 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.080006]\n",
      "547 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.972565]\n",
      "548 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.594990]\n",
      "549 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.425789]\n",
      "550 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.472054]\n",
      "551 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.378025]\n",
      "552 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.801987]\n",
      "553 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.544052]\n",
      "554 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.707039]\n",
      "555 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.621803]\n",
      "556 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.638393]\n",
      "557 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.246754]\n",
      "558 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.194965]\n",
      "559 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.508099]\n",
      "560 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.878914]\n",
      "561 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.303989]\n",
      "562 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.601780]\n",
      "563 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.632538]\n",
      "564 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.080608]\n",
      "565 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.027115]\n",
      "566 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 65.353523]\n",
      "567 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.558083]\n",
      "568 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.220436]\n",
      "569 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.246841]\n",
      "570 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.225788]\n",
      "571 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.157875]\n",
      "572 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.031368]\n",
      "573 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.835243]\n",
      "574 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.403061]\n",
      "575 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.801579]\n",
      "576 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.179722]\n",
      "577 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.818241]\n",
      "578 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.192116]\n",
      "579 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.515003]\n",
      "580 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.405827]\n",
      "581 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.586868]\n",
      "582 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.363689]\n",
      "583 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.067257]\n",
      "584 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.088676]\n",
      "585 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.150536]\n",
      "586 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.365341]\n",
      "587 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.715652]\n",
      "588 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.890781]\n",
      "589 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.721462]\n",
      "590 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.007866]\n",
      "591 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.224468]\n",
      "592 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.978127]\n",
      "593 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.430599]\n",
      "594 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.994652]\n",
      "595 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.334229]\n",
      "596 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.921715]\n",
      "2021-10-01 09:11:12.419928: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "597 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.285252]\n",
      "598 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.395458]\n",
      "599 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.583099]\n",
      "600 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.122463]\n",
      "601 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.973030]\n",
      "602 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.998150]\n",
      "603 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.063026]\n",
      "604 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.553444]\n",
      "605 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.913429]\n",
      "606 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.538021]\n",
      "607 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.108028]\n",
      "608 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.587357]\n",
      "609 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.079586]\n",
      "610 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.583561]\n",
      "611 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.755741]\n",
      "612 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.608089]\n",
      "613 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.335716]\n",
      "614 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.251804]\n",
      "615 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.335545]\n",
      "616 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.124062]\n",
      "617 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.694935]\n",
      "618 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.021954]\n",
      "619 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.178459]\n",
      "620 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.946529]\n",
      "621 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.488300]\n",
      "622 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.165909]\n",
      "623 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 61.931557]\n",
      "624 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.607285]\n",
      "625 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.261921]\n",
      "626 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.713989]\n",
      "627 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.675640]\n",
      "628 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.707603]\n",
      "629 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.161201]\n",
      "630 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.175690]\n",
      "631 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.890202]\n",
      "632 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.432285]\n",
      "633 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.051701]\n",
      "634 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.876457]\n",
      "635 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.919102]\n",
      "636 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.520176]\n",
      "637 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.325855]\n",
      "638 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.959919]\n",
      "639 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.331993]\n",
      "640 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.194103]\n",
      "641 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.269222]\n",
      "642 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.525276]\n",
      "643 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.285252]\n",
      "644 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.638279]\n",
      "645 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.424400]\n",
      "646 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.826172]\n",
      "647 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.905209]\n",
      "648 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.693913]\n",
      "649 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.313427]\n",
      "650 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.885361]\n",
      "651 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.595509]\n",
      "652 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.217682]\n",
      "653 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.636658]\n",
      "654 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.993176]\n",
      "655 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.174129]\n",
      "656 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.215572]\n",
      "657 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.289700]\n",
      "658 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.753723]\n",
      "659 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.563740]\n",
      "660 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.854561]\n",
      "661 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.498413]\n",
      "662 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.335838]\n",
      "663 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.066975]\n",
      "664 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.216660]\n",
      "665 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.077682]\n",
      "666 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.459503]\n",
      "667 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.957165]\n",
      "668 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.234196]\n",
      "669 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.535942]\n",
      "670 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.935833]\n",
      "671 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.368477]\n",
      "672 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.997746]\n",
      "673 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.113728]\n",
      "674 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.772125]\n",
      "675 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.050182]\n",
      "676 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.830158]\n",
      "677 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.710239]\n",
      "678 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.279808]\n",
      "679 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.527107]\n",
      "680 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.286160]\n",
      "681 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.961288]\n",
      "682 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.721447]\n",
      "683 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.567619]\n",
      "684 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.863880]\n",
      "685 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.928829]\n",
      "686 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.121040]\n",
      "687 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.528664]\n",
      "688 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.997078]\n",
      "689 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.705475]\n",
      "690 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.937923]\n",
      "691 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.125561]\n",
      "692 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.372055]\n",
      "693 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.651085]\n",
      "694 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 63.398880]\n",
      "695 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.553043]\n",
      "696 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.910267]\n",
      "697 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.896877]\n",
      "698 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.484993]\n",
      "699 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.387093]\n",
      "700 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.557320]\n",
      "701 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.167805]\n",
      "702 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.673065]\n",
      "703 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 60.260864]\n",
      "704 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.930260]\n",
      "705 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.727589]\n",
      "706 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.180363]\n",
      "707 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.584007]\n",
      "708 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.748093]\n",
      "709 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.315701]\n",
      "710 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.940575]\n",
      "711 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.491158]\n",
      "712 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.811806]\n",
      "713 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.713104]\n",
      "714 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.754059]\n",
      "715 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.695061]\n",
      "716 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.689362]\n",
      "717 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.366199]\n",
      "718 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.124928]\n",
      "719 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.378502]\n",
      "720 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.798569]\n",
      "721 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.517586]\n",
      "722 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.312836]\n",
      "723 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.287258]\n",
      "724 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.342003]\n",
      "725 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.405045]\n",
      "726 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.442818]\n",
      "727 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.626827]\n",
      "728 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.333359]\n",
      "729 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.742916]\n",
      "730 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.943172]\n",
      "731 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.889160]\n",
      "732 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.135426]\n",
      "733 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.747330]\n",
      "734 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.500389]\n",
      "735 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.268314]\n",
      "736 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.455002]\n",
      "737 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.377132]\n",
      "2021-10-01 09:12:15.150573: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "738 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.369011]\n",
      "739 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.839108]\n",
      "740 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.837418]\n",
      "741 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.182037]\n",
      "742 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.420696]\n",
      "743 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.395874]\n",
      "744 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.800922]\n",
      "745 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.481266]\n",
      "746 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.730618]\n",
      "747 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.453636]\n",
      "748 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.828308]\n",
      "749 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.932434]\n",
      "750 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.575287]\n",
      "751 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 57.466026]\n",
      "752 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.955399]\n",
      "753 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 62.383564]\n",
      "754 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.317684]\n",
      "755 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.321613]\n",
      "756 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.916901]\n",
      "757 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.938812]\n",
      "758 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.869579]\n",
      "759 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.348454]\n",
      "760 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.546036]\n",
      "761 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.250008]\n",
      "762 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.727833]\n",
      "763 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.459927]\n",
      "764 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.814575]\n",
      "765 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.104950]\n",
      "766 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.929173]\n",
      "2021-10-01 09:12:28.324084: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:12:28.335016: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "767 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.090775]\n",
      "768 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.110859]\n",
      "769 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.767620]\n",
      "770 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.698204]\n",
      "771 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.868073]\n",
      "772 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.862633]\n",
      "773 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.869778]\n",
      "774 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.050293]\n",
      "775 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.216507]\n",
      "776 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.254204]\n",
      "777 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.671597]\n",
      "778 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.954456]\n",
      "779 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.045513]\n",
      "780 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.966007]\n",
      "781 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.276909]\n",
      "782 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.026272]\n",
      "783 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.725189]\n",
      "784 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.713631]\n",
      "785 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.384377]\n",
      "786 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.808552]\n",
      "787 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.725803]\n",
      "788 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.392929]\n",
      "789 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.310993]\n",
      "2021-10-01 09:12:38.529257: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "790 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.712620]\n",
      "791 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.802094]\n",
      "792 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.478817]\n",
      "793 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.625599]\n",
      "794 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.296204]\n",
      "795 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.672962]\n",
      "796 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.035282]\n",
      "797 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.349205]\n",
      "798 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.642109]\n",
      "799 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.287975]\n",
      "800 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.822559]\n",
      "801 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 58.175747]\n",
      "2021-10-01 09:12:44.308075: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "802 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.298904]\n",
      "803 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.058739]\n",
      "804 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.809994]\n",
      "805 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.364845]\n",
      "806 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.574139]\n",
      "807 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.841743]\n",
      "808 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.332359]\n",
      "809 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.061657]\n",
      "810 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.855442]\n",
      "811 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.575310]\n",
      "812 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.467415]\n",
      "813 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.087223]\n",
      "814 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.652298]\n",
      "815 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.536957]\n",
      "816 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 59.222984]\n",
      "817 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.122219]\n",
      "818 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.609211]\n",
      "819 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.257912]\n",
      "820 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.402374]\n",
      "821 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.181679]\n",
      "822 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.810909]\n",
      "823 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.544514]\n",
      "824 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.306377]\n",
      "825 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.611004]\n",
      "826 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.799377]\n",
      "827 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.304062]\n",
      "828 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.681881]\n",
      "829 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.017647]\n",
      "830 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.062099]\n",
      "831 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.359451]\n",
      "832 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.621059]\n",
      "833 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.794430]\n",
      "834 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.061707]\n",
      "835 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.617378]\n",
      "836 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.411030]\n",
      "837 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.781708]\n",
      "838 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.406513]\n",
      "839 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.654663]\n",
      "840 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.359531]\n",
      "841 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.790672]\n",
      "842 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.687412]\n",
      "843 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.046894]\n",
      "844 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.676563]\n",
      "845 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.110535]\n",
      "846 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.079342]\n",
      "847 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.667343]\n",
      "848 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.626896]\n",
      "2021-10-01 09:13:05.154551: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "849 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.290447]\n",
      "850 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.863293]\n",
      "851 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.262104]\n",
      "852 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.565636]\n",
      "853 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.166767]\n",
      "854 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.770336]\n",
      "855 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.843033]\n",
      "856 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.144642]\n",
      "857 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.301678]\n",
      "858 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.593559]\n",
      "859 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.119144]\n",
      "860 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.907864]\n",
      "861 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.480782]\n",
      "862 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.489243]\n",
      "863 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.956520]\n",
      "864 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.731064]\n",
      "865 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.949974]\n",
      "866 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.138840]\n",
      "867 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.971119]\n",
      "868 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.678421]\n",
      "869 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.792881]\n",
      "870 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.080887]\n",
      "871 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.147148]\n",
      "872 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.322144]\n",
      "873 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.930813]\n",
      "874 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.117775]\n",
      "875 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.570412]\n",
      "876 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.509502]\n",
      "877 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.909508]\n",
      "878 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.564053]\n",
      "879 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.541805]\n",
      "880 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.851036]\n",
      "881 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.051868]\n",
      "882 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.929390]\n",
      "883 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.561176]\n",
      "884 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.942066]\n",
      "885 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.430336]\n",
      "886 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.570354]\n",
      "887 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.337025]\n",
      "2021-10-01 09:13:22.587307: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "888 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.794731]\n",
      "889 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.113449]\n",
      "890 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.902184]\n",
      "891 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.832874]\n",
      "892 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.117558]\n",
      "893 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.064613]\n",
      "894 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.396820]\n",
      "895 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.771523]\n",
      "896 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.784546]\n",
      "897 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.493359]\n",
      "898 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.640766]\n",
      "899 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 51.097813]\n",
      "900 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.845436]\n",
      "901 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.668739]\n",
      "902 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.875450]\n",
      "903 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.065266]\n",
      "904 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.557796]\n",
      "905 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.637760]\n",
      "906 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.502064]\n",
      "907 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.629093]\n",
      "908 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.617451]\n",
      "909 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.408642]\n",
      "910 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.432034]\n",
      "911 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.780807]\n",
      "912 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 52.396431]\n",
      "913 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.018200]\n",
      "914 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.903831]\n",
      "915 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.643726]\n",
      "916 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.990719]\n",
      "917 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.134411]\n",
      "918 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.438053]\n",
      "919 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.400253]\n",
      "920 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.211079]\n",
      "921 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.417652]\n",
      "922 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.483143]\n",
      "923 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.081024]\n",
      "924 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.635784]\n",
      "925 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.380028]\n",
      "926 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.932915]\n",
      "927 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.370449]\n",
      "928 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.044109]\n",
      "929 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.153191]\n",
      "930 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.501816]\n",
      "931 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.940968]\n",
      "932 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.433460]\n",
      "933 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.965534]\n",
      "934 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.197876]\n",
      "935 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.717251]\n",
      "2021-10-01 09:13:44.030645: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "936 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 50.568714]\n",
      "937 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.230732]\n",
      "938 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.778599]\n",
      "939 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.725780]\n",
      "940 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.625267]\n",
      "941 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.550091]\n",
      "942 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.743492]\n",
      "943 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.824375]\n",
      "944 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 55.718903]\n",
      "945 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.717709]\n",
      "946 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.650097]\n",
      "947 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.728718]\n",
      "2021-10-01 09:13:49.376264: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "948 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.263287]\n",
      "949 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.116611]\n",
      "950 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.708210]\n",
      "951 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 51.041275]\n",
      "952 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.322720]\n",
      "953 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.508537]\n",
      "954 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.050671]\n",
      "955 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.869221]\n",
      "956 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.937675]\n",
      "957 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.440643]\n",
      "2021-10-01 09:13:53.623382: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "958 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.816460]\n",
      "959 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.496681]\n",
      "960 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.719795]\n",
      "961 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.012310]\n",
      "962 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.121506]\n",
      "963 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.351360]\n",
      "964 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.622650]\n",
      "965 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.773468]\n",
      "966 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.519016]\n",
      "967 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.474388]\n",
      "968 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.687351]\n",
      "969 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.509521]\n",
      "970 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.608768]\n",
      "971 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.362835]\n",
      "972 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.508286]\n",
      "973 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.862236]\n",
      "974 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.587257]\n",
      "975 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.841057]\n",
      "976 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.043682]\n",
      "977 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.961426]\n",
      "978 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.116531]\n",
      "979 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.968349]\n",
      "980 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.412399]\n",
      "981 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.744572]\n",
      "982 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.846710]\n",
      "983 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.647022]\n",
      "984 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.807632]\n",
      "985 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.379509]\n",
      "986 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.024624]\n",
      "987 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.405670]\n",
      "988 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.195225]\n",
      "989 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.255764]\n",
      "990 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.127937]\n",
      "991 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.443829]\n",
      "992 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.937016]\n",
      "993 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.758396]\n",
      "994 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.666042]\n",
      "995 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.041458]\n",
      "996 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.611366]\n",
      "997 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.283070]\n",
      "998 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.059582]\n",
      "999 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.177853]\n",
      "1000 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.600471]\n",
      "1001 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.733841]\n",
      "1002 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.821537]\n",
      "1003 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 57.965347]\n",
      "1004 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.142803]\n",
      "1005 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.546772]\n",
      "1006 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.452579]\n",
      "1007 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.523457]\n",
      "1008 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.280540]\n",
      "1009 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.353344]\n",
      "1010 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.083305]\n",
      "1011 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.887661]\n",
      "1012 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.134972]\n",
      "1013 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.296822]\n",
      "1014 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.424835]\n",
      "1015 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.724182]\n",
      "1016 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.051788]\n",
      "1017 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.326599]\n",
      "1018 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.885956]\n",
      "1019 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.727951]\n",
      "1020 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.938847]\n",
      "1021 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.388092]\n",
      "1022 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.012402]\n",
      "1023 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.068096]\n",
      "1024 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.011505]\n",
      "1025 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 56.835518]\n",
      "1026 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.560905]\n",
      "1027 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 53.764359]\n",
      "1028 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.333618]\n",
      "1029 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.711140]\n",
      "1030 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.202576]\n",
      "1031 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.897438]\n",
      "1032 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.839264]\n",
      "1033 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 52.564831]\n",
      "1034 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 51.575474]\n",
      "1035 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.045105]\n",
      "1036 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.040386]\n",
      "2021-10-01 09:14:29.557687: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:29.570040: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:29.582340: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1037 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.768475]\n",
      "2021-10-01 09:14:29.975769: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.005273: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.127606: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.141261: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.422566: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1038 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.176586]\n",
      "2021-10-01 09:14:30.681614: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.801316: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.830202: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.843223: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.960416: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:30.997269: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.133439: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.144604: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1039 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 52.977489]\n",
      "2021-10-01 09:14:31.345256: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.487469: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.647208: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.661266: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.689259: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:31.801747: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1040 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.414616]\n",
      "2021-10-01 09:14:32.048954: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:32.075908: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:32.088957: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:32.218953: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:32.355094: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:32.383889: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:14:32.529757: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1041 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.643452]\n",
      "1042 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.681206]\n",
      "1043 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.259140]\n",
      "1044 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.104446]\n",
      "1045 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 54.114941]\n",
      "1046 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.272007]\n",
      "1047 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.819508]\n",
      "1048 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.826382]\n",
      "1049 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.616917]\n",
      "2021-10-01 09:14:36.703253: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1050 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.609676]\n",
      "1051 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.952309]\n",
      "1052 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.916687]\n",
      "1053 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.655869]\n",
      "1054 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.875027]\n",
      "1055 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.272778]\n",
      "1056 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.194386]\n",
      "1057 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.131935]\n",
      "1058 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.751312]\n",
      "1059 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.898560]\n",
      "1060 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.924675]\n",
      "1061 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.122856]\n",
      "1062 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.719120]\n",
      "1063 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.534637]\n",
      "1064 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.783127]\n",
      "1065 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.378372]\n",
      "1066 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.889015]\n",
      "1067 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.232533]\n",
      "1068 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.551411]\n",
      "1069 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.285706]\n",
      "1070 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.935078]\n",
      "1071 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.140266]\n",
      "1072 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.905636]\n",
      "1073 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.845211]\n",
      "1074 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.734390]\n",
      "1075 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.337902]\n",
      "1076 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.312557]\n",
      "1077 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.824043]\n",
      "1078 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.334217]\n",
      "1079 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.628513]\n",
      "1080 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.425980]\n",
      "1081 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.321552]\n",
      "1082 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.424507]\n",
      "1083 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.012486]\n",
      "1084 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.345024]\n",
      "1085 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.076191]\n",
      "1086 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.000034]\n",
      "1087 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.473969]\n",
      "1088 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.397373]\n",
      "1089 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.156212]\n",
      "1090 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.863762]\n",
      "1091 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.492439]\n",
      "1092 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.960060]\n",
      "1093 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.944645]\n",
      "1094 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.156208]\n",
      "1095 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.990822]\n",
      "1096 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.274437]\n",
      "1097 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.380337]\n",
      "1098 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.382088]\n",
      "1099 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.356319]\n",
      "1100 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.340042]\n",
      "1101 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.256783]\n",
      "1102 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.914886]\n",
      "1103 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 44.836021]\n",
      "1104 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.493683]\n",
      "1105 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.332882]\n",
      "2021-10-01 09:15:01.477316: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.492784: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.508183: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.522109: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.643920: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.657887: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.671632: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.685589: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.832567: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.850790: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:01.865256: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1106 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.274879]\n",
      "2021-10-01 09:15:02.086490: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.104675: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.119052: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.133746: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.259959: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.289405: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.304561: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.441053: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.456447: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.475684: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.612708: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.630445: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:02.644579: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1107 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.775494]\n",
      "2021-10-01 09:15:02.971871: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.015949: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.133364: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.151150: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.165759: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.180689: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.308795: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.335951: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.350593: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.522741: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.538375: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1108 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.301273]\n",
      "2021-10-01 09:15:03.801155: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.833040: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:15:03.845881: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1109 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.763988]\n",
      "1110 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.398533]\n",
      "1111 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.601486]\n",
      "1112 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.534718]\n",
      "1113 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.428074]\n",
      "1114 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.450031]\n",
      "1115 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.681637]\n",
      "1116 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.113190]\n",
      "1117 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.521870]\n",
      "1118 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.436089]\n",
      "1119 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.077614]\n",
      "1120 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.772198]\n",
      "1121 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.694431]\n",
      "1122 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.656322]\n",
      "1123 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.485668]\n",
      "1124 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.618267]\n",
      "1125 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.617142]\n",
      "1126 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.666260]\n",
      "1127 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.622829]\n",
      "2021-10-01 09:15:12.710602: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1128 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.443790]\n",
      "1129 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.543831]\n",
      "1130 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.337017]\n",
      "1131 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.316517]\n",
      "1132 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 51.858932]\n",
      "1133 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 45.727371]\n",
      "1134 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.020462]\n",
      "1135 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.320801]\n",
      "1136 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 51.941338]\n",
      "1137 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.421535]\n",
      "1138 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.374146]\n",
      "1139 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.198826]\n",
      "1140 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.981743]\n",
      "1141 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 46.660053]\n",
      "1142 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 51.516113]\n",
      "1143 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.501213]\n",
      "1144 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.533276]\n",
      "1145 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.882519]\n",
      "1146 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.896286]\n",
      "1147 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.551731]\n",
      "1148 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.167469]\n",
      "1149 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.235756]\n",
      "1150 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.276394]\n",
      "1151 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 55.152893]\n",
      "1152 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.468529]\n",
      "1153 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.362972]\n",
      "1154 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.155365]\n",
      "1155 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.205570]\n",
      "1156 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.788486]\n",
      "1157 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.347687]\n",
      "1158 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.750839]\n",
      "1159 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.102173]\n",
      "1160 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.273438]\n",
      "1161 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.385582]\n",
      "1162 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.464928]\n",
      "1163 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.770626]\n",
      "1164 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.839294]\n",
      "1165 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.450069]\n",
      "1166 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.757450]\n",
      "1167 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.867809]\n",
      "1168 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.560314]\n",
      "1169 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 45.282593]\n",
      "1170 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.851860]\n",
      "1171 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.225613]\n",
      "1172 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.073818]\n",
      "1173 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.840183]\n",
      "1174 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.403687]\n",
      "1175 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.948551]\n",
      "1176 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.186829]\n",
      "1177 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.146206]\n",
      "1178 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.912338]\n",
      "1179 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.648090]\n",
      "1180 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.647251]\n",
      "1181 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.204136]\n",
      "1182 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.836105]\n",
      "1183 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.300110]\n",
      "1184 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.138866]\n",
      "1185 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.916374]\n",
      "1186 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.312939]\n",
      "1187 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.539703]\n",
      "1188 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.439922]\n",
      "1189 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.946854]\n",
      "1190 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.932625]\n",
      "1191 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.453903]\n",
      "1192 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.324970]\n",
      "1193 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.326630]\n",
      "1194 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.948288]\n",
      "1195 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.134052]\n",
      "1196 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.463600]\n",
      "1197 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.279507]\n",
      "1198 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.004436]\n",
      "1199 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.717102]\n",
      "1200 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.505684]\n",
      "1201 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.028416]\n",
      "1202 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.054634]\n",
      "1203 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 50.659676]\n",
      "1204 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.353683]\n",
      "1205 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.280231]\n",
      "1206 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.322464]\n",
      "1207 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.300289]\n",
      "1208 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.383900]\n",
      "1209 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.942650]\n",
      "1210 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.809273]\n",
      "1211 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.780338]\n",
      "1212 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.802834]\n",
      "1213 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.235992]\n",
      "1214 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.295773]\n",
      "1215 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.365486]\n",
      "1216 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.175564]\n",
      "1217 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.439587]\n",
      "1218 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.038803]\n",
      "1219 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.068771]\n",
      "1220 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.095627]\n",
      "1221 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.988163]\n",
      "1222 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.243916]\n",
      "1223 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.595009]\n",
      "1224 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.535213]\n",
      "1225 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.569771]\n",
      "1226 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.748993]\n",
      "1227 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 47.226669]\n",
      "1228 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.305458]\n",
      "1229 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.921955]\n",
      "1230 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.115696]\n",
      "1231 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.348972]\n",
      "1232 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.474918]\n",
      "1233 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.952095]\n",
      "1234 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 48.061222]\n",
      "1235 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.382984]\n",
      "1236 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.806511]\n",
      "1237 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.289780]\n",
      "1238 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.402428]\n",
      "1239 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.030807]\n",
      "1240 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.063591]\n",
      "1241 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.971554]\n",
      "1242 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.365791]\n",
      "1243 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.126629]\n",
      "1244 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 51.502075]\n",
      "1245 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.638443]\n",
      "1246 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.863285]\n",
      "1247 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.548153]\n",
      "1248 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.965023]\n",
      "1249 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.160847]\n",
      "1250 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.742378]\n",
      "1251 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.939560]\n",
      "1252 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.133118]\n",
      "1253 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.824230]\n",
      "1254 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.694778]\n",
      "1255 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.724548]\n",
      "1256 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.837303]\n",
      "1257 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.515518]\n",
      "1258 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.206955]\n",
      "1259 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.339630]\n",
      "1260 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.939682]\n",
      "1261 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.603680]\n",
      "1262 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 44.450615]\n",
      "1263 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 45.619919]\n",
      "1264 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.914764]\n",
      "1265 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.298321]\n",
      "1266 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.924797]\n",
      "1267 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.750526]\n",
      "1268 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.644226]\n",
      "1269 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.945923]\n",
      "1270 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.918011]\n",
      "1271 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.672188]\n",
      "1272 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.472008]\n",
      "1273 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.202354]\n",
      "1274 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.996613]\n",
      "1275 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.106350]\n",
      "1276 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.053307]\n",
      "1277 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.150150]\n",
      "1278 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.442944]\n",
      "1279 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.074368]\n",
      "1280 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.892292]\n",
      "1281 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 52.822338]\n",
      "1282 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.111053]\n",
      "1283 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 53.210323]\n",
      "1284 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.447479]\n",
      "1285 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.609028]\n",
      "1286 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.867912]\n",
      "1287 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.101131]\n",
      "1288 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.706760]\n",
      "1289 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.344620]\n",
      "1290 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.920265]\n",
      "2021-10-01 09:16:26.070944: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1291 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.844604]\n",
      "1292 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.327435]\n",
      "1293 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 47.259567]\n",
      "1294 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.072212]\n",
      "1295 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 45.636837]\n",
      "1296 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.433296]\n",
      "1297 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.568512]\n",
      "1298 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.537056]\n",
      "1299 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.238239]\n",
      "1300 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.669304]\n",
      "1301 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.862164]\n",
      "1302 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.579453]\n",
      "1303 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.875935]\n",
      "1304 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.499435]\n",
      "1305 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.757412]\n",
      "1306 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.158520]\n",
      "1307 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.389732]\n",
      "1308 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.522709]\n",
      "1309 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.883198]\n",
      "1310 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.377548]\n",
      "1311 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.225018]\n",
      "1312 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.360317]\n",
      "1313 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.896530]\n",
      "1314 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.816410]\n",
      "1315 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.954353]\n",
      "1316 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.407372]\n",
      "1317 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.362438]\n",
      "1318 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.799931]\n",
      "1319 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.788105]\n",
      "1320 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.228378]\n",
      "1321 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.909519]\n",
      "1322 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.573700]\n",
      "1323 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.154659]\n",
      "1324 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.559338]\n",
      "1325 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.238850]\n",
      "1326 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.751213]\n",
      "1327 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 46.413475]\n",
      "1328 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.426033]\n",
      "1329 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.852631]\n",
      "1330 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.676907]\n",
      "1331 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.329910]\n",
      "1332 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.298759]\n",
      "1333 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.792347]\n",
      "1334 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.066650]\n",
      "1335 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.530746]\n",
      "1336 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.470829]\n",
      "1337 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.259613]\n",
      "1338 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.975494]\n",
      "1339 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.829113]\n",
      "1340 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.792019]\n",
      "1341 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.071213]\n",
      "1342 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.006104]\n",
      "1343 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.975876]\n",
      "1344 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.726734]\n",
      "1345 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.269810]\n",
      "1346 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.758354]\n",
      "1347 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.479019]\n",
      "1348 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.530575]\n",
      "1349 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 51.145580]\n",
      "1350 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.384422]\n",
      "1351 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 56.422237]\n",
      "1352 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.799118]\n",
      "1353 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.516937]\n",
      "1354 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 45.478004]\n",
      "1355 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.461929]\n",
      "1356 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.481487]\n",
      "1357 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.417942]\n",
      "1358 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.555775]\n",
      "1359 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.865646]\n",
      "1360 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.974560]\n",
      "1361 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.612869]\n",
      "1362 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.058208]\n",
      "1363 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.382355]\n",
      "1364 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.540318]\n",
      "1365 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.786148]\n",
      "1366 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.107407]\n",
      "1367 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.812851]\n",
      "1368 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.742447]\n",
      "1369 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.956627]\n",
      "1370 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.540207]\n",
      "1371 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.520477]\n",
      "1372 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.444859]\n",
      "1373 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.032619]\n",
      "1374 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.355324]\n",
      "1375 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.138123]\n",
      "1376 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.251831]\n",
      "1377 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.638474]\n",
      "1378 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.639465]\n",
      "1379 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.200710]\n",
      "1380 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.068535]\n",
      "1381 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 39.829746]\n",
      "1382 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 49.461563]\n",
      "1383 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.718208]\n",
      "1384 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.896057]\n",
      "1385 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.611401]\n",
      "1386 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.601303]\n",
      "1387 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.016724]\n",
      "1388 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.511791]\n",
      "1389 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.486835]\n",
      "1390 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.873207]\n",
      "1391 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.471519]\n",
      "1392 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.455788]\n",
      "1393 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 47.126476]\n",
      "1394 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.600590]\n",
      "1395 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.664860]\n",
      "1396 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.819202]\n",
      "1397 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.425243]\n",
      "1398 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.971184]\n",
      "1399 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.808002]\n",
      "1400 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.147980]\n",
      "1401 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.919388]\n",
      "1402 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.438831]\n",
      "1403 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.805847]\n",
      "1404 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.141430]\n",
      "1405 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.513077]\n",
      "1406 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.063011]\n",
      "1407 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.551147]\n",
      "1408 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.453369]\n",
      "1409 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.742626]\n",
      "1410 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.316513]\n",
      "1411 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.294369]\n",
      "1412 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.383835]\n",
      "1413 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.734241]\n",
      "1414 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.543049]\n",
      "1415 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.706654]\n",
      "1416 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.374409]\n",
      "1417 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.154015]\n",
      "1418 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.976994]\n",
      "1419 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.333809]\n",
      "1420 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.339767]\n",
      "1421 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.895042]\n",
      "1422 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.827625]\n",
      "1423 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.792969]\n",
      "1424 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.968792]\n",
      "1425 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 53.435154]\n",
      "1426 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.815773]\n",
      "1427 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.384411]\n",
      "1428 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.908466]\n",
      "1429 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.438225]\n",
      "1430 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.403812]\n",
      "1431 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.333420]\n",
      "1432 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.564064]\n",
      "1433 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.517189]\n",
      "1434 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.269543]\n",
      "1435 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.438496]\n",
      "1436 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.603508]\n",
      "1437 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.779427]\n",
      "1438 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 46.790405]\n",
      "1439 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.539616]\n",
      "1440 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.695183]\n",
      "1441 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.172695]\n",
      "1442 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 44.722237]\n",
      "1443 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.754642]\n",
      "1444 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.222897]\n",
      "1445 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.016190]\n",
      "1446 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.704018]\n",
      "1447 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.526245]\n",
      "1448 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.434818]\n",
      "1449 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.933212]\n",
      "1450 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.522526]\n",
      "1451 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.448421]\n",
      "1452 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.431995]\n",
      "1453 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.404171]\n",
      "1454 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.294060]\n",
      "1455 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.902321]\n",
      "1456 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.598331]\n",
      "1457 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.245403]\n",
      "1458 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.899086]\n",
      "1459 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.148178]\n",
      "1460 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.732738]\n",
      "1461 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 49.988174]\n",
      "1462 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.221680]\n",
      "1463 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.297298]\n",
      "1464 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.290791]\n",
      "1465 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.927753]\n",
      "1466 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.719528]\n",
      "2021-10-01 09:17:45.696777: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1467 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.979439]\n",
      "1468 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.689564]\n",
      "1469 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.928997]\n",
      "1470 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.110229]\n",
      "1471 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.249676]\n",
      "1472 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.078373]\n",
      "1473 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.585930]\n",
      "1474 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.788525]\n",
      "2021-10-01 09:17:49.272281: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:17:49.285540: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1475 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.329578]\n",
      "2021-10-01 09:17:49.905260: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:17:50.156410: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:17:50.174044: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "2021-10-01 09:17:50.190972: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1476 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.572418]\n",
      "1477 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.529427]\n",
      "1478 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.406883]\n",
      "2021-10-01 09:17:51.410944: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1479 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.432823]\n",
      "1480 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.603989]\n",
      "1481 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.904846]\n",
      "1482 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.510277]\n",
      "1483 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.063774]\n",
      "1484 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.811081]\n",
      "1485 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.519611]\n",
      "2021-10-01 09:17:54.676769: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1486 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.451382]\n",
      "1487 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.064011]\n",
      "1488 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.704727]\n",
      "1489 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.577702]\n",
      "1490 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 46.042492]\n",
      "1491 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.632477]\n",
      "1492 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.278996]\n",
      "1493 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.601498]\n",
      "1494 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.976242]\n",
      "1495 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.940247]\n",
      "1496 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.582649]\n",
      "1497 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.240391]\n",
      "1498 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.527096]\n",
      "1499 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.801666]\n",
      "1500 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.419056]\n",
      "1501 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.435863]\n",
      "1502 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.738472]\n",
      "1503 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.105690]\n",
      "1504 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.169315]\n",
      "1505 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.054646]\n",
      "1506 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.540413]\n",
      "1507 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.149601]\n",
      "1508 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.934784]\n",
      "1509 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.376492]\n",
      "1510 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.023590]\n",
      "1511 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.224815]\n",
      "1512 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.661293]\n",
      "1513 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.851917]\n",
      "1514 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.239162]\n",
      "1515 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.045982]\n",
      "1516 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.367550]\n",
      "1517 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.820621]\n",
      "1518 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.753750]\n",
      "1519 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 39.683861]\n",
      "1520 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.083752]\n",
      "1521 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.769539]\n",
      "1522 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.491707]\n",
      "1523 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 48.547997]\n",
      "1524 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.790302]\n",
      "1525 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.249756]\n",
      "1526 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.404713]\n",
      "1527 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.741116]\n",
      "1528 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.338505]\n",
      "1529 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.171055]\n",
      "1530 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.394218]\n",
      "1531 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.212116]\n",
      "1532 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.908447]\n",
      "1533 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.254520]\n",
      "1534 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.964153]\n",
      "1535 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.517452]\n",
      "2021-10-01 09:18:17.504218: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1536 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.482628]\n",
      "1537 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.619370]\n",
      "1538 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.222595]\n",
      "1539 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.699219]\n",
      "1540 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.880379]\n",
      "1541 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.681526]\n",
      "1542 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.220695]\n",
      "1543 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 47.889305]\n",
      "1544 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 44.427258]\n",
      "1545 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.592979]\n",
      "1546 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.699181]\n",
      "1547 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.868134]\n",
      "1548 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.753727]\n",
      "1549 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.136189]\n",
      "1550 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.052063]\n",
      "1551 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 40.110573]\n",
      "1552 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.211399]\n",
      "1553 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.405285]\n",
      "1554 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 49.228226]\n",
      "1555 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.471603]\n",
      "1556 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 42.184372]\n",
      "1557 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.928284]\n",
      "1558 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 47.752796]\n",
      "1559 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.588371]\n",
      "1560 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 45.073032]\n",
      "2021-10-01 09:18:28.457134: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1561 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.488098]\n",
      "1562 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.195686]\n",
      "1563 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.483841]\n",
      "1564 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.086323]\n",
      "1565 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.532986]\n",
      "1566 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.592045]\n",
      "1567 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.605011]\n",
      "1568 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.987541]\n",
      "1569 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 39.925472]\n",
      "1570 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 42.348907]\n",
      "1571 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.878300]\n",
      "1572 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.647640]\n",
      "1573 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 38.827435]\n",
      "1574 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.922726]\n",
      "1575 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.901054]\n",
      "1576 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 43.832413]\n",
      "1577 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 40.038845]\n",
      "1578 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.644703]\n",
      "1579 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.791698]\n",
      "1580 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.624641]\n",
      "1581 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.373363]\n",
      "2021-10-01 09:18:38.304410: W tensorflow/core/data/root_dataset.cc:167] Optimization loop failed: Cancelled: Operation was cancelled\n",
      "1582 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.798161]\n",
      "1583 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.177444]\n",
      "1584 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.774773]\n",
      "1585 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.196365]\n",
      "1586 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.484451]\n",
      "1587 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 43.558151]\n",
      "1588 [D1 loss: 0.000035] [D2 loss: 0.000035] [G loss: 43.630638]\n",
      "1589 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.456291]\n",
      "1590 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.340973]\n",
      "1591 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 41.479111]\n",
      "1592 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 48.904007]\n",
      "1593 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.574341]\n",
      "1594 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 48.169415]\n",
      "1595 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.851112]\n",
      "1596 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.100666]\n",
      "1597 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.966145]\n",
      "1598 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.606895]\n",
      "1599 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.103504]\n",
      "1600 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 39.985287]\n",
      "1601 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 40.803192]\n",
      "1602 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.128857]\n",
      "1603 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.936527]\n",
      "1604 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 44.119534]\n",
      "1605 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.437084]\n",
      "1606 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 46.239151]\n",
      "1607 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 45.814068]\n",
      "1608 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.070049]\n",
      "1609 [D1 loss: 0.000035] [D2 loss: 0.000034] [G loss: 41.903389]\n",
      "1610 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 42.542850]\n",
      "1611 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.664375]\n",
      "1612 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 46.875298]\n",
      "1613 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 41.167641]\n",
      "1614 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 50.950382]\n",
      "1615 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 45.124859]\n",
      "1616 [D1 loss: 0.000034] [D2 loss: 0.000034] [G loss: 43.852036]\n",
      "1617 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.059147]\n",
      "1618 [D1 loss: 0.000034] [D2 loss: 0.000035] [G loss: 44.782272]\n"
     ]
    }
   ],
   "source": [
    "!python dualgan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255041b-187a-46b6-a7ce-4bf6486b784e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "dualGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
