{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92504203",
   "metadata": {
    "id": "92504203"
   },
   "source": [
    "# CellStrat Hub Pack - Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1861dc7",
   "metadata": {
    "id": "e1861dc7"
   },
   "source": [
    "#### GAN1 - **Auxiliary Classifier Generative Adversarial Network**(**ACGAN**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd14c2-c7ff-40e7-b66b-afc1da66f2c9",
   "metadata": {},
   "source": [
    "##### Kernel : Tensorflow 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f0e4ab",
   "metadata": {
    "id": "12f0e4ab",
    "outputId": "7f45bca5-c98a-4225-d38d-a2fa8b2eb6da"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Importing required dependencies\n",
    "#==============================================================================\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcfae27",
   "metadata": {
    "id": "cfcfae27"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Create the class\n",
    "#==============================================================================\n",
    "\n",
    "class ACGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=losses,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid, target_label = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model([noise, label], [valid, target_label])\n",
    "        self.combined.compile(loss=losses,\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Extract feature representation\n",
    "        features = model(img)\n",
    "\n",
    "        # Determine validity and label of the image\n",
    "        validity = Dense(1, activation=\"sigmoid\")(features)\n",
    "        label = Dense(self.num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "        return Model(img, [validity, label])\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Configure inputs\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # The labels of the digits that the generator tries to create an\n",
    "            # image representation of\n",
    "            sampled_labels = np.random.randint(0, 10, (batch_size, 1))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "\n",
    "            # Image labels. 0-9 \n",
    "            img_labels = y_train[idx]\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, sampled_labels])\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.save_model()\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    # Save the generated images in images folder\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.discriminator, \"discriminator\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a334a09c",
   "metadata": {
    "id": "a334a09c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 07:41:18.738680: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 97,536\n",
      "Trainable params: 97,344\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "0 [D loss: 3.220330, acc.: 51.56%, op_acc: 7.81%] [G loss: 2.972117]\n",
      "1 [D loss: 3.313813, acc.: 53.12%, op_acc: 12.50%] [G loss: 2.957981]\n",
      "2 [D loss: 3.141563, acc.: 59.38%, op_acc: 7.81%] [G loss: 3.002481]\n",
      "3 [D loss: 2.934278, acc.: 82.81%, op_acc: 15.62%] [G loss: 2.998251]\n",
      "4 [D loss: 3.031402, acc.: 84.38%, op_acc: 4.69%] [G loss: 2.879708]\n",
      "5 [D loss: 2.886357, acc.: 96.88%, op_acc: 7.81%] [G loss: 3.001803]\n",
      "6 [D loss: 3.011629, acc.: 98.44%, op_acc: 3.12%] [G loss: 3.004173]\n",
      "7 [D loss: 2.625437, acc.: 100.00%, op_acc: 9.38%] [G loss: 2.896755]\n",
      "8 [D loss: 2.511057, acc.: 98.44%, op_acc: 20.31%] [G loss: 2.935746]\n",
      "9 [D loss: 2.427838, acc.: 100.00%, op_acc: 20.31%] [G loss: 2.663938]\n",
      "10 [D loss: 2.638220, acc.: 100.00%, op_acc: 6.25%] [G loss: 2.772675]\n",
      "11 [D loss: 2.557931, acc.: 100.00%, op_acc: 14.06%] [G loss: 2.746182]\n",
      "12 [D loss: 2.531571, acc.: 100.00%, op_acc: 15.62%] [G loss: 2.866305]\n",
      "13 [D loss: 2.609775, acc.: 100.00%, op_acc: 12.50%] [G loss: 2.936472]\n",
      "14 [D loss: 2.407591, acc.: 100.00%, op_acc: 15.62%] [G loss: 2.663940]\n",
      "15 [D loss: 2.460668, acc.: 100.00%, op_acc: 14.06%] [G loss: 3.112592]\n",
      "16 [D loss: 2.367434, acc.: 98.44%, op_acc: 23.44%] [G loss: 2.657551]\n",
      "17 [D loss: 2.547008, acc.: 93.75%, op_acc: 17.19%] [G loss: 3.614506]\n",
      "18 [D loss: 2.523627, acc.: 98.44%, op_acc: 15.62%] [G loss: 2.848981]\n",
      "19 [D loss: 2.587615, acc.: 95.31%, op_acc: 10.94%] [G loss: 2.779407]\n",
      "20 [D loss: 2.515630, acc.: 92.19%, op_acc: 15.62%] [G loss: 2.966915]\n",
      "21 [D loss: 2.657055, acc.: 87.50%, op_acc: 18.75%] [G loss: 3.070879]\n",
      "22 [D loss: 2.876171, acc.: 67.19%, op_acc: 20.31%] [G loss: 2.995870]\n",
      "23 [D loss: 2.856865, acc.: 70.31%, op_acc: 17.19%] [G loss: 2.970981]\n",
      "24 [D loss: 2.614596, acc.: 82.81%, op_acc: 20.31%] [G loss: 3.437757]\n",
      "25 [D loss: 2.785720, acc.: 76.56%, op_acc: 18.75%] [G loss: 3.735467]\n",
      "26 [D loss: 2.803660, acc.: 71.88%, op_acc: 21.88%] [G loss: 4.175346]\n",
      "27 [D loss: 2.949391, acc.: 65.62%, op_acc: 18.75%] [G loss: 3.923660]\n",
      "28 [D loss: 2.863528, acc.: 62.50%, op_acc: 21.88%] [G loss: 3.750150]\n",
      "29 [D loss: 2.799963, acc.: 73.44%, op_acc: 21.88%] [G loss: 3.749966]\n",
      "30 [D loss: 2.734037, acc.: 70.31%, op_acc: 28.12%] [G loss: 3.781480]\n",
      "31 [D loss: 2.786752, acc.: 76.56%, op_acc: 28.12%] [G loss: 3.652842]\n",
      "32 [D loss: 2.643257, acc.: 75.00%, op_acc: 26.56%] [G loss: 3.726804]\n",
      "33 [D loss: 2.669484, acc.: 81.25%, op_acc: 28.12%] [G loss: 3.816894]\n",
      "34 [D loss: 2.690227, acc.: 76.56%, op_acc: 28.12%] [G loss: 3.512394]\n",
      "35 [D loss: 2.662502, acc.: 84.38%, op_acc: 25.00%] [G loss: 3.867035]\n",
      "36 [D loss: 2.504713, acc.: 84.38%, op_acc: 28.12%] [G loss: 3.446613]\n",
      "37 [D loss: 2.627303, acc.: 85.94%, op_acc: 20.31%] [G loss: 3.477962]\n",
      "38 [D loss: 2.569506, acc.: 82.81%, op_acc: 26.56%] [G loss: 3.640035]\n",
      "39 [D loss: 2.689512, acc.: 87.50%, op_acc: 14.06%] [G loss: 3.864295]\n",
      "40 [D loss: 2.525033, acc.: 79.69%, op_acc: 29.69%] [G loss: 3.609734]\n",
      "41 [D loss: 2.437128, acc.: 79.69%, op_acc: 31.25%] [G loss: 3.341100]\n",
      "42 [D loss: 2.586058, acc.: 76.56%, op_acc: 26.56%] [G loss: 3.345372]\n",
      "43 [D loss: 2.617424, acc.: 82.81%, op_acc: 29.69%] [G loss: 3.809305]\n",
      "44 [D loss: 2.616786, acc.: 78.12%, op_acc: 28.12%] [G loss: 3.397942]\n",
      "45 [D loss: 2.687439, acc.: 84.38%, op_acc: 23.44%] [G loss: 3.297263]\n",
      "46 [D loss: 2.645971, acc.: 79.69%, op_acc: 20.31%] [G loss: 3.247875]\n",
      "47 [D loss: 2.423365, acc.: 90.62%, op_acc: 39.06%] [G loss: 3.488693]\n",
      "48 [D loss: 2.560827, acc.: 78.12%, op_acc: 28.12%] [G loss: 3.379333]\n",
      "49 [D loss: 2.539734, acc.: 87.50%, op_acc: 25.00%] [G loss: 3.334504]\n",
      "50 [D loss: 2.566762, acc.: 79.69%, op_acc: 23.44%] [G loss: 3.311661]\n",
      "51 [D loss: 2.754085, acc.: 65.62%, op_acc: 25.00%] [G loss: 3.552905]\n",
      "52 [D loss: 2.335920, acc.: 93.75%, op_acc: 32.81%] [G loss: 3.852793]\n",
      "53 [D loss: 2.442831, acc.: 85.94%, op_acc: 35.94%] [G loss: 3.706713]\n",
      "54 [D loss: 2.400940, acc.: 92.19%, op_acc: 29.69%] [G loss: 3.825084]\n",
      "55 [D loss: 2.361008, acc.: 90.62%, op_acc: 34.38%] [G loss: 3.705525]\n",
      "56 [D loss: 2.548970, acc.: 90.62%, op_acc: 28.12%] [G loss: 3.978548]\n",
      "57 [D loss: 2.361761, acc.: 93.75%, op_acc: 29.69%] [G loss: 3.588514]\n",
      "58 [D loss: 2.493145, acc.: 85.94%, op_acc: 28.12%] [G loss: 3.445134]\n",
      "59 [D loss: 2.319681, acc.: 82.81%, op_acc: 35.94%] [G loss: 3.232880]\n",
      "60 [D loss: 2.450610, acc.: 84.38%, op_acc: 37.50%] [G loss: 4.120758]\n",
      "61 [D loss: 2.345984, acc.: 87.50%, op_acc: 26.56%] [G loss: 3.378239]\n",
      "62 [D loss: 2.184844, acc.: 93.75%, op_acc: 37.50%] [G loss: 3.277193]\n",
      "63 [D loss: 2.510155, acc.: 82.81%, op_acc: 28.12%] [G loss: 3.672739]\n",
      "64 [D loss: 2.481535, acc.: 82.81%, op_acc: 34.38%] [G loss: 3.508431]\n",
      "65 [D loss: 2.268355, acc.: 95.31%, op_acc: 39.06%] [G loss: 3.799883]\n",
      "66 [D loss: 2.354105, acc.: 81.25%, op_acc: 29.69%] [G loss: 3.286066]\n",
      "67 [D loss: 2.521926, acc.: 79.69%, op_acc: 31.25%] [G loss: 3.622159]\n",
      "68 [D loss: 2.096084, acc.: 92.19%, op_acc: 39.06%] [G loss: 3.539820]\n",
      "69 [D loss: 2.342783, acc.: 87.50%, op_acc: 37.50%] [G loss: 3.530849]\n",
      "70 [D loss: 2.337717, acc.: 85.94%, op_acc: 35.94%] [G loss: 3.336617]\n",
      "71 [D loss: 2.118778, acc.: 90.62%, op_acc: 37.50%] [G loss: 3.475810]\n",
      "72 [D loss: 2.231204, acc.: 79.69%, op_acc: 39.06%] [G loss: 3.676995]\n",
      "73 [D loss: 2.417738, acc.: 89.06%, op_acc: 34.38%] [G loss: 3.861954]\n",
      "74 [D loss: 2.311680, acc.: 76.56%, op_acc: 34.38%] [G loss: 3.419160]\n",
      "75 [D loss: 2.184787, acc.: 92.19%, op_acc: 39.06%] [G loss: 3.497961]\n",
      "76 [D loss: 2.231330, acc.: 90.62%, op_acc: 37.50%] [G loss: 3.502254]\n",
      "77 [D loss: 2.132030, acc.: 82.81%, op_acc: 42.19%] [G loss: 3.287970]\n",
      "78 [D loss: 2.449381, acc.: 89.06%, op_acc: 34.38%] [G loss: 3.518021]\n",
      "79 [D loss: 2.068939, acc.: 95.31%, op_acc: 42.19%] [G loss: 3.743994]\n",
      "80 [D loss: 2.248931, acc.: 90.62%, op_acc: 34.38%] [G loss: 3.774290]\n",
      "81 [D loss: 2.006810, acc.: 90.62%, op_acc: 50.00%] [G loss: 3.596894]\n",
      "82 [D loss: 1.939173, acc.: 95.31%, op_acc: 40.62%] [G loss: 3.517796]\n",
      "83 [D loss: 2.210061, acc.: 90.62%, op_acc: 35.94%] [G loss: 3.639369]\n",
      "84 [D loss: 2.150862, acc.: 82.81%, op_acc: 42.19%] [G loss: 3.392317]\n",
      "85 [D loss: 2.257913, acc.: 81.25%, op_acc: 43.75%] [G loss: 3.555927]\n",
      "86 [D loss: 2.165663, acc.: 90.62%, op_acc: 35.94%] [G loss: 3.535697]\n",
      "87 [D loss: 2.168993, acc.: 87.50%, op_acc: 40.62%] [G loss: 3.446207]\n",
      "88 [D loss: 2.103193, acc.: 76.56%, op_acc: 54.69%] [G loss: 2.840971]\n",
      "89 [D loss: 2.060769, acc.: 92.19%, op_acc: 35.94%] [G loss: 3.261440]\n",
      "90 [D loss: 2.206297, acc.: 81.25%, op_acc: 39.06%] [G loss: 3.358510]\n",
      "91 [D loss: 2.214885, acc.: 82.81%, op_acc: 42.19%] [G loss: 3.108943]\n",
      "92 [D loss: 2.094490, acc.: 87.50%, op_acc: 48.44%] [G loss: 3.600119]\n",
      "93 [D loss: 2.194046, acc.: 82.81%, op_acc: 37.50%] [G loss: 3.349213]\n",
      "94 [D loss: 1.949636, acc.: 85.94%, op_acc: 50.00%] [G loss: 3.345057]\n",
      "95 [D loss: 2.088844, acc.: 85.94%, op_acc: 48.44%] [G loss: 3.549366]\n",
      "96 [D loss: 2.223658, acc.: 89.06%, op_acc: 43.75%] [G loss: 3.367716]\n",
      "97 [D loss: 2.086717, acc.: 95.31%, op_acc: 42.19%] [G loss: 3.471420]\n",
      "98 [D loss: 2.093020, acc.: 87.50%, op_acc: 45.31%] [G loss: 3.205854]\n",
      "99 [D loss: 2.163614, acc.: 84.38%, op_acc: 40.62%] [G loss: 3.027091]\n",
      "100 [D loss: 2.100191, acc.: 90.62%, op_acc: 45.31%] [G loss: 3.301871]\n",
      "101 [D loss: 2.032065, acc.: 92.19%, op_acc: 45.31%] [G loss: 3.324475]\n",
      "102 [D loss: 2.001964, acc.: 90.62%, op_acc: 39.06%] [G loss: 3.540692]\n",
      "103 [D loss: 2.117345, acc.: 82.81%, op_acc: 35.94%] [G loss: 3.696463]\n",
      "104 [D loss: 1.834313, acc.: 90.62%, op_acc: 48.44%] [G loss: 3.528350]\n",
      "105 [D loss: 2.137312, acc.: 90.62%, op_acc: 39.06%] [G loss: 3.696717]\n",
      "106 [D loss: 1.992997, acc.: 93.75%, op_acc: 45.31%] [G loss: 3.622406]\n",
      "107 [D loss: 2.049174, acc.: 95.31%, op_acc: 45.31%] [G loss: 3.143107]\n",
      "108 [D loss: 2.042532, acc.: 89.06%, op_acc: 39.06%] [G loss: 3.306900]\n",
      "109 [D loss: 1.821370, acc.: 92.19%, op_acc: 46.88%] [G loss: 3.517922]\n",
      "110 [D loss: 1.822983, acc.: 87.50%, op_acc: 48.44%] [G loss: 3.474830]\n",
      "111 [D loss: 1.900843, acc.: 95.31%, op_acc: 43.75%] [G loss: 3.694980]\n",
      "112 [D loss: 1.914376, acc.: 90.62%, op_acc: 39.06%] [G loss: 3.477048]\n",
      "113 [D loss: 2.106602, acc.: 95.31%, op_acc: 34.38%] [G loss: 4.066540]\n",
      "114 [D loss: 1.777716, acc.: 96.88%, op_acc: 50.00%] [G loss: 3.830923]\n",
      "115 [D loss: 1.847856, acc.: 96.88%, op_acc: 43.75%] [G loss: 3.911893]\n",
      "116 [D loss: 1.872027, acc.: 89.06%, op_acc: 39.06%] [G loss: 3.745542]\n",
      "117 [D loss: 1.935798, acc.: 96.88%, op_acc: 40.62%] [G loss: 3.527781]\n",
      "118 [D loss: 1.682134, acc.: 100.00%, op_acc: 51.56%] [G loss: 3.334879]\n",
      "119 [D loss: 1.821022, acc.: 96.88%, op_acc: 46.88%] [G loss: 3.409150]\n",
      "120 [D loss: 1.965427, acc.: 95.31%, op_acc: 42.19%] [G loss: 3.630355]\n",
      "121 [D loss: 1.857494, acc.: 89.06%, op_acc: 45.31%] [G loss: 3.980336]\n",
      "122 [D loss: 1.779634, acc.: 100.00%, op_acc: 42.19%] [G loss: 4.220540]\n",
      "123 [D loss: 1.780465, acc.: 98.44%, op_acc: 43.75%] [G loss: 3.992875]\n",
      "124 [D loss: 1.646494, acc.: 98.44%, op_acc: 40.62%] [G loss: 3.734875]\n",
      "125 [D loss: 1.571224, acc.: 96.88%, op_acc: 56.25%] [G loss: 3.390563]\n",
      "126 [D loss: 1.826134, acc.: 98.44%, op_acc: 40.62%] [G loss: 3.628337]\n",
      "127 [D loss: 1.633758, acc.: 100.00%, op_acc: 39.06%] [G loss: 3.692376]\n",
      "128 [D loss: 1.788221, acc.: 95.31%, op_acc: 48.44%] [G loss: 3.800351]\n",
      "129 [D loss: 1.593806, acc.: 96.88%, op_acc: 46.88%] [G loss: 3.614872]\n",
      "130 [D loss: 1.777877, acc.: 96.88%, op_acc: 39.06%] [G loss: 3.563021]\n",
      "131 [D loss: 1.507484, acc.: 93.75%, op_acc: 56.25%] [G loss: 3.558692]\n",
      "132 [D loss: 1.731303, acc.: 100.00%, op_acc: 46.88%] [G loss: 3.981584]\n",
      "133 [D loss: 1.863032, acc.: 100.00%, op_acc: 42.19%] [G loss: 3.894057]\n",
      "134 [D loss: 1.591581, acc.: 98.44%, op_acc: 42.19%] [G loss: 3.761335]\n",
      "135 [D loss: 1.577247, acc.: 100.00%, op_acc: 53.12%] [G loss: 3.692410]\n",
      "136 [D loss: 1.633259, acc.: 100.00%, op_acc: 51.56%] [G loss: 3.850850]\n",
      "137 [D loss: 1.624109, acc.: 98.44%, op_acc: 43.75%] [G loss: 3.877557]\n",
      "138 [D loss: 1.665707, acc.: 100.00%, op_acc: 54.69%] [G loss: 4.161830]\n",
      "139 [D loss: 1.661002, acc.: 100.00%, op_acc: 43.75%] [G loss: 3.966195]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    acgan = ACGAN()\n",
    "    acgan.train(epochs=14000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca32534-363d-4526-910b-364c234f7a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a347c-5e64-4f5e-b5c2-98a8b356e73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ee42e-efe4-40d6-b2de-5a8d9a10a80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ACGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow 2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
