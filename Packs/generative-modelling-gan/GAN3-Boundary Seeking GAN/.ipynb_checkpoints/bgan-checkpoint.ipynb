{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66e20f4",
   "metadata": {},
   "source": [
    "# CellStrat Hub Pack - Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eea192",
   "metadata": {},
   "source": [
    "#### GAN3 -  **Boundary-Seeking Generative Adversarial Networks**(**BGAN**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b20363-72d5-4931-b6e0-86eb01a2ad76",
   "metadata": {},
   "source": [
    "##### Kernel : Tensorflow 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443148cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Importing required dependencies\n",
    "#==============================================================================\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0c506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Create a class \n",
    "#==============================================================================\n",
    "\n",
    "class BGAN():\n",
    "    \"\"\"Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.boundary_loss, optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def boundary_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Boundary seeking loss.\n",
    "        Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\n",
    "        \"\"\"\n",
    "        return 0.5 * K.mean((K.log(y_pred) - K.log(1 - y_pred))**2)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "    \n",
    "    #saving the generated images in images folder\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43db3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 15:49:40.957899: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.690193, acc.: 50.00%] [G loss: 0.208162]\n",
      "1 [D loss: 0.375381, acc.: 75.00%] [G loss: 0.160345]\n",
      "2 [D loss: 0.333213, acc.: 87.50%] [G loss: 0.114602]\n",
      "3 [D loss: 0.316659, acc.: 90.62%] [G loss: 0.144952]\n",
      "4 [D loss: 0.287471, acc.: 92.19%] [G loss: 0.297232]\n",
      "5 [D loss: 0.266316, acc.: 93.75%] [G loss: 0.455586]\n",
      "6 [D loss: 0.235724, acc.: 96.88%] [G loss: 0.717204]\n",
      "7 [D loss: 0.192723, acc.: 100.00%] [G loss: 0.967452]\n",
      "8 [D loss: 0.152528, acc.: 100.00%] [G loss: 1.155643]\n",
      "9 [D loss: 0.138439, acc.: 100.00%] [G loss: 1.552378]\n",
      "10 [D loss: 0.107326, acc.: 100.00%] [G loss: 1.889522]\n",
      "11 [D loss: 0.095495, acc.: 100.00%] [G loss: 2.018602]\n",
      "12 [D loss: 0.095281, acc.: 100.00%] [G loss: 2.244390]\n",
      "13 [D loss: 0.080570, acc.: 100.00%] [G loss: 2.561519]\n",
      "14 [D loss: 0.063162, acc.: 100.00%] [G loss: 2.879415]\n",
      "15 [D loss: 0.068211, acc.: 100.00%] [G loss: 2.739085]\n",
      "16 [D loss: 0.063321, acc.: 100.00%] [G loss: 2.999694]\n",
      "17 [D loss: 0.066253, acc.: 100.00%] [G loss: 3.457434]\n",
      "18 [D loss: 0.047286, acc.: 100.00%] [G loss: 3.863448]\n",
      "19 [D loss: 0.043311, acc.: 100.00%] [G loss: 4.115603]\n",
      "20 [D loss: 0.042857, acc.: 100.00%] [G loss: 4.294535]\n",
      "21 [D loss: 0.039296, acc.: 100.00%] [G loss: 4.372572]\n",
      "22 [D loss: 0.042380, acc.: 100.00%] [G loss: 4.624842]\n",
      "23 [D loss: 0.031175, acc.: 100.00%] [G loss: 4.532500]\n",
      "24 [D loss: 0.030495, acc.: 100.00%] [G loss: 4.721941]\n",
      "25 [D loss: 0.029308, acc.: 100.00%] [G loss: 5.214773]\n",
      "26 [D loss: 0.029956, acc.: 100.00%] [G loss: 5.315321]\n",
      "27 [D loss: 0.033702, acc.: 100.00%] [G loss: 5.476778]\n",
      "28 [D loss: 0.025484, acc.: 100.00%] [G loss: 5.784671]\n",
      "29 [D loss: 0.028637, acc.: 100.00%] [G loss: 5.965631]\n",
      "30 [D loss: 0.025807, acc.: 100.00%] [G loss: 5.818521]\n",
      "31 [D loss: 0.019998, acc.: 100.00%] [G loss: 6.080223]\n",
      "32 [D loss: 0.022287, acc.: 100.00%] [G loss: 6.514088]\n",
      "33 [D loss: 0.019495, acc.: 100.00%] [G loss: 6.077839]\n",
      "34 [D loss: 0.020862, acc.: 100.00%] [G loss: 6.373219]\n",
      "35 [D loss: 0.019300, acc.: 100.00%] [G loss: 6.451871]\n",
      "36 [D loss: 0.021909, acc.: 100.00%] [G loss: 6.869101]\n",
      "37 [D loss: 0.020716, acc.: 100.00%] [G loss: 6.609785]\n",
      "38 [D loss: 0.018587, acc.: 100.00%] [G loss: 6.859258]\n",
      "39 [D loss: 0.019111, acc.: 100.00%] [G loss: 7.047838]\n",
      "40 [D loss: 0.015184, acc.: 100.00%] [G loss: 7.387144]\n",
      "41 [D loss: 0.021358, acc.: 100.00%] [G loss: 7.139863]\n",
      "42 [D loss: 0.015281, acc.: 100.00%] [G loss: 7.461131]\n",
      "43 [D loss: 0.018966, acc.: 100.00%] [G loss: 7.698560]\n",
      "44 [D loss: 0.013244, acc.: 100.00%] [G loss: 7.539291]\n",
      "45 [D loss: 0.013651, acc.: 100.00%] [G loss: 7.565624]\n",
      "46 [D loss: 0.014982, acc.: 100.00%] [G loss: 7.591477]\n",
      "47 [D loss: 0.020881, acc.: 100.00%] [G loss: 7.456794]\n",
      "48 [D loss: 0.013248, acc.: 100.00%] [G loss: 8.423578]\n",
      "49 [D loss: 0.016341, acc.: 100.00%] [G loss: 8.090562]\n",
      "50 [D loss: 0.013917, acc.: 100.00%] [G loss: 7.894992]\n",
      "51 [D loss: 0.013192, acc.: 100.00%] [G loss: 8.581713]\n",
      "52 [D loss: 0.015431, acc.: 100.00%] [G loss: 8.510336]\n",
      "53 [D loss: 0.010291, acc.: 100.00%] [G loss: 8.865722]\n",
      "54 [D loss: 0.013197, acc.: 100.00%] [G loss: 8.602512]\n",
      "55 [D loss: 0.011840, acc.: 100.00%] [G loss: 8.513205]\n",
      "56 [D loss: 0.014986, acc.: 100.00%] [G loss: 8.542173]\n",
      "57 [D loss: 0.015696, acc.: 100.00%] [G loss: 8.511806]\n",
      "58 [D loss: 0.011752, acc.: 100.00%] [G loss: 8.888113]\n",
      "59 [D loss: 0.013409, acc.: 100.00%] [G loss: 9.216112]\n",
      "60 [D loss: 0.012956, acc.: 100.00%] [G loss: 8.840714]\n",
      "61 [D loss: 0.009286, acc.: 100.00%] [G loss: 9.385958]\n",
      "62 [D loss: 0.013011, acc.: 100.00%] [G loss: 8.971178]\n",
      "63 [D loss: 0.010237, acc.: 100.00%] [G loss: 9.129080]\n",
      "64 [D loss: 0.012373, acc.: 100.00%] [G loss: 9.383258]\n",
      "65 [D loss: 0.008540, acc.: 100.00%] [G loss: 8.941807]\n",
      "66 [D loss: 0.011451, acc.: 100.00%] [G loss: 9.162943]\n",
      "67 [D loss: 0.015139, acc.: 100.00%] [G loss: 8.619251]\n",
      "68 [D loss: 0.009884, acc.: 100.00%] [G loss: 9.474405]\n",
      "69 [D loss: 0.012974, acc.: 100.00%] [G loss: 9.819462]\n",
      "70 [D loss: 0.010286, acc.: 100.00%] [G loss: 9.654696]\n",
      "71 [D loss: 0.014648, acc.: 100.00%] [G loss: 9.462420]\n",
      "72 [D loss: 0.010457, acc.: 100.00%] [G loss: 9.886734]\n",
      "73 [D loss: 0.009627, acc.: 100.00%] [G loss: 9.887572]\n",
      "74 [D loss: 0.009511, acc.: 100.00%] [G loss: 9.737745]\n",
      "75 [D loss: 0.011836, acc.: 100.00%] [G loss: 9.037047]\n",
      "76 [D loss: 0.011199, acc.: 100.00%] [G loss: 10.139004]\n",
      "77 [D loss: 0.011923, acc.: 100.00%] [G loss: 9.266689]\n",
      "78 [D loss: 0.008060, acc.: 100.00%] [G loss: 9.055304]\n",
      "79 [D loss: 0.015022, acc.: 100.00%] [G loss: 10.043395]\n",
      "80 [D loss: 0.014458, acc.: 100.00%] [G loss: 10.105637]\n",
      "81 [D loss: 0.007413, acc.: 100.00%] [G loss: 10.828484]\n",
      "82 [D loss: 0.010096, acc.: 100.00%] [G loss: 10.452698]\n",
      "83 [D loss: 0.009535, acc.: 100.00%] [G loss: 10.426628]\n",
      "84 [D loss: 0.010932, acc.: 100.00%] [G loss: 10.421947]\n",
      "85 [D loss: 0.011237, acc.: 100.00%] [G loss: 10.233425]\n",
      "86 [D loss: 0.010762, acc.: 100.00%] [G loss: 10.500751]\n",
      "87 [D loss: 0.011366, acc.: 100.00%] [G loss: 10.742624]\n",
      "88 [D loss: 0.010557, acc.: 100.00%] [G loss: 10.555109]\n",
      "89 [D loss: 0.010035, acc.: 100.00%] [G loss: 11.189808]\n",
      "90 [D loss: 0.009955, acc.: 100.00%] [G loss: 10.794696]\n",
      "91 [D loss: 0.010319, acc.: 100.00%] [G loss: 11.134600]\n",
      "92 [D loss: 0.010262, acc.: 100.00%] [G loss: 11.083828]\n",
      "93 [D loss: 0.008912, acc.: 100.00%] [G loss: 10.855528]\n",
      "94 [D loss: 0.013809, acc.: 100.00%] [G loss: 11.192061]\n",
      "95 [D loss: 0.010455, acc.: 100.00%] [G loss: 10.812908]\n",
      "96 [D loss: 0.010342, acc.: 100.00%] [G loss: 10.716370]\n",
      "97 [D loss: 0.010790, acc.: 100.00%] [G loss: 11.107816]\n",
      "98 [D loss: 0.014123, acc.: 100.00%] [G loss: 11.749856]\n",
      "99 [D loss: 0.014783, acc.: 100.00%] [G loss: 11.124261]\n",
      "100 [D loss: 0.007840, acc.: 100.00%] [G loss: 10.382607]\n",
      "101 [D loss: 0.010241, acc.: 100.00%] [G loss: 11.181103]\n",
      "102 [D loss: 0.008031, acc.: 100.00%] [G loss: 10.929363]\n",
      "103 [D loss: 0.008838, acc.: 100.00%] [G loss: 10.872036]\n",
      "104 [D loss: 0.008852, acc.: 100.00%] [G loss: 10.466630]\n",
      "105 [D loss: 0.015361, acc.: 100.00%] [G loss: 11.442545]\n",
      "106 [D loss: 0.012301, acc.: 100.00%] [G loss: 11.983465]\n",
      "107 [D loss: 0.015679, acc.: 100.00%] [G loss: 10.996717]\n",
      "108 [D loss: 0.022006, acc.: 100.00%] [G loss: 11.338037]\n",
      "109 [D loss: 0.020775, acc.: 100.00%] [G loss: 11.418900]\n",
      "110 [D loss: 0.018784, acc.: 100.00%] [G loss: 11.946188]\n",
      "111 [D loss: 0.018255, acc.: 100.00%] [G loss: 12.526859]\n",
      "112 [D loss: 0.030036, acc.: 100.00%] [G loss: 12.856462]\n",
      "113 [D loss: 0.209531, acc.: 89.06%] [G loss: 25.661413]\n",
      "114 [D loss: 2.081839, acc.: 39.06%] [G loss: 64.825562]\n",
      "115 [D loss: 0.566623, acc.: 85.94%] [G loss: 8.153901]\n",
      "116 [D loss: 0.460847, acc.: 85.94%] [G loss: 32.796070]\n",
      "117 [D loss: 0.750445, acc.: 76.56%] [G loss: 24.603632]\n",
      "118 [D loss: 0.106643, acc.: 95.31%] [G loss: 11.838732]\n",
      "119 [D loss: 0.124693, acc.: 92.19%] [G loss: 19.513470]\n",
      "120 [D loss: 0.063088, acc.: 96.88%] [G loss: 18.934778]\n",
      "121 [D loss: 0.039933, acc.: 98.44%] [G loss: 17.306236]\n",
      "122 [D loss: 0.043274, acc.: 100.00%] [G loss: 15.648451]\n",
      "123 [D loss: 0.032765, acc.: 100.00%] [G loss: 12.706722]\n",
      "124 [D loss: 0.034751, acc.: 98.44%] [G loss: 12.293575]\n",
      "125 [D loss: 0.022543, acc.: 100.00%] [G loss: 11.655422]\n",
      "126 [D loss: 0.028642, acc.: 100.00%] [G loss: 10.604837]\n",
      "127 [D loss: 0.031063, acc.: 100.00%] [G loss: 9.591776]\n",
      "128 [D loss: 0.034760, acc.: 100.00%] [G loss: 10.279535]\n",
      "129 [D loss: 0.045730, acc.: 100.00%] [G loss: 9.237507]\n",
      "130 [D loss: 0.024648, acc.: 100.00%] [G loss: 8.050984]\n",
      "131 [D loss: 0.040161, acc.: 98.44%] [G loss: 7.276506]\n",
      "132 [D loss: 0.028670, acc.: 100.00%] [G loss: 7.689514]\n",
      "133 [D loss: 0.037914, acc.: 100.00%] [G loss: 7.587914]\n",
      "134 [D loss: 0.039586, acc.: 100.00%] [G loss: 7.361425]\n",
      "135 [D loss: 0.041563, acc.: 100.00%] [G loss: 7.255180]\n",
      "136 [D loss: 0.036264, acc.: 100.00%] [G loss: 6.523106]\n",
      "137 [D loss: 0.061302, acc.: 98.44%] [G loss: 7.015430]\n",
      "138 [D loss: 0.049853, acc.: 100.00%] [G loss: 7.417301]\n",
      "139 [D loss: 0.068783, acc.: 96.88%] [G loss: 8.029984]\n",
      "140 [D loss: 0.056908, acc.: 100.00%] [G loss: 7.131494]\n",
      "141 [D loss: 0.034979, acc.: 100.00%] [G loss: 6.865383]\n",
      "142 [D loss: 0.051196, acc.: 98.44%] [G loss: 7.121807]\n",
      "143 [D loss: 0.056648, acc.: 98.44%] [G loss: 7.385353]\n",
      "144 [D loss: 0.031461, acc.: 100.00%] [G loss: 7.179878]\n",
      "145 [D loss: 0.042749, acc.: 100.00%] [G loss: 6.437005]\n",
      "146 [D loss: 0.056082, acc.: 100.00%] [G loss: 6.648777]\n",
      "147 [D loss: 0.051425, acc.: 98.44%] [G loss: 7.777934]\n",
      "148 [D loss: 0.056076, acc.: 100.00%] [G loss: 7.323459]\n",
      "149 [D loss: 0.058714, acc.: 100.00%] [G loss: 7.010842]\n",
      "150 [D loss: 0.074699, acc.: 98.44%] [G loss: 8.454941]\n",
      "151 [D loss: 0.066524, acc.: 98.44%] [G loss: 8.280693]\n",
      "152 [D loss: 0.141580, acc.: 95.31%] [G loss: 5.465981]\n",
      "153 [D loss: 0.092335, acc.: 96.88%] [G loss: 9.063777]\n",
      "154 [D loss: 0.043743, acc.: 100.00%] [G loss: 8.161148]\n",
      "155 [D loss: 0.072799, acc.: 100.00%] [G loss: 6.209521]\n",
      "156 [D loss: 0.053360, acc.: 98.44%] [G loss: 7.356507]\n",
      "157 [D loss: 0.071342, acc.: 98.44%] [G loss: 9.037692]\n",
      "158 [D loss: 0.430755, acc.: 82.81%] [G loss: 5.234961]\n",
      "159 [D loss: 0.113314, acc.: 95.31%] [G loss: 7.616332]\n",
      "160 [D loss: 0.030032, acc.: 100.00%] [G loss: 10.686613]\n",
      "161 [D loss: 0.094254, acc.: 96.88%] [G loss: 9.124898]\n",
      "162 [D loss: 0.062210, acc.: 100.00%] [G loss: 10.624301]\n",
      "163 [D loss: 0.097079, acc.: 100.00%] [G loss: 8.471998]\n",
      "164 [D loss: 0.046798, acc.: 100.00%] [G loss: 7.686117]\n",
      "165 [D loss: 0.123086, acc.: 95.31%] [G loss: 9.087713]\n",
      "166 [D loss: 0.066616, acc.: 98.44%] [G loss: 9.646371]\n",
      "167 [D loss: 0.114050, acc.: 95.31%] [G loss: 8.704123]\n",
      "168 [D loss: 0.030743, acc.: 100.00%] [G loss: 7.877872]\n",
      "169 [D loss: 0.121690, acc.: 95.31%] [G loss: 10.621630]\n",
      "170 [D loss: 0.225329, acc.: 89.06%] [G loss: 12.996599]\n",
      "171 [D loss: 0.452910, acc.: 81.25%] [G loss: 6.207376]\n",
      "172 [D loss: 0.061657, acc.: 98.44%] [G loss: 8.819190]\n",
      "173 [D loss: 0.041887, acc.: 98.44%] [G loss: 13.291765]\n",
      "174 [D loss: 0.356446, acc.: 87.50%] [G loss: 3.439133]\n",
      "175 [D loss: 0.213005, acc.: 89.06%] [G loss: 10.457212]\n",
      "176 [D loss: 0.052603, acc.: 100.00%] [G loss: 11.629799]\n",
      "177 [D loss: 0.232127, acc.: 89.06%] [G loss: 12.790121]\n",
      "178 [D loss: 0.133254, acc.: 98.44%] [G loss: 7.810796]\n",
      "179 [D loss: 0.042397, acc.: 100.00%] [G loss: 9.990412]\n",
      "180 [D loss: 0.217954, acc.: 90.62%] [G loss: 11.594598]\n",
      "181 [D loss: 0.165185, acc.: 95.31%] [G loss: 7.496091]\n",
      "182 [D loss: 0.133968, acc.: 95.31%] [G loss: 8.044208]\n",
      "183 [D loss: 0.047197, acc.: 100.00%] [G loss: 9.609457]\n",
      "184 [D loss: 0.110133, acc.: 95.31%] [G loss: 7.173809]\n",
      "185 [D loss: 0.097652, acc.: 98.44%] [G loss: 12.068163]\n",
      "186 [D loss: 0.792567, acc.: 60.94%] [G loss: 3.411903]\n",
      "187 [D loss: 0.317146, acc.: 87.50%] [G loss: 13.525615]\n",
      "188 [D loss: 0.087915, acc.: 98.44%] [G loss: 11.170441]\n",
      "189 [D loss: 0.110304, acc.: 98.44%] [G loss: 10.437538]\n",
      "190 [D loss: 0.183398, acc.: 92.19%] [G loss: 9.576303]\n",
      "191 [D loss: 0.087323, acc.: 98.44%] [G loss: 8.910938]\n",
      "192 [D loss: 0.258407, acc.: 85.94%] [G loss: 16.928232]\n",
      "193 [D loss: 1.319633, acc.: 53.12%] [G loss: 3.927248]\n",
      "194 [D loss: 0.523575, acc.: 78.12%] [G loss: 6.181901]\n",
      "195 [D loss: 0.027181, acc.: 100.00%] [G loss: 14.489201]\n",
      "196 [D loss: 0.310650, acc.: 89.06%] [G loss: 6.585280]\n",
      "197 [D loss: 0.109997, acc.: 96.88%] [G loss: 8.307327]\n",
      "198 [D loss: 0.065652, acc.: 100.00%] [G loss: 8.496029]\n",
      "199 [D loss: 0.110616, acc.: 98.44%] [G loss: 7.604528]\n",
      "200 [D loss: 0.147982, acc.: 95.31%] [G loss: 9.504723]\n",
      "201 [D loss: 0.538964, acc.: 75.00%] [G loss: 5.981770]\n",
      "202 [D loss: 0.086395, acc.: 100.00%] [G loss: 7.327553]\n",
      "203 [D loss: 0.375072, acc.: 82.81%] [G loss: 2.655416]\n",
      "204 [D loss: 0.155347, acc.: 92.19%] [G loss: 7.654537]\n",
      "205 [D loss: 0.141834, acc.: 95.31%] [G loss: 7.635309]\n",
      "206 [D loss: 0.162124, acc.: 98.44%] [G loss: 7.468601]\n",
      "207 [D loss: 0.213944, acc.: 93.75%] [G loss: 8.777298]\n",
      "208 [D loss: 0.404862, acc.: 75.00%] [G loss: 10.877999]\n",
      "209 [D loss: 0.321824, acc.: 89.06%] [G loss: 8.013638]\n",
      "210 [D loss: 0.189853, acc.: 92.19%] [G loss: 8.442018]\n",
      "211 [D loss: 0.226743, acc.: 92.19%] [G loss: 5.187932]\n",
      "212 [D loss: 0.137008, acc.: 98.44%] [G loss: 8.639790]\n",
      "213 [D loss: 0.398397, acc.: 76.56%] [G loss: 6.778432]\n",
      "214 [D loss: 0.155111, acc.: 95.31%] [G loss: 9.603539]\n",
      "215 [D loss: 0.272965, acc.: 92.19%] [G loss: 7.090207]\n",
      "216 [D loss: 0.218138, acc.: 92.19%] [G loss: 7.905400]\n",
      "217 [D loss: 0.227474, acc.: 93.75%] [G loss: 8.408382]\n",
      "218 [D loss: 0.263251, acc.: 93.75%] [G loss: 5.592786]\n",
      "219 [D loss: 0.127163, acc.: 95.31%] [G loss: 13.002320]\n",
      "220 [D loss: 1.408639, acc.: 45.31%] [G loss: 2.382607]\n",
      "221 [D loss: 0.157590, acc.: 95.31%] [G loss: 11.209398]\n",
      "222 [D loss: 0.351210, acc.: 84.38%] [G loss: 6.773476]\n",
      "223 [D loss: 0.165149, acc.: 96.88%] [G loss: 7.421873]\n",
      "224 [D loss: 0.186954, acc.: 95.31%] [G loss: 5.323962]\n",
      "225 [D loss: 0.102139, acc.: 100.00%] [G loss: 8.536448]\n",
      "226 [D loss: 0.524318, acc.: 75.00%] [G loss: 5.339357]\n",
      "227 [D loss: 0.100240, acc.: 100.00%] [G loss: 10.429983]\n",
      "228 [D loss: 0.840931, acc.: 54.69%] [G loss: 3.565397]\n",
      "229 [D loss: 0.095418, acc.: 98.44%] [G loss: 13.922218]\n",
      "230 [D loss: 1.555782, acc.: 32.81%] [G loss: 1.397991]\n",
      "231 [D loss: 0.329847, acc.: 78.12%] [G loss: 5.938415]\n",
      "232 [D loss: 0.189539, acc.: 96.88%] [G loss: 7.433211]\n",
      "233 [D loss: 0.309422, acc.: 92.19%] [G loss: 3.176906]\n",
      "234 [D loss: 0.290285, acc.: 85.94%] [G loss: 5.336844]\n",
      "235 [D loss: 0.208894, acc.: 93.75%] [G loss: 5.848729]\n",
      "236 [D loss: 0.215046, acc.: 95.31%] [G loss: 3.578485]\n",
      "237 [D loss: 0.207012, acc.: 90.62%] [G loss: 5.866273]\n",
      "238 [D loss: 0.287064, acc.: 85.94%] [G loss: 5.551039]\n",
      "239 [D loss: 0.399774, acc.: 84.38%] [G loss: 5.301607]\n",
      "240 [D loss: 0.408841, acc.: 85.94%] [G loss: 5.546309]\n",
      "241 [D loss: 0.427042, acc.: 78.12%] [G loss: 4.099936]\n",
      "242 [D loss: 0.258210, acc.: 90.62%] [G loss: 6.265836]\n",
      "243 [D loss: 0.379358, acc.: 85.94%] [G loss: 3.979260]\n",
      "244 [D loss: 0.445394, acc.: 78.12%] [G loss: 6.546673]\n",
      "245 [D loss: 0.557050, acc.: 79.69%] [G loss: 10.828305]\n",
      "246 [D loss: 1.307066, acc.: 32.81%] [G loss: 0.676337]\n",
      "247 [D loss: 0.345036, acc.: 75.00%] [G loss: 6.818737]\n",
      "248 [D loss: 0.666179, acc.: 68.75%] [G loss: 1.972188]\n",
      "249 [D loss: 0.335288, acc.: 84.38%] [G loss: 6.373271]\n",
      "250 [D loss: 0.699438, acc.: 59.38%] [G loss: 2.323565]\n",
      "251 [D loss: 0.150127, acc.: 100.00%] [G loss: 5.182305]\n",
      "252 [D loss: 0.345589, acc.: 89.06%] [G loss: 3.103797]\n",
      "253 [D loss: 0.339550, acc.: 84.38%] [G loss: 4.385113]\n",
      "254 [D loss: 0.526426, acc.: 68.75%] [G loss: 4.983276]\n",
      "255 [D loss: 0.574853, acc.: 73.44%] [G loss: 3.902282]\n",
      "256 [D loss: 0.501097, acc.: 78.12%] [G loss: 4.505969]\n",
      "257 [D loss: 0.631817, acc.: 62.50%] [G loss: 4.443831]\n",
      "258 [D loss: 0.838698, acc.: 45.31%] [G loss: 1.395479]\n",
      "259 [D loss: 0.225768, acc.: 92.19%] [G loss: 5.810937]\n",
      "260 [D loss: 0.772183, acc.: 54.69%] [G loss: 2.229628]\n",
      "261 [D loss: 0.357969, acc.: 84.38%] [G loss: 5.400222]\n",
      "262 [D loss: 0.682146, acc.: 62.50%] [G loss: 1.118057]\n",
      "263 [D loss: 0.289018, acc.: 85.94%] [G loss: 4.424412]\n",
      "264 [D loss: 0.643099, acc.: 64.06%] [G loss: 2.447628]\n",
      "265 [D loss: 0.465073, acc.: 73.44%] [G loss: 3.819864]\n",
      "266 [D loss: 0.747186, acc.: 53.12%] [G loss: 0.885841]\n",
      "267 [D loss: 0.324177, acc.: 85.94%] [G loss: 4.770063]\n",
      "268 [D loss: 1.154308, acc.: 31.25%] [G loss: 0.407162]\n",
      "269 [D loss: 0.425924, acc.: 75.00%] [G loss: 3.811948]\n",
      "270 [D loss: 0.724446, acc.: 56.25%] [G loss: 0.818052]\n",
      "271 [D loss: 0.410119, acc.: 82.81%] [G loss: 3.718161]\n",
      "272 [D loss: 0.521706, acc.: 73.44%] [G loss: 1.810121]\n",
      "273 [D loss: 0.510942, acc.: 73.44%] [G loss: 3.471940]\n",
      "274 [D loss: 0.614812, acc.: 65.62%] [G loss: 1.707341]\n",
      "275 [D loss: 0.375432, acc.: 84.38%] [G loss: 2.948337]\n",
      "276 [D loss: 0.636458, acc.: 57.81%] [G loss: 1.524755]\n",
      "277 [D loss: 0.338229, acc.: 89.06%] [G loss: 3.413797]\n",
      "278 [D loss: 0.642039, acc.: 71.88%] [G loss: 1.528779]\n",
      "279 [D loss: 0.399537, acc.: 76.56%] [G loss: 2.720434]\n",
      "280 [D loss: 0.581238, acc.: 67.19%] [G loss: 1.947658]\n",
      "281 [D loss: 0.484213, acc.: 75.00%] [G loss: 2.328197]\n",
      "282 [D loss: 0.748081, acc.: 57.81%] [G loss: 1.059523]\n",
      "283 [D loss: 0.570723, acc.: 62.50%] [G loss: 3.060928]\n",
      "284 [D loss: 0.677007, acc.: 60.94%] [G loss: 1.551522]\n",
      "285 [D loss: 0.445458, acc.: 79.69%] [G loss: 1.371114]\n",
      "286 [D loss: 0.653832, acc.: 59.38%] [G loss: 1.260503]\n",
      "287 [D loss: 0.466231, acc.: 76.56%] [G loss: 3.302941]\n",
      "288 [D loss: 1.012879, acc.: 40.62%] [G loss: 0.586550]\n",
      "289 [D loss: 0.357807, acc.: 84.38%] [G loss: 2.634133]\n",
      "290 [D loss: 0.796676, acc.: 48.44%] [G loss: 0.698176]\n",
      "291 [D loss: 0.449739, acc.: 81.25%] [G loss: 1.903242]\n",
      "292 [D loss: 0.666947, acc.: 62.50%] [G loss: 0.531004]\n",
      "293 [D loss: 0.656593, acc.: 56.25%] [G loss: 1.909891]\n",
      "294 [D loss: 0.508544, acc.: 81.25%] [G loss: 1.683270]\n",
      "295 [D loss: 0.603859, acc.: 62.50%] [G loss: 0.833466]\n",
      "296 [D loss: 0.566649, acc.: 70.31%] [G loss: 1.447133]\n",
      "297 [D loss: 0.543862, acc.: 76.56%] [G loss: 1.429428]\n",
      "298 [D loss: 0.525446, acc.: 78.12%] [G loss: 2.311062]\n",
      "299 [D loss: 0.736116, acc.: 51.56%] [G loss: 0.603411]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bgan = BGAN()\n",
    "    bgan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a134c9e-cbbb-423d-a698-a0247edabee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bed2f-f992-4a84-92f0-c7f062b3ae04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
