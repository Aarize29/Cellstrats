{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f6b280",
   "metadata": {
    "id": "86f6b280"
   },
   "source": [
    "# CellStrat Hub Pack - Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10b460",
   "metadata": {
    "id": "4f10b460"
   },
   "source": [
    "#### GAN 5 - **Coupled generative adversarial networks**(**CoGAN**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d445b5-54ca-448a-b1f2-ba9250eef8ab",
   "metadata": {},
   "source": [
    "##### Kernel : Tensorflow 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9abdb4d",
   "metadata": {
    "id": "c9abdb4d",
    "outputId": "2112c390-b2b6-4c97-87a1-78ba96ea6e2d"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Importing required dependencies\n",
    "#==============================================================================\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c9d90f9",
   "metadata": {
    "id": "8c9d90f9"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Create a class\n",
    "#==============================================================================\n",
    "\n",
    "class COGAN():\n",
    "    \"\"\"Reference: https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/\"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.d1, self.d2 = self.build_discriminators()\n",
    "        self.d1.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d2.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.g1, self.g2 = self.build_generators()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img1 = self.g1(z)\n",
    "        img2 = self.g2(z)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d1.trainable = False\n",
    "        self.d2.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid1 = self.d1(img1)\n",
    "        valid2 = self.d2(img2)\n",
    "\n",
    "        # The combined model  (stacked generators and discriminators)\n",
    "        # Trains generators to fool discriminators\n",
    "        self.combined = Model(z, [valid1, valid2])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "                                    optimizer=optimizer)\n",
    "\n",
    "    def build_generators(self):\n",
    "\n",
    "        # Shared weights between generators\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        feature_repr = model(noise)\n",
    "\n",
    "        # Generator 1\n",
    "        g1 = Dense(1024)(feature_repr)\n",
    "        g1 = LeakyReLU(alpha=0.2)(g1)\n",
    "        g1 = BatchNormalization(momentum=0.8)(g1)\n",
    "        g1 = Dense(np.prod(self.img_shape), activation='tanh')(g1)\n",
    "        img1 = Reshape(self.img_shape)(g1)\n",
    "\n",
    "        # Generator 2\n",
    "        g2 = Dense(1024)(feature_repr)\n",
    "        g2 = LeakyReLU(alpha=0.2)(g2)\n",
    "        g2 = BatchNormalization(momentum=0.8)(g2)\n",
    "        g2 = Dense(np.prod(self.img_shape), activation='tanh')(g2)\n",
    "        img2 = Reshape(self.img_shape)(g2)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model(noise, img1), Model(noise, img2)\n",
    "\n",
    "    def build_discriminators(self):\n",
    "\n",
    "        img1 = Input(shape=self.img_shape)\n",
    "        img2 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Shared discriminator layers\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        img1_embedding = model(img1)\n",
    "        img2_embedding = model(img2)\n",
    "\n",
    "        # Discriminator 1\n",
    "        validity1 = Dense(1, activation='sigmoid')(img1_embedding)\n",
    "        # Discriminator 2\n",
    "        validity2 = Dense(1, activation='sigmoid')(img2_embedding)\n",
    "\n",
    "        return Model(img1, validity1), Model(img2, validity2)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Images in domain A and B (rotated)\n",
    "        X1 = X_train[:int(X_train.shape[0]/2)]\n",
    "        X2 = X_train[int(X_train.shape[0]/2):]\n",
    "        X2 = scipy.ndimage.interpolation.rotate(X2, 90, axes=(1, 2))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminators\n",
    "            # ----------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X1.shape[0], batch_size)\n",
    "            imgs1 = X1[idx]\n",
    "            imgs2 = X2[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs1 = self.g1.predict(noise)\n",
    "            gen_imgs2 = self.g2.predict(noise)\n",
    "\n",
    "            # Train the discriminators\n",
    "            d1_loss_real = self.d1.train_on_batch(imgs1, valid)\n",
    "            d2_loss_real = self.d2.train_on_batch(imgs2, valid)\n",
    "            d1_loss_fake = self.d1.train_on_batch(gen_imgs1, fake)\n",
    "            d2_loss_fake = self.d2.train_on_batch(gen_imgs2, fake)\n",
    "            d1_loss = 0.5 * np.add(d1_loss_real, d1_loss_fake)\n",
    "            d2_loss = 0.5 * np.add(d2_loss_real, d2_loss_fake)\n",
    "\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, [valid, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D1 loss: %f, acc.: %.2f%%] [D2 loss: %f, acc.: %.2f%%] [G loss: %f]\" \\\n",
    "                % (epoch, d1_loss[0], 100*d1_loss[1], d2_loss[0], 100*d2_loss[1], g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "\n",
    "    # Save the generated images in images folder\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 4, 4\n",
    "        noise = np.random.normal(0, 1, (r * int(c/2), 100))\n",
    "        gen_imgs1 = self.g1.predict(noise)\n",
    "        gen_imgs2 = self.g2.predict(noise)\n",
    "\n",
    "        gen_imgs = np.concatenate([gen_imgs1, gen_imgs2])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca5afe9",
   "metadata": {
    "id": "dca5afe9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 03:57:27.324538: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-01 03:57:27.324743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-01 03:57:27.443985: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 160,512\n",
      "Trainable params: 158,976\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 03:57:33.059826: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-01 03:57:33.182714: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D1 loss: 0.753675, acc.: 35.94%] [D2 loss: 0.819976, acc.: 45.31%] [G loss: 1.707760]\n",
      "1 [D1 loss: 0.362213, acc.: 82.81%] [D2 loss: 0.373142, acc.: 92.19%] [G loss: 1.728586]\n",
      "2 [D1 loss: 0.350254, acc.: 78.12%] [D2 loss: 0.323505, acc.: 93.75%] [G loss: 1.821468]\n",
      "3 [D1 loss: 0.328791, acc.: 90.62%] [D2 loss: 0.323233, acc.: 90.62%] [G loss: 1.974193]\n",
      "4 [D1 loss: 0.308250, acc.: 89.06%] [D2 loss: 0.250967, acc.: 98.44%] [G loss: 2.185667]\n",
      "5 [D1 loss: 0.269047, acc.: 95.31%] [D2 loss: 0.219118, acc.: 96.88%] [G loss: 2.552810]\n",
      "6 [D1 loss: 0.241248, acc.: 93.75%] [D2 loss: 0.198905, acc.: 100.00%] [G loss: 2.834114]\n",
      "7 [D1 loss: 0.179283, acc.: 100.00%] [D2 loss: 0.163067, acc.: 100.00%] [G loss: 3.072896]\n",
      "8 [D1 loss: 0.156394, acc.: 100.00%] [D2 loss: 0.140250, acc.: 100.00%] [G loss: 3.506333]\n",
      "9 [D1 loss: 0.127664, acc.: 100.00%] [D2 loss: 0.120300, acc.: 100.00%] [G loss: 3.589766]\n",
      "10 [D1 loss: 0.140188, acc.: 100.00%] [D2 loss: 0.116139, acc.: 100.00%] [G loss: 3.797857]\n",
      "11 [D1 loss: 0.107548, acc.: 100.00%] [D2 loss: 0.082636, acc.: 100.00%] [G loss: 4.225275]\n",
      "12 [D1 loss: 0.085711, acc.: 100.00%] [D2 loss: 0.080081, acc.: 100.00%] [G loss: 4.271984]\n",
      "13 [D1 loss: 0.089491, acc.: 100.00%] [D2 loss: 0.079034, acc.: 100.00%] [G loss: 4.486791]\n",
      "14 [D1 loss: 0.064391, acc.: 100.00%] [D2 loss: 0.066110, acc.: 100.00%] [G loss: 4.750237]\n",
      "15 [D1 loss: 0.065479, acc.: 100.00%] [D2 loss: 0.057401, acc.: 100.00%] [G loss: 4.752402]\n",
      "16 [D1 loss: 0.061019, acc.: 100.00%] [D2 loss: 0.066074, acc.: 100.00%] [G loss: 4.957043]\n",
      "17 [D1 loss: 0.073665, acc.: 100.00%] [D2 loss: 0.056778, acc.: 100.00%] [G loss: 5.167782]\n",
      "18 [D1 loss: 0.048663, acc.: 100.00%] [D2 loss: 0.042315, acc.: 100.00%] [G loss: 5.275133]\n",
      "19 [D1 loss: 0.050868, acc.: 100.00%] [D2 loss: 0.055903, acc.: 100.00%] [G loss: 5.495211]\n",
      "20 [D1 loss: 0.052399, acc.: 100.00%] [D2 loss: 0.044416, acc.: 100.00%] [G loss: 5.771233]\n",
      "21 [D1 loss: 0.042393, acc.: 100.00%] [D2 loss: 0.042364, acc.: 100.00%] [G loss: 5.737581]\n",
      "22 [D1 loss: 0.027414, acc.: 100.00%] [D2 loss: 0.031863, acc.: 100.00%] [G loss: 5.935563]\n",
      "23 [D1 loss: 0.044003, acc.: 100.00%] [D2 loss: 0.040097, acc.: 100.00%] [G loss: 6.182136]\n",
      "24 [D1 loss: 0.028293, acc.: 100.00%] [D2 loss: 0.028490, acc.: 100.00%] [G loss: 6.175185]\n",
      "25 [D1 loss: 0.045958, acc.: 100.00%] [D2 loss: 0.029701, acc.: 100.00%] [G loss: 6.161335]\n",
      "26 [D1 loss: 0.029320, acc.: 100.00%] [D2 loss: 0.028345, acc.: 100.00%] [G loss: 6.412838]\n",
      "27 [D1 loss: 0.035349, acc.: 100.00%] [D2 loss: 0.026961, acc.: 100.00%] [G loss: 6.343731]\n",
      "28 [D1 loss: 0.026313, acc.: 100.00%] [D2 loss: 0.029918, acc.: 100.00%] [G loss: 6.348467]\n",
      "29 [D1 loss: 0.038143, acc.: 100.00%] [D2 loss: 0.038514, acc.: 100.00%] [G loss: 6.519670]\n",
      "30 [D1 loss: 0.036804, acc.: 100.00%] [D2 loss: 0.022205, acc.: 100.00%] [G loss: 6.787311]\n",
      "31 [D1 loss: 0.023619, acc.: 100.00%] [D2 loss: 0.020774, acc.: 100.00%] [G loss: 6.841180]\n",
      "32 [D1 loss: 0.017700, acc.: 100.00%] [D2 loss: 0.021967, acc.: 100.00%] [G loss: 6.810092]\n",
      "33 [D1 loss: 0.022718, acc.: 100.00%] [D2 loss: 0.025352, acc.: 100.00%] [G loss: 6.839010]\n",
      "34 [D1 loss: 0.024248, acc.: 100.00%] [D2 loss: 0.026723, acc.: 100.00%] [G loss: 6.983054]\n",
      "35 [D1 loss: 0.026172, acc.: 100.00%] [D2 loss: 0.022470, acc.: 100.00%] [G loss: 7.060945]\n",
      "36 [D1 loss: 0.027848, acc.: 100.00%] [D2 loss: 0.029495, acc.: 100.00%] [G loss: 7.259946]\n",
      "37 [D1 loss: 0.023956, acc.: 100.00%] [D2 loss: 0.017664, acc.: 100.00%] [G loss: 7.461576]\n",
      "38 [D1 loss: 0.020186, acc.: 100.00%] [D2 loss: 0.016728, acc.: 100.00%] [G loss: 7.154939]\n",
      "39 [D1 loss: 0.021581, acc.: 100.00%] [D2 loss: 0.019487, acc.: 100.00%] [G loss: 7.238251]\n",
      "40 [D1 loss: 0.020361, acc.: 100.00%] [D2 loss: 0.014341, acc.: 100.00%] [G loss: 7.362292]\n",
      "41 [D1 loss: 0.017200, acc.: 100.00%] [D2 loss: 0.013262, acc.: 100.00%] [G loss: 7.484697]\n",
      "42 [D1 loss: 0.028855, acc.: 100.00%] [D2 loss: 0.019644, acc.: 100.00%] [G loss: 7.552521]\n",
      "43 [D1 loss: 0.024551, acc.: 100.00%] [D2 loss: 0.015639, acc.: 100.00%] [G loss: 7.449041]\n",
      "44 [D1 loss: 0.019261, acc.: 100.00%] [D2 loss: 0.019347, acc.: 100.00%] [G loss: 7.628687]\n",
      "45 [D1 loss: 0.017092, acc.: 100.00%] [D2 loss: 0.017475, acc.: 100.00%] [G loss: 7.432690]\n",
      "46 [D1 loss: 0.020043, acc.: 100.00%] [D2 loss: 0.027376, acc.: 100.00%] [G loss: 7.646382]\n",
      "47 [D1 loss: 0.025032, acc.: 100.00%] [D2 loss: 0.021749, acc.: 100.00%] [G loss: 8.030785]\n",
      "48 [D1 loss: 0.021009, acc.: 100.00%] [D2 loss: 0.019839, acc.: 100.00%] [G loss: 7.999035]\n",
      "49 [D1 loss: 0.018440, acc.: 100.00%] [D2 loss: 0.013556, acc.: 100.00%] [G loss: 7.960824]\n",
      "50 [D1 loss: 0.026164, acc.: 100.00%] [D2 loss: 0.019885, acc.: 100.00%] [G loss: 7.964123]\n",
      "51 [D1 loss: 0.017524, acc.: 100.00%] [D2 loss: 0.027359, acc.: 100.00%] [G loss: 7.978877]\n",
      "52 [D1 loss: 0.017567, acc.: 100.00%] [D2 loss: 0.017740, acc.: 100.00%] [G loss: 8.465276]\n",
      "53 [D1 loss: 0.014231, acc.: 100.00%] [D2 loss: 0.020436, acc.: 100.00%] [G loss: 8.314921]\n",
      "54 [D1 loss: 0.011080, acc.: 100.00%] [D2 loss: 0.012214, acc.: 100.00%] [G loss: 8.000458]\n",
      "55 [D1 loss: 0.018835, acc.: 100.00%] [D2 loss: 0.013147, acc.: 100.00%] [G loss: 8.367229]\n",
      "56 [D1 loss: 0.020410, acc.: 100.00%] [D2 loss: 0.016222, acc.: 100.00%] [G loss: 8.267006]\n",
      "57 [D1 loss: 0.011030, acc.: 100.00%] [D2 loss: 0.010715, acc.: 100.00%] [G loss: 8.304230]\n",
      "58 [D1 loss: 0.017895, acc.: 100.00%] [D2 loss: 0.014637, acc.: 100.00%] [G loss: 8.418303]\n",
      "59 [D1 loss: 0.028531, acc.: 100.00%] [D2 loss: 0.026202, acc.: 100.00%] [G loss: 8.985517]\n",
      "60 [D1 loss: 0.020435, acc.: 100.00%] [D2 loss: 0.018539, acc.: 100.00%] [G loss: 8.481077]\n",
      "61 [D1 loss: 0.010495, acc.: 100.00%] [D2 loss: 0.011287, acc.: 100.00%] [G loss: 8.570694]\n",
      "62 [D1 loss: 0.018071, acc.: 100.00%] [D2 loss: 0.017374, acc.: 100.00%] [G loss: 8.612708]\n",
      "63 [D1 loss: 0.014363, acc.: 100.00%] [D2 loss: 0.013906, acc.: 100.00%] [G loss: 8.124869]\n",
      "64 [D1 loss: 0.014368, acc.: 100.00%] [D2 loss: 0.019090, acc.: 100.00%] [G loss: 8.584108]\n",
      "65 [D1 loss: 0.017734, acc.: 100.00%] [D2 loss: 0.025494, acc.: 100.00%] [G loss: 8.663509]\n",
      "66 [D1 loss: 0.012185, acc.: 100.00%] [D2 loss: 0.014260, acc.: 100.00%] [G loss: 8.483387]\n",
      "67 [D1 loss: 0.018897, acc.: 100.00%] [D2 loss: 0.018501, acc.: 100.00%] [G loss: 8.921479]\n",
      "68 [D1 loss: 0.023510, acc.: 100.00%] [D2 loss: 0.023872, acc.: 100.00%] [G loss: 9.287201]\n",
      "69 [D1 loss: 0.016299, acc.: 100.00%] [D2 loss: 0.014160, acc.: 100.00%] [G loss: 8.894754]\n",
      "70 [D1 loss: 0.012840, acc.: 100.00%] [D2 loss: 0.012962, acc.: 100.00%] [G loss: 8.525279]\n",
      "71 [D1 loss: 0.018251, acc.: 100.00%] [D2 loss: 0.029897, acc.: 100.00%] [G loss: 8.940072]\n",
      "72 [D1 loss: 0.023439, acc.: 100.00%] [D2 loss: 0.017501, acc.: 100.00%] [G loss: 9.649380]\n",
      "73 [D1 loss: 0.021969, acc.: 100.00%] [D2 loss: 0.016213, acc.: 100.00%] [G loss: 9.232473]\n",
      "74 [D1 loss: 0.016541, acc.: 100.00%] [D2 loss: 0.018929, acc.: 100.00%] [G loss: 9.491085]\n",
      "75 [D1 loss: 0.012774, acc.: 100.00%] [D2 loss: 0.011884, acc.: 100.00%] [G loss: 9.634909]\n",
      "76 [D1 loss: 0.019661, acc.: 100.00%] [D2 loss: 0.016510, acc.: 100.00%] [G loss: 9.637837]\n",
      "77 [D1 loss: 0.025486, acc.: 100.00%] [D2 loss: 0.015419, acc.: 100.00%] [G loss: 10.021584]\n",
      "78 [D1 loss: 0.059340, acc.: 98.44%] [D2 loss: 0.039288, acc.: 98.44%] [G loss: 9.302749]\n",
      "79 [D1 loss: 0.027633, acc.: 100.00%] [D2 loss: 0.025663, acc.: 100.00%] [G loss: 10.248354]\n",
      "80 [D1 loss: 0.048883, acc.: 100.00%] [D2 loss: 0.035671, acc.: 98.44%] [G loss: 9.899477]\n",
      "81 [D1 loss: 0.011629, acc.: 100.00%] [D2 loss: 0.047987, acc.: 98.44%] [G loss: 10.577082]\n",
      "82 [D1 loss: 0.238177, acc.: 85.94%] [D2 loss: 0.131417, acc.: 98.44%] [G loss: 10.116742]\n",
      "83 [D1 loss: 0.033388, acc.: 100.00%] [D2 loss: 0.028884, acc.: 100.00%] [G loss: 11.037337]\n",
      "84 [D1 loss: 0.237899, acc.: 85.94%] [D2 loss: 0.125207, acc.: 90.62%] [G loss: 12.386800]\n",
      "85 [D1 loss: 0.034844, acc.: 100.00%] [D2 loss: 0.014693, acc.: 100.00%] [G loss: 12.370495]\n",
      "86 [D1 loss: 0.068344, acc.: 95.31%] [D2 loss: 0.014740, acc.: 100.00%] [G loss: 13.032537]\n",
      "87 [D1 loss: 0.895038, acc.: 70.31%] [D2 loss: 0.257756, acc.: 89.06%] [G loss: 9.562287]\n",
      "88 [D1 loss: 0.294501, acc.: 89.06%] [D2 loss: 0.180600, acc.: 90.62%] [G loss: 10.081910]\n",
      "89 [D1 loss: 0.171231, acc.: 92.19%] [D2 loss: 0.063304, acc.: 96.88%] [G loss: 10.322189]\n",
      "90 [D1 loss: 0.067319, acc.: 96.88%] [D2 loss: 0.047644, acc.: 100.00%] [G loss: 10.979484]\n",
      "91 [D1 loss: 0.055529, acc.: 98.44%] [D2 loss: 0.168558, acc.: 92.19%] [G loss: 8.979266]\n",
      "92 [D1 loss: 0.085555, acc.: 93.75%] [D2 loss: 0.079159, acc.: 95.31%] [G loss: 10.734599]\n",
      "93 [D1 loss: 0.015426, acc.: 100.00%] [D2 loss: 0.040482, acc.: 100.00%] [G loss: 9.991669]\n",
      "94 [D1 loss: 0.030288, acc.: 98.44%] [D2 loss: 0.024208, acc.: 100.00%] [G loss: 9.127685]\n",
      "95 [D1 loss: 0.035802, acc.: 100.00%] [D2 loss: 0.065655, acc.: 96.88%] [G loss: 8.754670]\n",
      "96 [D1 loss: 0.162830, acc.: 93.75%] [D2 loss: 0.063851, acc.: 98.44%] [G loss: 9.589921]\n",
      "97 [D1 loss: 0.179854, acc.: 93.75%] [D2 loss: 0.102975, acc.: 95.31%] [G loss: 9.583618]\n",
      "98 [D1 loss: 0.118358, acc.: 93.75%] [D2 loss: 0.096035, acc.: 96.88%] [G loss: 9.944675]\n",
      "99 [D1 loss: 0.056919, acc.: 100.00%] [D2 loss: 0.035110, acc.: 100.00%] [G loss: 9.619820]\n",
      "100 [D1 loss: 0.287101, acc.: 87.50%] [D2 loss: 0.077111, acc.: 95.31%] [G loss: 9.606241]\n",
      "101 [D1 loss: 0.133511, acc.: 92.19%] [D2 loss: 0.094052, acc.: 96.88%] [G loss: 10.412987]\n",
      "102 [D1 loss: 0.421034, acc.: 82.81%] [D2 loss: 0.060322, acc.: 100.00%] [G loss: 8.799786]\n",
      "103 [D1 loss: 0.118623, acc.: 96.88%] [D2 loss: 0.153569, acc.: 93.75%] [G loss: 10.519453]\n",
      "104 [D1 loss: 0.225291, acc.: 90.62%] [D2 loss: 0.299482, acc.: 84.38%] [G loss: 11.448125]\n",
      "105 [D1 loss: 0.243700, acc.: 85.94%] [D2 loss: 0.060992, acc.: 98.44%] [G loss: 10.532372]\n",
      "106 [D1 loss: 0.040583, acc.: 100.00%] [D2 loss: 0.062761, acc.: 100.00%] [G loss: 9.387703]\n",
      "107 [D1 loss: 0.067023, acc.: 100.00%] [D2 loss: 0.095456, acc.: 96.88%] [G loss: 9.988159]\n",
      "108 [D1 loss: 0.285763, acc.: 85.94%] [D2 loss: 0.127136, acc.: 93.75%] [G loss: 8.727362]\n",
      "109 [D1 loss: 0.098946, acc.: 93.75%] [D2 loss: 0.204405, acc.: 90.62%] [G loss: 8.341307]\n",
      "110 [D1 loss: 0.115057, acc.: 92.19%] [D2 loss: 0.059567, acc.: 98.44%] [G loss: 10.934969]\n",
      "111 [D1 loss: 0.639971, acc.: 75.00%] [D2 loss: 0.208814, acc.: 87.50%] [G loss: 9.957970]\n",
      "112 [D1 loss: 0.665785, acc.: 68.75%] [D2 loss: 1.002470, acc.: 59.38%] [G loss: 6.924878]\n",
      "113 [D1 loss: 0.158877, acc.: 95.31%] [D2 loss: 0.393031, acc.: 87.50%] [G loss: 8.986902]\n",
      "114 [D1 loss: 0.059831, acc.: 100.00%] [D2 loss: 0.107904, acc.: 96.88%] [G loss: 9.373447]\n",
      "115 [D1 loss: 0.136948, acc.: 98.44%] [D2 loss: 0.046567, acc.: 98.44%] [G loss: 8.889773]\n",
      "116 [D1 loss: 0.160228, acc.: 90.62%] [D2 loss: 0.061384, acc.: 98.44%] [G loss: 9.317030]\n",
      "117 [D1 loss: 0.174283, acc.: 96.88%] [D2 loss: 0.097042, acc.: 96.88%] [G loss: 8.463496]\n",
      "118 [D1 loss: 0.185041, acc.: 93.75%] [D2 loss: 0.109569, acc.: 95.31%] [G loss: 8.304495]\n",
      "119 [D1 loss: 0.114791, acc.: 93.75%] [D2 loss: 0.278934, acc.: 87.50%] [G loss: 7.969542]\n",
      "120 [D1 loss: 0.118641, acc.: 96.88%] [D2 loss: 0.129679, acc.: 96.88%] [G loss: 8.655815]\n",
      "121 [D1 loss: 0.376183, acc.: 84.38%] [D2 loss: 0.141370, acc.: 93.75%] [G loss: 9.482409]\n",
      "122 [D1 loss: 0.519948, acc.: 78.12%] [D2 loss: 0.185227, acc.: 90.62%] [G loss: 8.084254]\n",
      "123 [D1 loss: 0.262472, acc.: 89.06%] [D2 loss: 0.448260, acc.: 78.12%] [G loss: 6.779038]\n",
      "124 [D1 loss: 0.111781, acc.: 95.31%] [D2 loss: 0.096769, acc.: 96.88%] [G loss: 9.050151]\n",
      "125 [D1 loss: 0.513224, acc.: 76.56%] [D2 loss: 0.139286, acc.: 92.19%] [G loss: 7.207542]\n",
      "126 [D1 loss: 0.184247, acc.: 90.62%] [D2 loss: 0.161725, acc.: 96.88%] [G loss: 7.497184]\n",
      "127 [D1 loss: 0.248657, acc.: 87.50%] [D2 loss: 0.147072, acc.: 95.31%] [G loss: 9.252923]\n",
      "128 [D1 loss: 1.354818, acc.: 43.75%] [D2 loss: 0.679706, acc.: 73.44%] [G loss: 5.049404]\n",
      "129 [D1 loss: 0.423371, acc.: 81.25%] [D2 loss: 0.150680, acc.: 90.62%] [G loss: 6.560732]\n",
      "130 [D1 loss: 0.175908, acc.: 89.06%] [D2 loss: 0.205922, acc.: 93.75%] [G loss: 7.573756]\n",
      "131 [D1 loss: 0.101290, acc.: 96.88%] [D2 loss: 0.178759, acc.: 93.75%] [G loss: 7.668417]\n",
      "132 [D1 loss: 0.130022, acc.: 98.44%] [D2 loss: 0.203584, acc.: 92.19%] [G loss: 6.993935]\n",
      "133 [D1 loss: 0.127974, acc.: 95.31%] [D2 loss: 0.221794, acc.: 89.06%] [G loss: 6.564498]\n",
      "134 [D1 loss: 0.253574, acc.: 85.94%] [D2 loss: 0.290370, acc.: 82.81%] [G loss: 8.205032]\n",
      "135 [D1 loss: 0.489447, acc.: 75.00%] [D2 loss: 0.267170, acc.: 85.94%] [G loss: 6.325092]\n",
      "136 [D1 loss: 0.207160, acc.: 87.50%] [D2 loss: 0.282645, acc.: 92.19%] [G loss: 6.842332]\n",
      "137 [D1 loss: 0.150194, acc.: 92.19%] [D2 loss: 0.215826, acc.: 92.19%] [G loss: 8.035906]\n",
      "138 [D1 loss: 1.098010, acc.: 53.12%] [D2 loss: 0.443171, acc.: 84.38%] [G loss: 4.988139]\n",
      "139 [D1 loss: 0.542683, acc.: 71.88%] [D2 loss: 0.262000, acc.: 85.94%] [G loss: 7.014099]\n",
      "140 [D1 loss: 0.603805, acc.: 71.88%] [D2 loss: 1.552349, acc.: 29.69%] [G loss: 4.103893]\n",
      "141 [D1 loss: 0.252556, acc.: 84.38%] [D2 loss: 0.348030, acc.: 82.81%] [G loss: 6.223690]\n",
      "142 [D1 loss: 0.162722, acc.: 96.88%] [D2 loss: 0.232440, acc.: 85.94%] [G loss: 7.315987]\n",
      "143 [D1 loss: 0.164207, acc.: 98.44%] [D2 loss: 0.131726, acc.: 96.88%] [G loss: 7.332136]\n",
      "144 [D1 loss: 0.261700, acc.: 87.50%] [D2 loss: 0.196215, acc.: 92.19%] [G loss: 7.245538]\n",
      "145 [D1 loss: 0.339422, acc.: 87.50%] [D2 loss: 0.262144, acc.: 87.50%] [G loss: 6.226754]\n",
      "146 [D1 loss: 0.217326, acc.: 90.62%] [D2 loss: 0.256714, acc.: 89.06%] [G loss: 6.379917]\n",
      "147 [D1 loss: 0.287246, acc.: 89.06%] [D2 loss: 0.260161, acc.: 89.06%] [G loss: 6.268076]\n",
      "148 [D1 loss: 0.329249, acc.: 84.38%] [D2 loss: 0.232498, acc.: 89.06%] [G loss: 7.581071]\n",
      "149 [D1 loss: 0.557521, acc.: 73.44%] [D2 loss: 0.328728, acc.: 82.81%] [G loss: 5.308410]\n",
      "150 [D1 loss: 0.312069, acc.: 84.38%] [D2 loss: 0.342497, acc.: 81.25%] [G loss: 5.747680]\n",
      "151 [D1 loss: 0.239543, acc.: 90.62%] [D2 loss: 0.183121, acc.: 95.31%] [G loss: 6.055774]\n",
      "152 [D1 loss: 0.240858, acc.: 89.06%] [D2 loss: 0.243470, acc.: 85.94%] [G loss: 6.575827]\n",
      "153 [D1 loss: 0.662042, acc.: 68.75%] [D2 loss: 0.299545, acc.: 84.38%] [G loss: 5.791898]\n",
      "154 [D1 loss: 0.455180, acc.: 76.56%] [D2 loss: 0.734141, acc.: 65.62%] [G loss: 5.001306]\n",
      "155 [D1 loss: 0.260096, acc.: 89.06%] [D2 loss: 0.232800, acc.: 89.06%] [G loss: 7.409173]\n",
      "156 [D1 loss: 1.273183, acc.: 32.81%] [D2 loss: 0.427258, acc.: 78.12%] [G loss: 4.478653]\n",
      "157 [D1 loss: 0.550294, acc.: 70.31%] [D2 loss: 0.233517, acc.: 89.06%] [G loss: 6.567355]\n",
      "158 [D1 loss: 0.360919, acc.: 82.81%] [D2 loss: 0.698615, acc.: 62.50%] [G loss: 4.793340]\n",
      "159 [D1 loss: 0.188192, acc.: 92.19%] [D2 loss: 0.186755, acc.: 90.62%] [G loss: 7.171926]\n",
      "160 [D1 loss: 0.265608, acc.: 89.06%] [D2 loss: 0.285048, acc.: 87.50%] [G loss: 6.813119]\n",
      "161 [D1 loss: 0.456923, acc.: 76.56%] [D2 loss: 0.397410, acc.: 79.69%] [G loss: 5.304740]\n",
      "162 [D1 loss: 0.221865, acc.: 90.62%] [D2 loss: 0.275646, acc.: 87.50%] [G loss: 5.416384]\n",
      "163 [D1 loss: 0.408061, acc.: 76.56%] [D2 loss: 0.328998, acc.: 81.25%] [G loss: 6.435408]\n",
      "164 [D1 loss: 0.457238, acc.: 78.12%] [D2 loss: 0.411734, acc.: 84.38%] [G loss: 5.052034]\n",
      "165 [D1 loss: 0.351046, acc.: 79.69%] [D2 loss: 0.323780, acc.: 82.81%] [G loss: 6.502790]\n",
      "166 [D1 loss: 0.746987, acc.: 59.38%] [D2 loss: 0.410577, acc.: 79.69%] [G loss: 4.010555]\n",
      "167 [D1 loss: 0.329332, acc.: 82.81%] [D2 loss: 0.220410, acc.: 93.75%] [G loss: 7.320650]\n",
      "168 [D1 loss: 0.410138, acc.: 76.56%] [D2 loss: 0.591974, acc.: 71.88%] [G loss: 5.433869]\n",
      "169 [D1 loss: 0.247816, acc.: 89.06%] [D2 loss: 0.268438, acc.: 84.38%] [G loss: 6.500188]\n",
      "170 [D1 loss: 0.413125, acc.: 84.38%] [D2 loss: 0.332422, acc.: 85.94%] [G loss: 4.569694]\n",
      "171 [D1 loss: 0.372731, acc.: 75.00%] [D2 loss: 0.331792, acc.: 79.69%] [G loss: 7.081733]\n",
      "172 [D1 loss: 1.049330, acc.: 45.31%] [D2 loss: 0.572667, acc.: 70.31%] [G loss: 3.622285]\n",
      "173 [D1 loss: 0.283453, acc.: 82.81%] [D2 loss: 0.287348, acc.: 87.50%] [G loss: 6.117537]\n",
      "174 [D1 loss: 0.441542, acc.: 76.56%] [D2 loss: 0.505745, acc.: 78.12%] [G loss: 6.120047]\n",
      "175 [D1 loss: 0.303882, acc.: 85.94%] [D2 loss: 0.360652, acc.: 85.94%] [G loss: 5.428006]\n",
      "176 [D1 loss: 0.326184, acc.: 82.81%] [D2 loss: 0.335050, acc.: 84.38%] [G loss: 5.561133]\n",
      "177 [D1 loss: 0.439363, acc.: 76.56%] [D2 loss: 0.363552, acc.: 84.38%] [G loss: 4.688598]\n",
      "178 [D1 loss: 0.446653, acc.: 73.44%] [D2 loss: 0.525369, acc.: 76.56%] [G loss: 4.901090]\n",
      "179 [D1 loss: 0.508813, acc.: 70.31%] [D2 loss: 0.360812, acc.: 81.25%] [G loss: 5.217244]\n",
      "180 [D1 loss: 0.557044, acc.: 65.62%] [D2 loss: 0.362801, acc.: 79.69%] [G loss: 5.450180]\n",
      "181 [D1 loss: 0.571424, acc.: 68.75%] [D2 loss: 0.464636, acc.: 75.00%] [G loss: 4.947213]\n",
      "182 [D1 loss: 0.511832, acc.: 75.00%] [D2 loss: 0.331983, acc.: 87.50%] [G loss: 5.413379]\n",
      "183 [D1 loss: 0.402141, acc.: 76.56%] [D2 loss: 0.395553, acc.: 81.25%] [G loss: 5.384867]\n",
      "184 [D1 loss: 0.415528, acc.: 81.25%] [D2 loss: 0.334938, acc.: 85.94%] [G loss: 7.591566]\n",
      "185 [D1 loss: 0.600644, acc.: 70.31%] [D2 loss: 0.341662, acc.: 81.25%] [G loss: 4.772809]\n",
      "186 [D1 loss: 0.409455, acc.: 75.00%] [D2 loss: 0.421177, acc.: 75.00%] [G loss: 5.165750]\n",
      "187 [D1 loss: 0.490127, acc.: 68.75%] [D2 loss: 0.463353, acc.: 75.00%] [G loss: 4.763782]\n",
      "188 [D1 loss: 0.484173, acc.: 78.12%] [D2 loss: 0.287534, acc.: 90.62%] [G loss: 6.829777]\n",
      "189 [D1 loss: 0.685529, acc.: 60.94%] [D2 loss: 0.412994, acc.: 78.12%] [G loss: 4.397149]\n",
      "190 [D1 loss: 0.377351, acc.: 75.00%] [D2 loss: 0.509521, acc.: 75.00%] [G loss: 5.443833]\n",
      "191 [D1 loss: 0.433273, acc.: 79.69%] [D2 loss: 0.372597, acc.: 82.81%] [G loss: 5.853244]\n",
      "192 [D1 loss: 0.550178, acc.: 67.19%] [D2 loss: 0.408793, acc.: 78.12%] [G loss: 4.563765]\n",
      "193 [D1 loss: 0.457411, acc.: 70.31%] [D2 loss: 0.570166, acc.: 68.75%] [G loss: 5.563064]\n",
      "194 [D1 loss: 0.509175, acc.: 65.62%] [D2 loss: 0.436675, acc.: 79.69%] [G loss: 5.779048]\n",
      "195 [D1 loss: 0.612127, acc.: 64.06%] [D2 loss: 0.568134, acc.: 65.62%] [G loss: 5.040493]\n",
      "196 [D1 loss: 0.565893, acc.: 67.19%] [D2 loss: 0.440837, acc.: 81.25%] [G loss: 4.633391]\n",
      "197 [D1 loss: 0.413552, acc.: 76.56%] [D2 loss: 0.422088, acc.: 81.25%] [G loss: 4.984984]\n",
      "198 [D1 loss: 0.489859, acc.: 71.88%] [D2 loss: 0.410247, acc.: 79.69%] [G loss: 6.082580]\n",
      "199 [D1 loss: 0.772114, acc.: 50.00%] [D2 loss: 0.441852, acc.: 76.56%] [G loss: 4.678159]\n",
      "200 [D1 loss: 0.443677, acc.: 73.44%] [D2 loss: 0.598895, acc.: 68.75%] [G loss: 4.631617]\n",
      "201 [D1 loss: 0.468430, acc.: 71.88%] [D2 loss: 0.472453, acc.: 73.44%] [G loss: 4.521101]\n",
      "202 [D1 loss: 0.530301, acc.: 68.75%] [D2 loss: 0.498299, acc.: 73.44%] [G loss: 4.633826]\n",
      "203 [D1 loss: 0.581429, acc.: 68.75%] [D2 loss: 0.564867, acc.: 68.75%] [G loss: 4.297078]\n",
      "204 [D1 loss: 0.436981, acc.: 78.12%] [D2 loss: 0.420618, acc.: 76.56%] [G loss: 4.615036]\n",
      "205 [D1 loss: 0.462609, acc.: 73.44%] [D2 loss: 0.361277, acc.: 84.38%] [G loss: 5.138817]\n",
      "206 [D1 loss: 0.426854, acc.: 78.12%] [D2 loss: 0.368592, acc.: 81.25%] [G loss: 5.857389]\n",
      "207 [D1 loss: 0.425564, acc.: 82.81%] [D2 loss: 0.366018, acc.: 84.38%] [G loss: 6.106945]\n",
      "208 [D1 loss: 0.383992, acc.: 85.94%] [D2 loss: 0.343575, acc.: 85.94%] [G loss: 4.642488]\n",
      "209 [D1 loss: 0.484865, acc.: 71.88%] [D2 loss: 0.435522, acc.: 76.56%] [G loss: 5.957534]\n",
      "210 [D1 loss: 0.629977, acc.: 64.06%] [D2 loss: 0.389180, acc.: 79.69%] [G loss: 5.029711]\n",
      "211 [D1 loss: 0.507194, acc.: 70.31%] [D2 loss: 0.366286, acc.: 81.25%] [G loss: 7.262827]\n",
      "212 [D1 loss: 0.836255, acc.: 53.12%] [D2 loss: 0.479389, acc.: 73.44%] [G loss: 4.342811]\n",
      "213 [D1 loss: 0.470125, acc.: 71.88%] [D2 loss: 0.484209, acc.: 71.88%] [G loss: 4.853957]\n",
      "214 [D1 loss: 0.587518, acc.: 67.19%] [D2 loss: 0.537391, acc.: 71.88%] [G loss: 4.219154]\n",
      "215 [D1 loss: 0.404615, acc.: 78.12%] [D2 loss: 0.330978, acc.: 82.81%] [G loss: 6.178651]\n",
      "216 [D1 loss: 0.686964, acc.: 56.25%] [D2 loss: 0.417064, acc.: 78.12%] [G loss: 5.678059]\n",
      "217 [D1 loss: 0.656884, acc.: 64.06%] [D2 loss: 0.465432, acc.: 76.56%] [G loss: 5.853324]\n",
      "218 [D1 loss: 0.617993, acc.: 67.19%] [D2 loss: 0.530306, acc.: 73.44%] [G loss: 4.178914]\n",
      "219 [D1 loss: 0.431608, acc.: 75.00%] [D2 loss: 0.439493, acc.: 71.88%] [G loss: 3.663180]\n",
      "220 [D1 loss: 0.526135, acc.: 67.19%] [D2 loss: 0.353932, acc.: 87.50%] [G loss: 4.494261]\n",
      "221 [D1 loss: 0.592101, acc.: 59.38%] [D2 loss: 0.432815, acc.: 76.56%] [G loss: 5.304021]\n",
      "222 [D1 loss: 0.734896, acc.: 46.88%] [D2 loss: 0.419715, acc.: 84.38%] [G loss: 4.512728]\n",
      "223 [D1 loss: 0.446085, acc.: 73.44%] [D2 loss: 0.369099, acc.: 82.81%] [G loss: 4.897570]\n",
      "224 [D1 loss: 0.725873, acc.: 56.25%] [D2 loss: 0.452325, acc.: 73.44%] [G loss: 4.784670]\n",
      "225 [D1 loss: 0.688131, acc.: 51.56%] [D2 loss: 0.616120, acc.: 65.62%] [G loss: 3.818534]\n",
      "226 [D1 loss: 0.573415, acc.: 67.19%] [D2 loss: 0.497536, acc.: 73.44%] [G loss: 4.102021]\n",
      "227 [D1 loss: 0.726198, acc.: 50.00%] [D2 loss: 0.406313, acc.: 78.12%] [G loss: 5.298227]\n",
      "228 [D1 loss: 0.703271, acc.: 53.12%] [D2 loss: 0.635640, acc.: 64.06%] [G loss: 3.588104]\n",
      "229 [D1 loss: 0.557285, acc.: 64.06%] [D2 loss: 0.501921, acc.: 73.44%] [G loss: 4.615311]\n",
      "230 [D1 loss: 0.772218, acc.: 46.88%] [D2 loss: 0.530642, acc.: 70.31%] [G loss: 3.561260]\n",
      "231 [D1 loss: 0.594864, acc.: 57.81%] [D2 loss: 0.429869, acc.: 76.56%] [G loss: 4.353774]\n",
      "232 [D1 loss: 0.749377, acc.: 45.31%] [D2 loss: 0.418206, acc.: 78.12%] [G loss: 4.196362]\n",
      "233 [D1 loss: 0.705258, acc.: 57.81%] [D2 loss: 0.500559, acc.: 76.56%] [G loss: 3.334117]\n",
      "234 [D1 loss: 0.593631, acc.: 62.50%] [D2 loss: 0.496495, acc.: 78.12%] [G loss: 4.011151]\n",
      "235 [D1 loss: 0.683241, acc.: 56.25%] [D2 loss: 0.457736, acc.: 78.12%] [G loss: 4.931891]\n",
      "236 [D1 loss: 0.801169, acc.: 37.50%] [D2 loss: 0.773663, acc.: 51.56%] [G loss: 2.419466]\n",
      "237 [D1 loss: 0.589242, acc.: 53.12%] [D2 loss: 0.403152, acc.: 73.44%] [G loss: 4.509922]\n",
      "238 [D1 loss: 0.782703, acc.: 43.75%] [D2 loss: 0.596572, acc.: 59.38%] [G loss: 2.825820]\n",
      "239 [D1 loss: 0.620516, acc.: 53.12%] [D2 loss: 0.644516, acc.: 60.94%] [G loss: 2.764229]\n",
      "240 [D1 loss: 0.623878, acc.: 54.69%] [D2 loss: 0.537310, acc.: 75.00%] [G loss: 3.685181]\n",
      "241 [D1 loss: 0.598370, acc.: 60.94%] [D2 loss: 0.470599, acc.: 81.25%] [G loss: 4.672695]\n",
      "242 [D1 loss: 0.684803, acc.: 54.69%] [D2 loss: 0.461626, acc.: 78.12%] [G loss: 3.918560]\n",
      "243 [D1 loss: 0.718885, acc.: 50.00%] [D2 loss: 0.674770, acc.: 60.94%] [G loss: 2.498952]\n",
      "244 [D1 loss: 0.596332, acc.: 56.25%] [D2 loss: 0.521957, acc.: 70.31%] [G loss: 3.167697]\n",
      "245 [D1 loss: 0.772381, acc.: 35.94%] [D2 loss: 0.704877, acc.: 57.81%] [G loss: 2.157487]\n",
      "246 [D1 loss: 0.628250, acc.: 48.44%] [D2 loss: 0.519385, acc.: 70.31%] [G loss: 3.031301]\n",
      "247 [D1 loss: 0.783667, acc.: 34.38%] [D2 loss: 0.646825, acc.: 60.94%] [G loss: 1.863988]\n",
      "248 [D1 loss: 0.615093, acc.: 51.56%] [D2 loss: 0.512476, acc.: 70.31%] [G loss: 2.756601]\n",
      "249 [D1 loss: 0.650346, acc.: 50.00%] [D2 loss: 0.570965, acc.: 71.88%] [G loss: 2.624698]\n",
      "250 [D1 loss: 0.746220, acc.: 46.88%] [D2 loss: 0.742536, acc.: 46.88%] [G loss: 2.062002]\n",
      "251 [D1 loss: 0.677113, acc.: 46.88%] [D2 loss: 0.686418, acc.: 50.00%] [G loss: 2.079534]\n",
      "252 [D1 loss: 0.628420, acc.: 51.56%] [D2 loss: 0.530819, acc.: 67.19%] [G loss: 2.253573]\n",
      "253 [D1 loss: 0.675561, acc.: 50.00%] [D2 loss: 0.600619, acc.: 67.19%] [G loss: 2.362493]\n",
      "254 [D1 loss: 0.667140, acc.: 45.31%] [D2 loss: 0.604387, acc.: 65.62%] [G loss: 2.257233]\n",
      "255 [D1 loss: 0.689773, acc.: 42.19%] [D2 loss: 0.723729, acc.: 53.12%] [G loss: 2.001071]\n",
      "256 [D1 loss: 0.629357, acc.: 48.44%] [D2 loss: 0.516667, acc.: 73.44%] [G loss: 2.578985]\n",
      "257 [D1 loss: 0.703497, acc.: 40.62%] [D2 loss: 0.624631, acc.: 62.50%] [G loss: 2.192888]\n",
      "258 [D1 loss: 0.647385, acc.: 53.12%] [D2 loss: 0.690990, acc.: 59.38%] [G loss: 2.088536]\n",
      "259 [D1 loss: 0.663097, acc.: 51.56%] [D2 loss: 0.533481, acc.: 70.31%] [G loss: 2.403862]\n",
      "260 [D1 loss: 0.702151, acc.: 42.19%] [D2 loss: 0.732278, acc.: 51.56%] [G loss: 1.895546]\n",
      "261 [D1 loss: 0.622959, acc.: 53.12%] [D2 loss: 0.539801, acc.: 67.19%] [G loss: 2.457695]\n",
      "262 [D1 loss: 0.665581, acc.: 46.88%] [D2 loss: 0.711187, acc.: 53.12%] [G loss: 1.923499]\n",
      "263 [D1 loss: 0.617270, acc.: 53.12%] [D2 loss: 0.615525, acc.: 60.94%] [G loss: 2.153934]\n",
      "264 [D1 loss: 0.678209, acc.: 48.44%] [D2 loss: 0.620412, acc.: 64.06%] [G loss: 1.951790]\n",
      "265 [D1 loss: 0.647270, acc.: 57.81%] [D2 loss: 0.611737, acc.: 68.75%] [G loss: 2.212535]\n",
      "266 [D1 loss: 0.655678, acc.: 50.00%] [D2 loss: 0.636716, acc.: 60.94%] [G loss: 2.060633]\n",
      "267 [D1 loss: 0.662348, acc.: 48.44%] [D2 loss: 0.734538, acc.: 51.56%] [G loss: 1.782354]\n",
      "268 [D1 loss: 0.621341, acc.: 51.56%] [D2 loss: 0.623114, acc.: 67.19%] [G loss: 2.010198]\n",
      "269 [D1 loss: 0.665412, acc.: 46.88%] [D2 loss: 0.751848, acc.: 48.44%] [G loss: 1.608207]\n",
      "270 [D1 loss: 0.638273, acc.: 50.00%] [D2 loss: 0.569998, acc.: 67.19%] [G loss: 2.244379]\n",
      "271 [D1 loss: 0.654142, acc.: 51.56%] [D2 loss: 0.741816, acc.: 50.00%] [G loss: 1.596737]\n",
      "272 [D1 loss: 0.661655, acc.: 46.88%] [D2 loss: 0.709739, acc.: 46.88%] [G loss: 1.592242]\n",
      "273 [D1 loss: 0.617978, acc.: 53.12%] [D2 loss: 0.611816, acc.: 57.81%] [G loss: 1.944217]\n",
      "274 [D1 loss: 0.687956, acc.: 48.44%] [D2 loss: 0.758206, acc.: 48.44%] [G loss: 1.514326]\n",
      "275 [D1 loss: 0.614702, acc.: 54.69%] [D2 loss: 0.634072, acc.: 57.81%] [G loss: 1.808524]\n",
      "276 [D1 loss: 0.611248, acc.: 50.00%] [D2 loss: 0.764889, acc.: 42.19%] [G loss: 1.549017]\n",
      "277 [D1 loss: 0.618593, acc.: 60.94%] [D2 loss: 0.541797, acc.: 67.19%] [G loss: 1.844923]\n",
      "278 [D1 loss: 0.594866, acc.: 60.94%] [D2 loss: 0.691701, acc.: 50.00%] [G loss: 1.751623]\n",
      "279 [D1 loss: 0.616607, acc.: 56.25%] [D2 loss: 0.675349, acc.: 56.25%] [G loss: 1.569850]\n",
      "280 [D1 loss: 0.632929, acc.: 53.12%] [D2 loss: 0.650223, acc.: 50.00%] [G loss: 1.711325]\n",
      "281 [D1 loss: 0.663272, acc.: 43.75%] [D2 loss: 0.687109, acc.: 50.00%] [G loss: 1.522475]\n",
      "282 [D1 loss: 0.630028, acc.: 51.56%] [D2 loss: 0.634230, acc.: 59.38%] [G loss: 1.594399]\n",
      "283 [D1 loss: 0.598777, acc.: 62.50%] [D2 loss: 0.706964, acc.: 54.69%] [G loss: 1.509894]\n",
      "284 [D1 loss: 0.626074, acc.: 57.81%] [D2 loss: 0.696940, acc.: 48.44%] [G loss: 1.554471]\n",
      "285 [D1 loss: 0.643241, acc.: 57.81%] [D2 loss: 0.664061, acc.: 53.12%] [G loss: 1.557682]\n",
      "286 [D1 loss: 0.626742, acc.: 48.44%] [D2 loss: 0.717991, acc.: 39.06%] [G loss: 1.564871]\n",
      "287 [D1 loss: 0.631956, acc.: 50.00%] [D2 loss: 0.657992, acc.: 51.56%] [G loss: 1.611915]\n",
      "288 [D1 loss: 0.643309, acc.: 56.25%] [D2 loss: 0.680740, acc.: 51.56%] [G loss: 1.519466]\n",
      "289 [D1 loss: 0.641488, acc.: 51.56%] [D2 loss: 0.620313, acc.: 64.06%] [G loss: 1.626710]\n",
      "290 [D1 loss: 0.635402, acc.: 46.88%] [D2 loss: 0.707669, acc.: 42.19%] [G loss: 1.503397]\n",
      "291 [D1 loss: 0.594859, acc.: 54.69%] [D2 loss: 0.633358, acc.: 56.25%] [G loss: 1.581314]\n",
      "292 [D1 loss: 0.630987, acc.: 53.12%] [D2 loss: 0.746845, acc.: 43.75%] [G loss: 1.413939]\n",
      "293 [D1 loss: 0.618369, acc.: 57.81%] [D2 loss: 0.635570, acc.: 54.69%] [G loss: 1.476278]\n",
      "294 [D1 loss: 0.617310, acc.: 57.81%] [D2 loss: 0.730947, acc.: 42.19%] [G loss: 1.464701]\n",
      "295 [D1 loss: 0.614562, acc.: 65.62%] [D2 loss: 0.707153, acc.: 42.19%] [G loss: 1.510859]\n",
      "296 [D1 loss: 0.597673, acc.: 57.81%] [D2 loss: 0.682567, acc.: 46.88%] [G loss: 1.479444]\n",
      "297 [D1 loss: 0.626920, acc.: 56.25%] [D2 loss: 0.638388, acc.: 54.69%] [G loss: 1.480121]\n",
      "298 [D1 loss: 0.586636, acc.: 65.62%] [D2 loss: 0.705887, acc.: 39.06%] [G loss: 1.511004]\n",
      "299 [D1 loss: 0.613840, acc.: 57.81%] [D2 loss: 0.688056, acc.: 50.00%] [G loss: 1.575903]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = COGAN()\n",
    "    gan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e2eae-6555-40f3-9f15-fcbeec093c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cogan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow 2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
