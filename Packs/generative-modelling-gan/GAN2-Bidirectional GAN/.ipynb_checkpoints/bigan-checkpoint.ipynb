{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7be0d4",
   "metadata": {
    "id": "ea7be0d4"
   },
   "source": [
    "# CellStrat Hub Pack - Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e603e",
   "metadata": {
    "id": "281e603e"
   },
   "source": [
    "#### GAN2 - **Bidirectional Generative Adversarial Networks**(**BiGAN**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c16e17-32b3-4b74-9d7a-dca6c75c7af5",
   "metadata": {},
   "source": [
    "##### Kernel : Tensorflow 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4cb80c",
   "metadata": {
    "id": "dd4cb80c",
    "outputId": "0231c227-d0d9-418a-c1c7-b553b77aa8fa"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Importing required dependencies\n",
    "#==============================================================================\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6ebdae",
   "metadata": {
    "id": "1c6ebdae"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Create a class\n",
    "#==============================================================================\n",
    "\n",
    "class BIGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Build the encoder\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # The part of the bigan that trains the discriminator and encoder\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Generate image from sampled noise\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img_ = self.generator(z)\n",
    "\n",
    "        # Encode image\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z_ = self.encoder(img)\n",
    "\n",
    "        # Latent -> img is fake, and img -> latent is valid\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "\n",
    "        # Set up and compile the combined model\n",
    "        # Trains generator to fool the discriminator\n",
    "        self.bigan_generator = Model([z, img], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.latent_dim))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z = model(img)\n",
    "\n",
    "        return Model(img, z)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        gen_img = model(z)\n",
    "\n",
    "        return Model(z, gen_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "        model = Dense(1024)(d_in)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "        return Model([z, img], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Sample noise and generate img\n",
    "            z = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "            imgs_ = self.generator.predict(z)\n",
    "\n",
    "            # Select a random batch of images and encode\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            z_ = self.encoder.predict(imgs)\n",
    "\n",
    "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "            d_loss_real = self.discriminator.train_on_batch([z_, imgs], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "            g_loss = self.bigan_generator.train_on_batch([z, imgs], [valid, fake])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_interval(epoch)\n",
    "\n",
    "\n",
    "    # Save the generated images in images folder\n",
    "    def sample_interval(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        z = np.random.normal(size=(25, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(z)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087dd36a",
   "metadata": {
    "id": "087dd36a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 15:27:38.540285: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-30 15:27:38.540458: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-30 15:27:38.541185: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-30 15:27:39.213848: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-30 15:27:39.233787: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.892648, acc: 39.06%] [G loss: 4.268425]\n",
      "1 [D loss: 0.480261, acc: 60.94%] [G loss: 4.524778]\n",
      "2 [D loss: 0.335201, acc: 79.69%] [G loss: 4.678745]\n",
      "3 [D loss: 0.204732, acc: 96.88%] [G loss: 5.757043]\n",
      "4 [D loss: 0.160158, acc: 98.44%] [G loss: 6.608389]\n",
      "5 [D loss: 0.097997, acc: 100.00%] [G loss: 7.514520]\n",
      "6 [D loss: 0.081999, acc: 98.44%] [G loss: 9.037548]\n",
      "7 [D loss: 0.045025, acc: 100.00%] [G loss: 9.046580]\n",
      "8 [D loss: 0.020937, acc: 100.00%] [G loss: 9.955734]\n",
      "9 [D loss: 0.025866, acc: 100.00%] [G loss: 10.174637]\n",
      "10 [D loss: 0.020536, acc: 100.00%] [G loss: 10.602449]\n",
      "11 [D loss: 0.016094, acc: 100.00%] [G loss: 10.637330]\n",
      "12 [D loss: 0.021132, acc: 100.00%] [G loss: 11.186583]\n",
      "13 [D loss: 0.014769, acc: 100.00%] [G loss: 11.944171]\n",
      "14 [D loss: 0.014002, acc: 100.00%] [G loss: 12.729465]\n",
      "15 [D loss: 0.006491, acc: 100.00%] [G loss: 12.996114]\n",
      "16 [D loss: 0.010492, acc: 100.00%] [G loss: 14.352415]\n",
      "17 [D loss: 0.006046, acc: 100.00%] [G loss: 13.941457]\n",
      "18 [D loss: 0.004807, acc: 100.00%] [G loss: 14.608850]\n",
      "19 [D loss: 0.009375, acc: 100.00%] [G loss: 14.111756]\n",
      "20 [D loss: 0.005872, acc: 100.00%] [G loss: 14.807373]\n",
      "21 [D loss: 0.003104, acc: 100.00%] [G loss: 14.659716]\n",
      "22 [D loss: 0.003228, acc: 100.00%] [G loss: 14.032064]\n",
      "23 [D loss: 0.013289, acc: 100.00%] [G loss: 16.106758]\n",
      "24 [D loss: 0.003947, acc: 100.00%] [G loss: 16.130564]\n",
      "25 [D loss: 0.005637, acc: 100.00%] [G loss: 15.326445]\n",
      "26 [D loss: 0.008681, acc: 100.00%] [G loss: 15.822269]\n",
      "27 [D loss: 0.006298, acc: 100.00%] [G loss: 15.879323]\n",
      "28 [D loss: 0.003904, acc: 100.00%] [G loss: 16.761993]\n",
      "29 [D loss: 0.010470, acc: 100.00%] [G loss: 17.021996]\n",
      "30 [D loss: 0.010800, acc: 100.00%] [G loss: 17.700487]\n",
      "31 [D loss: 0.005791, acc: 100.00%] [G loss: 17.946102]\n",
      "32 [D loss: 0.098676, acc: 96.88%] [G loss: 25.233711]\n",
      "33 [D loss: 0.008414, acc: 100.00%] [G loss: 27.958517]\n",
      "34 [D loss: 0.086045, acc: 98.44%] [G loss: 28.668636]\n",
      "35 [D loss: 0.022965, acc: 100.00%] [G loss: 28.853094]\n",
      "36 [D loss: 0.015926, acc: 100.00%] [G loss: 29.684036]\n",
      "37 [D loss: 0.009249, acc: 100.00%] [G loss: 29.098797]\n",
      "38 [D loss: 0.003484, acc: 100.00%] [G loss: 28.959770]\n",
      "39 [D loss: 0.016960, acc: 100.00%] [G loss: 31.873669]\n",
      "40 [D loss: 0.007571, acc: 100.00%] [G loss: 30.799419]\n",
      "41 [D loss: 0.003712, acc: 100.00%] [G loss: 29.269047]\n",
      "42 [D loss: 0.002888, acc: 100.00%] [G loss: 30.495071]\n",
      "43 [D loss: 0.006204, acc: 100.00%] [G loss: 29.778690]\n",
      "44 [D loss: 0.008994, acc: 100.00%] [G loss: 30.350555]\n",
      "45 [D loss: 0.012969, acc: 100.00%] [G loss: 28.874233]\n",
      "46 [D loss: 0.002543, acc: 100.00%] [G loss: 28.589493]\n",
      "47 [D loss: 0.023685, acc: 98.44%] [G loss: 36.258160]\n",
      "48 [D loss: 0.006961, acc: 100.00%] [G loss: 37.376511]\n",
      "49 [D loss: 0.044750, acc: 98.44%] [G loss: 35.639454]\n",
      "50 [D loss: 0.010592, acc: 100.00%] [G loss: 35.500835]\n",
      "51 [D loss: 0.058830, acc: 98.44%] [G loss: 37.467861]\n",
      "52 [D loss: 0.033790, acc: 100.00%] [G loss: 41.914387]\n",
      "53 [D loss: 0.046882, acc: 98.44%] [G loss: 38.861477]\n",
      "54 [D loss: 0.006109, acc: 100.00%] [G loss: 39.013969]\n",
      "55 [D loss: 0.053991, acc: 96.88%] [G loss: 39.902687]\n",
      "56 [D loss: 0.043271, acc: 98.44%] [G loss: 39.846596]\n",
      "57 [D loss: 0.004639, acc: 100.00%] [G loss: 36.066879]\n",
      "58 [D loss: 0.034642, acc: 98.44%] [G loss: 43.533157]\n",
      "59 [D loss: 0.022992, acc: 100.00%] [G loss: 44.952229]\n",
      "60 [D loss: 0.073266, acc: 96.88%] [G loss: 39.841106]\n",
      "61 [D loss: 1.000839, acc: 89.06%] [G loss: 52.895473]\n",
      "62 [D loss: 0.524913, acc: 92.19%] [G loss: 57.211510]\n",
      "63 [D loss: 0.226157, acc: 92.19%] [G loss: 55.588863]\n",
      "64 [D loss: 0.218608, acc: 95.31%] [G loss: 54.865837]\n",
      "65 [D loss: 0.015914, acc: 100.00%] [G loss: 50.945396]\n",
      "66 [D loss: 0.293381, acc: 92.19%] [G loss: 55.513855]\n",
      "67 [D loss: 0.087383, acc: 93.75%] [G loss: 44.849258]\n",
      "68 [D loss: 0.024427, acc: 98.44%] [G loss: 44.399635]\n",
      "69 [D loss: 0.008970, acc: 100.00%] [G loss: 40.472633]\n",
      "70 [D loss: 1.029410, acc: 92.19%] [G loss: 57.566734]\n",
      "71 [D loss: 0.932315, acc: 87.50%] [G loss: 55.447014]\n",
      "72 [D loss: 0.195000, acc: 89.06%] [G loss: 48.394768]\n",
      "73 [D loss: 0.127870, acc: 98.44%] [G loss: 38.132072]\n",
      "74 [D loss: 0.421072, acc: 93.75%] [G loss: 57.411591]\n",
      "75 [D loss: 0.578466, acc: 82.81%] [G loss: 47.755280]\n",
      "76 [D loss: 0.433355, acc: 85.94%] [G loss: 64.244453]\n",
      "77 [D loss: 0.635984, acc: 84.38%] [G loss: 56.227497]\n",
      "78 [D loss: 0.537368, acc: 84.38%] [G loss: 43.226803]\n",
      "79 [D loss: 0.388767, acc: 90.62%] [G loss: 46.031265]\n",
      "80 [D loss: 0.173620, acc: 95.31%] [G loss: 34.784313]\n",
      "81 [D loss: 0.086820, acc: 96.88%] [G loss: 29.825359]\n",
      "82 [D loss: 0.246270, acc: 95.31%] [G loss: 42.493195]\n",
      "83 [D loss: 0.153803, acc: 92.19%] [G loss: 40.374035]\n",
      "84 [D loss: 0.116719, acc: 92.19%] [G loss: 40.714390]\n",
      "85 [D loss: 0.439138, acc: 98.44%] [G loss: 41.402199]\n",
      "86 [D loss: 0.017255, acc: 100.00%] [G loss: 39.630821]\n",
      "87 [D loss: 0.420141, acc: 85.94%] [G loss: 33.448933]\n",
      "88 [D loss: 0.686430, acc: 85.94%] [G loss: 30.228119]\n",
      "89 [D loss: 0.368669, acc: 92.19%] [G loss: 33.180733]\n",
      "90 [D loss: 0.492090, acc: 82.81%] [G loss: 33.981609]\n",
      "91 [D loss: 0.561935, acc: 82.81%] [G loss: 29.552170]\n",
      "92 [D loss: 0.210343, acc: 92.19%] [G loss: 22.159863]\n",
      "93 [D loss: 0.065974, acc: 96.88%] [G loss: 28.295843]\n",
      "94 [D loss: 0.141258, acc: 92.19%] [G loss: 24.222370]\n",
      "95 [D loss: 0.342649, acc: 92.19%] [G loss: 25.389931]\n",
      "96 [D loss: 0.288133, acc: 93.75%] [G loss: 27.725170]\n",
      "97 [D loss: 0.236738, acc: 89.06%] [G loss: 28.628229]\n",
      "98 [D loss: 0.152273, acc: 93.75%] [G loss: 28.671677]\n",
      "99 [D loss: 0.023766, acc: 100.00%] [G loss: 18.948460]\n",
      "100 [D loss: 0.352204, acc: 87.50%] [G loss: 34.541950]\n",
      "101 [D loss: 0.355418, acc: 81.25%] [G loss: 27.546135]\n",
      "102 [D loss: 0.808055, acc: 84.38%] [G loss: 30.244434]\n",
      "103 [D loss: 0.278974, acc: 89.06%] [G loss: 24.888153]\n",
      "104 [D loss: 0.073397, acc: 98.44%] [G loss: 20.147245]\n",
      "105 [D loss: 0.810514, acc: 82.81%] [G loss: 34.854549]\n",
      "106 [D loss: 0.332231, acc: 90.62%] [G loss: 30.229050]\n",
      "107 [D loss: 0.288929, acc: 90.62%] [G loss: 19.939770]\n",
      "108 [D loss: 0.420012, acc: 89.06%] [G loss: 30.065363]\n",
      "109 [D loss: 0.219836, acc: 90.62%] [G loss: 23.582249]\n",
      "110 [D loss: 0.192445, acc: 90.62%] [G loss: 19.388306]\n",
      "111 [D loss: 0.141330, acc: 95.31%] [G loss: 20.596289]\n",
      "112 [D loss: 0.275988, acc: 90.62%] [G loss: 18.877333]\n",
      "113 [D loss: 0.047602, acc: 100.00%] [G loss: 17.105085]\n",
      "114 [D loss: 0.809155, acc: 82.81%] [G loss: 29.497597]\n",
      "115 [D loss: 0.628771, acc: 79.69%] [G loss: 23.434008]\n",
      "116 [D loss: 0.149769, acc: 92.19%] [G loss: 19.435030]\n",
      "117 [D loss: 0.044073, acc: 98.44%] [G loss: 13.271380]\n",
      "118 [D loss: 0.192937, acc: 89.06%] [G loss: 15.766561]\n",
      "119 [D loss: 0.154492, acc: 93.75%] [G loss: 15.257540]\n",
      "120 [D loss: 0.346776, acc: 93.75%] [G loss: 16.565638]\n",
      "121 [D loss: 0.107075, acc: 95.31%] [G loss: 14.938179]\n",
      "122 [D loss: 0.355301, acc: 84.38%] [G loss: 17.727146]\n",
      "123 [D loss: 0.190193, acc: 93.75%] [G loss: 10.768959]\n",
      "124 [D loss: 0.845339, acc: 85.94%] [G loss: 15.745705]\n",
      "125 [D loss: 0.233665, acc: 87.50%] [G loss: 10.949876]\n",
      "126 [D loss: 0.472864, acc: 81.25%] [G loss: 13.153746]\n",
      "127 [D loss: 0.075310, acc: 95.31%] [G loss: 11.458504]\n",
      "128 [D loss: 0.815089, acc: 78.12%] [G loss: 15.364743]\n",
      "129 [D loss: 0.502622, acc: 84.38%] [G loss: 11.914078]\n",
      "130 [D loss: 0.279498, acc: 85.94%] [G loss: 12.595940]\n",
      "131 [D loss: 0.252602, acc: 87.50%] [G loss: 12.899536]\n",
      "132 [D loss: 0.687119, acc: 75.00%] [G loss: 14.612020]\n",
      "133 [D loss: 0.132121, acc: 93.75%] [G loss: 8.960372]\n",
      "134 [D loss: 0.877953, acc: 78.12%] [G loss: 13.830475]\n",
      "135 [D loss: 0.336613, acc: 84.38%] [G loss: 11.663399]\n",
      "136 [D loss: 0.209609, acc: 84.38%] [G loss: 10.263592]\n",
      "137 [D loss: 0.432959, acc: 79.69%] [G loss: 13.056967]\n",
      "138 [D loss: 0.360521, acc: 81.25%] [G loss: 12.086185]\n",
      "139 [D loss: 0.156990, acc: 90.62%] [G loss: 10.109978]\n",
      "140 [D loss: 0.362167, acc: 90.62%] [G loss: 12.017781]\n",
      "141 [D loss: 0.154708, acc: 93.75%] [G loss: 10.645340]\n",
      "142 [D loss: 0.145215, acc: 95.31%] [G loss: 8.356869]\n",
      "143 [D loss: 0.366890, acc: 95.31%] [G loss: 10.085665]\n",
      "144 [D loss: 0.123978, acc: 93.75%] [G loss: 8.888601]\n",
      "145 [D loss: 0.185438, acc: 95.31%] [G loss: 10.091074]\n",
      "146 [D loss: 0.286816, acc: 87.50%] [G loss: 8.158209]\n",
      "147 [D loss: 0.188251, acc: 89.06%] [G loss: 7.658548]\n",
      "148 [D loss: 1.049411, acc: 62.50%] [G loss: 13.723297]\n",
      "149 [D loss: 0.444341, acc: 78.12%] [G loss: 9.616825]\n",
      "150 [D loss: 0.501364, acc: 84.38%] [G loss: 8.457575]\n",
      "151 [D loss: 0.742851, acc: 70.31%] [G loss: 12.943584]\n",
      "152 [D loss: 0.185068, acc: 89.06%] [G loss: 10.345968]\n",
      "153 [D loss: 0.486797, acc: 89.06%] [G loss: 9.557983]\n",
      "154 [D loss: 0.160831, acc: 90.62%] [G loss: 10.760235]\n",
      "155 [D loss: 0.271811, acc: 82.81%] [G loss: 8.635553]\n",
      "156 [D loss: 0.211773, acc: 90.62%] [G loss: 7.925341]\n",
      "157 [D loss: 0.219523, acc: 89.06%] [G loss: 7.976862]\n",
      "158 [D loss: 0.128272, acc: 95.31%] [G loss: 8.600101]\n",
      "159 [D loss: 0.237693, acc: 85.94%] [G loss: 9.707613]\n",
      "160 [D loss: 0.321220, acc: 87.50%] [G loss: 7.939406]\n",
      "161 [D loss: 0.290910, acc: 85.94%] [G loss: 6.810357]\n",
      "162 [D loss: 0.472305, acc: 76.56%] [G loss: 8.019313]\n",
      "163 [D loss: 0.306788, acc: 87.50%] [G loss: 7.289291]\n",
      "164 [D loss: 0.817793, acc: 70.31%] [G loss: 11.184492]\n",
      "165 [D loss: 0.497375, acc: 75.00%] [G loss: 9.362993]\n",
      "166 [D loss: 0.235887, acc: 89.06%] [G loss: 7.118720]\n",
      "167 [D loss: 0.539536, acc: 85.94%] [G loss: 9.628967]\n",
      "168 [D loss: 0.269196, acc: 84.38%] [G loss: 7.731687]\n",
      "169 [D loss: 0.341344, acc: 89.06%] [G loss: 7.771608]\n",
      "170 [D loss: 0.189158, acc: 93.75%] [G loss: 7.517359]\n",
      "171 [D loss: 0.319340, acc: 84.38%] [G loss: 6.461050]\n",
      "172 [D loss: 0.436003, acc: 79.69%] [G loss: 7.886264]\n",
      "173 [D loss: 0.258571, acc: 92.19%] [G loss: 7.064685]\n",
      "174 [D loss: 0.224453, acc: 89.06%] [G loss: 6.904070]\n",
      "175 [D loss: 0.321011, acc: 82.81%] [G loss: 9.037611]\n",
      "176 [D loss: 0.162868, acc: 95.31%] [G loss: 6.004677]\n",
      "177 [D loss: 0.316353, acc: 87.50%] [G loss: 7.652215]\n",
      "178 [D loss: 0.135151, acc: 95.31%] [G loss: 7.155534]\n",
      "179 [D loss: 0.377565, acc: 81.25%] [G loss: 8.790645]\n",
      "180 [D loss: 0.444655, acc: 82.81%] [G loss: 8.428735]\n",
      "181 [D loss: 0.326786, acc: 87.50%] [G loss: 5.795490]\n",
      "182 [D loss: 0.628643, acc: 70.31%] [G loss: 7.219601]\n",
      "183 [D loss: 0.210538, acc: 92.19%] [G loss: 8.049631]\n",
      "184 [D loss: 0.199214, acc: 93.75%] [G loss: 8.251554]\n",
      "185 [D loss: 0.237209, acc: 90.62%] [G loss: 6.982087]\n",
      "186 [D loss: 0.225553, acc: 89.06%] [G loss: 6.424737]\n",
      "187 [D loss: 0.120328, acc: 98.44%] [G loss: 5.819390]\n",
      "188 [D loss: 0.519385, acc: 67.19%] [G loss: 9.353273]\n",
      "189 [D loss: 0.242707, acc: 87.50%] [G loss: 6.999563]\n",
      "190 [D loss: 0.329574, acc: 93.75%] [G loss: 6.469451]\n",
      "191 [D loss: 0.338767, acc: 85.94%] [G loss: 8.747154]\n",
      "192 [D loss: 0.200055, acc: 93.75%] [G loss: 7.176528]\n",
      "193 [D loss: 0.091757, acc: 98.44%] [G loss: 5.654273]\n",
      "194 [D loss: 0.356330, acc: 82.81%] [G loss: 6.631973]\n",
      "195 [D loss: 0.263926, acc: 82.81%] [G loss: 6.317031]\n",
      "196 [D loss: 0.394495, acc: 82.81%] [G loss: 7.298431]\n",
      "197 [D loss: 0.206080, acc: 89.06%] [G loss: 5.353459]\n",
      "198 [D loss: 0.704828, acc: 71.88%] [G loss: 7.378816]\n",
      "199 [D loss: 0.228106, acc: 90.62%] [G loss: 5.334109]\n",
      "200 [D loss: 0.395128, acc: 76.56%] [G loss: 8.260077]\n",
      "201 [D loss: 0.232670, acc: 90.62%] [G loss: 6.309581]\n",
      "202 [D loss: 0.166161, acc: 95.31%] [G loss: 6.190049]\n",
      "203 [D loss: 0.502274, acc: 75.00%] [G loss: 7.513863]\n",
      "204 [D loss: 0.212922, acc: 89.06%] [G loss: 6.263188]\n",
      "205 [D loss: 0.465277, acc: 75.00%] [G loss: 7.532881]\n",
      "206 [D loss: 0.167480, acc: 92.19%] [G loss: 5.327270]\n",
      "207 [D loss: 0.254180, acc: 87.50%] [G loss: 7.002505]\n",
      "208 [D loss: 0.161628, acc: 95.31%] [G loss: 5.590946]\n",
      "209 [D loss: 0.314347, acc: 90.62%] [G loss: 6.146773]\n",
      "210 [D loss: 0.242263, acc: 87.50%] [G loss: 5.921514]\n",
      "211 [D loss: 0.483784, acc: 73.44%] [G loss: 6.920962]\n",
      "212 [D loss: 0.384741, acc: 81.25%] [G loss: 5.822732]\n",
      "213 [D loss: 0.404459, acc: 81.25%] [G loss: 7.940294]\n",
      "214 [D loss: 0.587538, acc: 67.19%] [G loss: 7.179552]\n",
      "215 [D loss: 0.293137, acc: 85.94%] [G loss: 5.361471]\n",
      "216 [D loss: 0.262911, acc: 90.62%] [G loss: 5.473905]\n",
      "217 [D loss: 0.380309, acc: 84.38%] [G loss: 7.692345]\n",
      "218 [D loss: 0.567864, acc: 67.19%] [G loss: 6.921640]\n",
      "219 [D loss: 0.294149, acc: 90.62%] [G loss: 4.923839]\n",
      "220 [D loss: 0.349208, acc: 90.62%] [G loss: 6.957954]\n",
      "221 [D loss: 0.326430, acc: 85.94%] [G loss: 6.339090]\n",
      "222 [D loss: 0.194228, acc: 93.75%] [G loss: 5.892550]\n",
      "223 [D loss: 0.453219, acc: 73.44%] [G loss: 6.559061]\n",
      "224 [D loss: 0.228954, acc: 90.62%] [G loss: 5.933915]\n",
      "225 [D loss: 0.282474, acc: 89.06%] [G loss: 5.652107]\n",
      "226 [D loss: 0.397882, acc: 81.25%] [G loss: 6.464612]\n",
      "227 [D loss: 0.387258, acc: 90.62%] [G loss: 6.144072]\n",
      "228 [D loss: 0.434792, acc: 75.00%] [G loss: 5.780842]\n",
      "229 [D loss: 0.318443, acc: 85.94%] [G loss: 6.420956]\n",
      "230 [D loss: 0.243630, acc: 87.50%] [G loss: 5.179116]\n",
      "231 [D loss: 0.387155, acc: 81.25%] [G loss: 5.950584]\n",
      "232 [D loss: 0.171301, acc: 93.75%] [G loss: 4.512244]\n",
      "233 [D loss: 0.938612, acc: 53.12%] [G loss: 8.378778]\n",
      "234 [D loss: 0.304565, acc: 82.81%] [G loss: 5.524163]\n",
      "235 [D loss: 0.224468, acc: 93.75%] [G loss: 5.540942]\n",
      "236 [D loss: 0.880622, acc: 46.88%] [G loss: 8.498782]\n",
      "237 [D loss: 0.426562, acc: 73.44%] [G loss: 5.983445]\n",
      "238 [D loss: 0.407255, acc: 81.25%] [G loss: 5.869884]\n",
      "239 [D loss: 0.181867, acc: 98.44%] [G loss: 5.497430]\n",
      "240 [D loss: 0.209137, acc: 92.19%] [G loss: 5.752693]\n",
      "241 [D loss: 0.410656, acc: 81.25%] [G loss: 6.313356]\n",
      "242 [D loss: 0.188433, acc: 93.75%] [G loss: 4.511984]\n",
      "243 [D loss: 0.360290, acc: 89.06%] [G loss: 5.943791]\n",
      "244 [D loss: 0.260314, acc: 89.06%] [G loss: 5.122675]\n",
      "245 [D loss: 0.173418, acc: 98.44%] [G loss: 5.006322]\n",
      "246 [D loss: 0.318348, acc: 82.81%] [G loss: 5.473183]\n",
      "247 [D loss: 0.349965, acc: 81.25%] [G loss: 5.208568]\n",
      "248 [D loss: 0.398310, acc: 79.69%] [G loss: 6.310649]\n",
      "249 [D loss: 0.306487, acc: 90.62%] [G loss: 4.808178]\n",
      "250 [D loss: 0.625739, acc: 64.06%] [G loss: 6.270000]\n",
      "251 [D loss: 0.171136, acc: 95.31%] [G loss: 4.515761]\n",
      "252 [D loss: 0.235870, acc: 89.06%] [G loss: 4.392268]\n",
      "253 [D loss: 0.304643, acc: 87.50%] [G loss: 4.785977]\n",
      "254 [D loss: 0.441763, acc: 79.69%] [G loss: 5.502323]\n",
      "255 [D loss: 0.217405, acc: 89.06%] [G loss: 4.519025]\n",
      "256 [D loss: 0.582600, acc: 67.19%] [G loss: 5.794523]\n",
      "257 [D loss: 0.242782, acc: 87.50%] [G loss: 4.384367]\n",
      "258 [D loss: 0.545008, acc: 76.56%] [G loss: 5.298099]\n",
      "259 [D loss: 0.319138, acc: 85.94%] [G loss: 4.925590]\n",
      "260 [D loss: 0.274636, acc: 90.62%] [G loss: 4.378965]\n",
      "261 [D loss: 0.504059, acc: 73.44%] [G loss: 5.194852]\n",
      "262 [D loss: 0.279872, acc: 89.06%] [G loss: 4.368652]\n",
      "263 [D loss: 0.400198, acc: 78.12%] [G loss: 5.443770]\n",
      "264 [D loss: 0.426690, acc: 79.69%] [G loss: 5.469193]\n",
      "265 [D loss: 0.215435, acc: 90.62%] [G loss: 3.907950]\n",
      "266 [D loss: 0.317235, acc: 85.94%] [G loss: 5.007911]\n",
      "267 [D loss: 0.501136, acc: 73.44%] [G loss: 5.212732]\n",
      "268 [D loss: 0.519010, acc: 71.88%] [G loss: 4.703353]\n",
      "269 [D loss: 0.169176, acc: 95.31%] [G loss: 4.329006]\n",
      "270 [D loss: 0.435174, acc: 76.56%] [G loss: 5.185545]\n",
      "271 [D loss: 0.192598, acc: 96.88%] [G loss: 4.709569]\n",
      "272 [D loss: 0.993739, acc: 48.44%] [G loss: 7.071239]\n",
      "273 [D loss: 0.295535, acc: 85.94%] [G loss: 4.970921]\n",
      "274 [D loss: 0.286703, acc: 92.19%] [G loss: 4.654458]\n",
      "275 [D loss: 0.333286, acc: 84.38%] [G loss: 4.959991]\n",
      "276 [D loss: 0.345664, acc: 82.81%] [G loss: 5.326799]\n",
      "277 [D loss: 0.207380, acc: 93.75%] [G loss: 5.203532]\n",
      "278 [D loss: 0.322027, acc: 89.06%] [G loss: 4.959939]\n",
      "279 [D loss: 0.322698, acc: 87.50%] [G loss: 5.422301]\n",
      "280 [D loss: 0.828276, acc: 68.75%] [G loss: 5.391527]\n",
      "281 [D loss: 0.338523, acc: 81.25%] [G loss: 4.273201]\n",
      "282 [D loss: 0.252579, acc: 93.75%] [G loss: 4.216612]\n",
      "283 [D loss: 0.372440, acc: 82.81%] [G loss: 5.288621]\n",
      "284 [D loss: 0.437069, acc: 75.00%] [G loss: 4.833907]\n",
      "285 [D loss: 0.178881, acc: 95.31%] [G loss: 4.204239]\n",
      "286 [D loss: 0.328837, acc: 89.06%] [G loss: 4.646409]\n",
      "287 [D loss: 0.378393, acc: 84.38%] [G loss: 5.324213]\n",
      "288 [D loss: 0.291421, acc: 92.19%] [G loss: 3.860814]\n",
      "289 [D loss: 0.320902, acc: 82.81%] [G loss: 4.682356]\n",
      "290 [D loss: 0.423012, acc: 81.25%] [G loss: 5.105053]\n",
      "291 [D loss: 0.303140, acc: 89.06%] [G loss: 4.407573]\n",
      "292 [D loss: 0.350816, acc: 82.81%] [G loss: 5.003802]\n",
      "293 [D loss: 0.407570, acc: 79.69%] [G loss: 4.619467]\n",
      "294 [D loss: 0.436008, acc: 73.44%] [G loss: 4.370590]\n",
      "295 [D loss: 0.400431, acc: 84.38%] [G loss: 4.578589]\n",
      "296 [D loss: 0.288848, acc: 89.06%] [G loss: 4.685845]\n",
      "297 [D loss: 0.355696, acc: 87.50%] [G loss: 4.613959]\n",
      "298 [D loss: 0.318409, acc: 90.62%] [G loss: 4.614058]\n",
      "299 [D loss: 0.353469, acc: 85.94%] [G loss: 5.175070]\n",
      "300 [D loss: 0.234760, acc: 95.31%] [G loss: 4.204024]\n",
      "301 [D loss: 0.364776, acc: 84.38%] [G loss: 4.003031]\n",
      "302 [D loss: 0.462489, acc: 78.12%] [G loss: 5.107217]\n",
      "303 [D loss: 0.461430, acc: 75.00%] [G loss: 4.908318]\n",
      "304 [D loss: 0.207656, acc: 96.88%] [G loss: 4.301156]\n",
      "305 [D loss: 0.363210, acc: 84.38%] [G loss: 4.752524]\n",
      "306 [D loss: 0.244691, acc: 95.31%] [G loss: 4.456491]\n",
      "307 [D loss: 0.441783, acc: 75.00%] [G loss: 5.637852]\n",
      "308 [D loss: 0.310382, acc: 89.06%] [G loss: 4.892245]\n",
      "309 [D loss: 0.360359, acc: 82.81%] [G loss: 4.563903]\n",
      "310 [D loss: 0.271269, acc: 92.19%] [G loss: 4.360255]\n",
      "311 [D loss: 0.331304, acc: 89.06%] [G loss: 5.096618]\n",
      "312 [D loss: 0.427558, acc: 79.69%] [G loss: 4.478885]\n",
      "313 [D loss: 0.248797, acc: 90.62%] [G loss: 4.921536]\n",
      "314 [D loss: 0.267047, acc: 89.06%] [G loss: 4.448434]\n",
      "315 [D loss: 0.510626, acc: 73.44%] [G loss: 4.529535]\n",
      "316 [D loss: 0.470564, acc: 84.38%] [G loss: 5.298687]\n",
      "317 [D loss: 0.302048, acc: 89.06%] [G loss: 4.581687]\n",
      "318 [D loss: 0.349130, acc: 79.69%] [G loss: 4.232806]\n",
      "319 [D loss: 0.295118, acc: 89.06%] [G loss: 5.137511]\n",
      "320 [D loss: 0.294225, acc: 87.50%] [G loss: 4.995475]\n",
      "321 [D loss: 0.283117, acc: 84.38%] [G loss: 4.237369]\n",
      "322 [D loss: 0.602295, acc: 70.31%] [G loss: 5.839461]\n",
      "323 [D loss: 0.392442, acc: 81.25%] [G loss: 4.113602]\n",
      "324 [D loss: 0.318540, acc: 89.06%] [G loss: 4.220667]\n",
      "325 [D loss: 0.581155, acc: 65.62%] [G loss: 5.609984]\n",
      "326 [D loss: 0.261287, acc: 92.19%] [G loss: 3.875612]\n",
      "327 [D loss: 0.428207, acc: 82.81%] [G loss: 5.472107]\n",
      "328 [D loss: 0.345299, acc: 85.94%] [G loss: 4.607392]\n",
      "329 [D loss: 0.358253, acc: 87.50%] [G loss: 4.482366]\n",
      "330 [D loss: 0.411892, acc: 73.44%] [G loss: 4.975480]\n",
      "331 [D loss: 0.277950, acc: 92.19%] [G loss: 3.928440]\n",
      "332 [D loss: 0.270921, acc: 90.62%] [G loss: 4.605516]\n",
      "333 [D loss: 0.461821, acc: 73.44%] [G loss: 5.109506]\n",
      "334 [D loss: 0.223944, acc: 93.75%] [G loss: 4.212778]\n",
      "335 [D loss: 0.268821, acc: 93.75%] [G loss: 4.460272]\n",
      "336 [D loss: 0.330199, acc: 85.94%] [G loss: 4.363412]\n",
      "337 [D loss: 0.473390, acc: 75.00%] [G loss: 5.525856]\n",
      "338 [D loss: 0.260639, acc: 92.19%] [G loss: 5.057691]\n",
      "339 [D loss: 0.433782, acc: 76.56%] [G loss: 5.263331]\n",
      "340 [D loss: 0.345625, acc: 84.38%] [G loss: 5.121888]\n",
      "341 [D loss: 0.316587, acc: 92.19%] [G loss: 4.668551]\n",
      "342 [D loss: 0.426427, acc: 79.69%] [G loss: 5.059126]\n",
      "343 [D loss: 0.219423, acc: 93.75%] [G loss: 4.405478]\n",
      "344 [D loss: 0.444411, acc: 79.69%] [G loss: 5.877724]\n",
      "345 [D loss: 0.216070, acc: 93.75%] [G loss: 3.577702]\n",
      "346 [D loss: 0.720228, acc: 64.06%] [G loss: 5.060514]\n",
      "347 [D loss: 0.273966, acc: 89.06%] [G loss: 3.706981]\n",
      "348 [D loss: 0.367128, acc: 85.94%] [G loss: 4.915503]\n",
      "349 [D loss: 0.424760, acc: 78.12%] [G loss: 4.564459]\n",
      "350 [D loss: 0.324702, acc: 87.50%] [G loss: 4.608212]\n",
      "351 [D loss: 0.380225, acc: 82.81%] [G loss: 3.706972]\n",
      "352 [D loss: 0.332814, acc: 89.06%] [G loss: 4.863220]\n",
      "353 [D loss: 0.343892, acc: 84.38%] [G loss: 4.672582]\n",
      "354 [D loss: 0.289712, acc: 89.06%] [G loss: 4.776387]\n",
      "355 [D loss: 0.328609, acc: 84.38%] [G loss: 5.283713]\n",
      "356 [D loss: 0.253775, acc: 89.06%] [G loss: 5.163990]\n",
      "357 [D loss: 0.715444, acc: 60.94%] [G loss: 4.656419]\n",
      "358 [D loss: 0.246972, acc: 93.75%] [G loss: 4.097355]\n",
      "359 [D loss: 0.529150, acc: 76.56%] [G loss: 5.733048]\n",
      "360 [D loss: 0.294804, acc: 82.81%] [G loss: 4.290568]\n",
      "361 [D loss: 0.334288, acc: 89.06%] [G loss: 4.833591]\n",
      "362 [D loss: 0.335150, acc: 87.50%] [G loss: 5.400739]\n",
      "363 [D loss: 0.440925, acc: 75.00%] [G loss: 4.955977]\n",
      "364 [D loss: 0.236494, acc: 90.62%] [G loss: 4.745122]\n",
      "365 [D loss: 0.519448, acc: 78.12%] [G loss: 4.588967]\n",
      "366 [D loss: 0.389323, acc: 82.81%] [G loss: 5.181117]\n",
      "367 [D loss: 0.301170, acc: 84.38%] [G loss: 4.899555]\n",
      "368 [D loss: 0.386989, acc: 84.38%] [G loss: 4.784073]\n",
      "369 [D loss: 0.378974, acc: 79.69%] [G loss: 4.476460]\n",
      "370 [D loss: 0.342385, acc: 85.94%] [G loss: 4.636604]\n",
      "371 [D loss: 0.344317, acc: 84.38%] [G loss: 4.596044]\n",
      "372 [D loss: 0.342420, acc: 87.50%] [G loss: 5.170500]\n",
      "373 [D loss: 0.270043, acc: 90.62%] [G loss: 4.625753]\n",
      "374 [D loss: 0.384032, acc: 82.81%] [G loss: 5.240972]\n",
      "375 [D loss: 0.271375, acc: 90.62%] [G loss: 4.697777]\n",
      "376 [D loss: 0.239818, acc: 93.75%] [G loss: 4.494624]\n",
      "377 [D loss: 0.323844, acc: 85.94%] [G loss: 4.058053]\n",
      "378 [D loss: 0.306585, acc: 92.19%] [G loss: 4.339233]\n",
      "379 [D loss: 0.521820, acc: 78.12%] [G loss: 4.284161]\n",
      "380 [D loss: 0.218298, acc: 96.88%] [G loss: 4.631001]\n",
      "381 [D loss: 0.378634, acc: 82.81%] [G loss: 5.021643]\n",
      "382 [D loss: 0.441514, acc: 78.12%] [G loss: 4.570902]\n",
      "383 [D loss: 0.472222, acc: 79.69%] [G loss: 4.802215]\n",
      "384 [D loss: 0.259534, acc: 90.62%] [G loss: 4.105479]\n",
      "385 [D loss: 0.368933, acc: 79.69%] [G loss: 4.374922]\n",
      "386 [D loss: 0.329404, acc: 89.06%] [G loss: 4.594507]\n",
      "387 [D loss: 0.424517, acc: 79.69%] [G loss: 4.420026]\n",
      "388 [D loss: 0.404415, acc: 81.25%] [G loss: 5.381058]\n",
      "389 [D loss: 0.422914, acc: 75.00%] [G loss: 4.896863]\n",
      "390 [D loss: 0.336701, acc: 84.38%] [G loss: 5.126619]\n",
      "391 [D loss: 0.377126, acc: 79.69%] [G loss: 4.919955]\n",
      "392 [D loss: 0.294281, acc: 87.50%] [G loss: 4.751690]\n",
      "393 [D loss: 0.721553, acc: 59.38%] [G loss: 5.410393]\n",
      "394 [D loss: 0.329343, acc: 87.50%] [G loss: 4.198261]\n",
      "395 [D loss: 0.390573, acc: 92.19%] [G loss: 4.714697]\n",
      "396 [D loss: 0.250657, acc: 93.75%] [G loss: 3.823822]\n",
      "397 [D loss: 0.289053, acc: 92.19%] [G loss: 5.077478]\n",
      "398 [D loss: 0.209093, acc: 95.31%] [G loss: 4.630544]\n",
      "399 [D loss: 0.317149, acc: 89.06%] [G loss: 5.088668]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bigan = BIGAN()\n",
    "    bigan.train(epochs=40000, batch_size=32, sample_interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e022aa-a58a-4b08-8329-ae971390962e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5779757-ac96-4065-bad9-89e1ec9c345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7e676-6a9c-4f34-9984-637218a72760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bigan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow 2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
