{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b17bff",
   "metadata": {
    "id": "c5b17bff",
    "tags": []
   },
   "source": [
    "# CellStrat Hub Pack - Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8ce80",
   "metadata": {
    "id": "74b8ce80"
   },
   "source": [
    "#### GAN7 - **Least Squares Generative Adversarial Networks**(**LSGAN**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d06a-950e-4cfa-90e4-26ba3926046e",
   "metadata": {},
   "source": [
    "##### Kernel : Tensorflow 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2c9f52",
   "metadata": {
    "id": "6e2c9f52",
    "outputId": "26b240cb-3945-4ac6-da52-88d29d2c99bd"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Importing required dependencies\n",
    "#==============================================================================\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dea14a4",
   "metadata": {
    "id": "4dea14a4"
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Create a class\n",
    "#==============================================================================\n",
    "class LSGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        # (!!!) Optimize w.r.t. MSE loss instead of crossentropy\n",
    "        self.combined.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # (!!!) No softmax\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "\n",
    "    # Save the generated images in images folder\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2300e6",
   "metadata": {
    "id": "6a2300e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 08:08:19.873281: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-01 08:08:19.875000: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-01 08:08:19.967023: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-01 08:08:22.505206: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-01 08:08:22.601307: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.130556, acc.: 85.94%] [G loss: 0.616257]\n",
      "1 [D loss: 2.975231, acc.: 100.00%] [G loss: 0.654357]\n",
      "2 [D loss: 0.279320, acc.: 60.94%] [G loss: 0.681492]\n",
      "3 [D loss: 0.202766, acc.: 68.75%] [G loss: 0.419331]\n",
      "4 [D loss: 0.200126, acc.: 89.06%] [G loss: 0.446373]\n",
      "5 [D loss: 0.119209, acc.: 81.25%] [G loss: 0.625212]\n",
      "6 [D loss: 0.104036, acc.: 90.62%] [G loss: 0.543843]\n",
      "7 [D loss: 0.100318, acc.: 89.06%] [G loss: 0.595298]\n",
      "8 [D loss: 0.103443, acc.: 96.88%] [G loss: 0.623455]\n",
      "9 [D loss: 0.193264, acc.: 78.12%] [G loss: 0.757936]\n",
      "10 [D loss: 0.131704, acc.: 92.19%] [G loss: 0.866886]\n",
      "11 [D loss: 0.119073, acc.: 85.94%] [G loss: 0.873181]\n",
      "12 [D loss: 0.184086, acc.: 85.94%] [G loss: 0.998542]\n",
      "13 [D loss: 0.245617, acc.: 68.75%] [G loss: 1.024941]\n",
      "14 [D loss: 0.353553, acc.: 89.06%] [G loss: 1.358607]\n",
      "15 [D loss: 0.454103, acc.: 40.62%] [G loss: 0.736485]\n",
      "16 [D loss: 0.532538, acc.: 85.94%] [G loss: 1.117114]\n",
      "17 [D loss: 0.444826, acc.: 45.31%] [G loss: 0.876436]\n",
      "18 [D loss: 0.133430, acc.: 95.31%] [G loss: 1.107166]\n",
      "19 [D loss: 0.155346, acc.: 85.94%] [G loss: 1.147039]\n",
      "20 [D loss: 0.117856, acc.: 90.62%] [G loss: 1.078122]\n",
      "21 [D loss: 0.145403, acc.: 85.94%] [G loss: 1.115123]\n",
      "22 [D loss: 0.106536, acc.: 95.31%] [G loss: 1.141049]\n",
      "23 [D loss: 0.144536, acc.: 87.50%] [G loss: 1.113601]\n",
      "24 [D loss: 0.080842, acc.: 98.44%] [G loss: 1.064250]\n",
      "25 [D loss: 0.105130, acc.: 92.19%] [G loss: 1.091286]\n",
      "26 [D loss: 0.149084, acc.: 89.06%] [G loss: 1.145136]\n",
      "27 [D loss: 0.139528, acc.: 87.50%] [G loss: 1.199334]\n",
      "28 [D loss: 0.119726, acc.: 87.50%] [G loss: 0.962996]\n",
      "29 [D loss: 0.137034, acc.: 90.62%] [G loss: 0.988226]\n",
      "30 [D loss: 0.158935, acc.: 84.38%] [G loss: 1.197208]\n",
      "31 [D loss: 0.196541, acc.: 98.44%] [G loss: 1.141211]\n",
      "32 [D loss: 0.264242, acc.: 60.94%] [G loss: 1.045533]\n",
      "33 [D loss: 0.324408, acc.: 89.06%] [G loss: 1.068999]\n",
      "34 [D loss: 0.325762, acc.: 54.69%] [G loss: 0.854653]\n",
      "35 [D loss: 0.299097, acc.: 90.62%] [G loss: 1.068957]\n",
      "36 [D loss: 0.219533, acc.: 70.31%] [G loss: 1.064958]\n",
      "37 [D loss: 0.120518, acc.: 95.31%] [G loss: 1.094302]\n",
      "38 [D loss: 0.120679, acc.: 95.31%] [G loss: 0.923436]\n",
      "39 [D loss: 0.121120, acc.: 92.19%] [G loss: 0.853197]\n",
      "40 [D loss: 0.090107, acc.: 90.62%] [G loss: 1.048217]\n",
      "41 [D loss: 0.080448, acc.: 96.88%] [G loss: 1.069078]\n",
      "42 [D loss: 0.110117, acc.: 92.19%] [G loss: 1.067244]\n",
      "43 [D loss: 0.065176, acc.: 98.44%] [G loss: 0.766520]\n",
      "44 [D loss: 0.071882, acc.: 95.31%] [G loss: 0.966245]\n",
      "45 [D loss: 0.081661, acc.: 92.19%] [G loss: 1.155355]\n",
      "46 [D loss: 0.074824, acc.: 95.31%] [G loss: 0.985432]\n",
      "47 [D loss: 0.134639, acc.: 90.62%] [G loss: 1.081828]\n",
      "48 [D loss: 0.117206, acc.: 93.75%] [G loss: 0.826524]\n",
      "49 [D loss: 0.153760, acc.: 79.69%] [G loss: 1.142489]\n",
      "50 [D loss: 0.081191, acc.: 98.44%] [G loss: 1.112569]\n",
      "51 [D loss: 0.089044, acc.: 92.19%] [G loss: 1.138507]\n",
      "52 [D loss: 0.097090, acc.: 100.00%] [G loss: 0.912846]\n",
      "53 [D loss: 0.119898, acc.: 85.94%] [G loss: 1.022814]\n",
      "54 [D loss: 0.093916, acc.: 95.31%] [G loss: 0.825969]\n",
      "55 [D loss: 0.085976, acc.: 98.44%] [G loss: 0.823945]\n",
      "56 [D loss: 0.123300, acc.: 85.94%] [G loss: 0.947179]\n",
      "57 [D loss: 0.123806, acc.: 85.94%] [G loss: 1.036761]\n",
      "58 [D loss: 0.128003, acc.: 98.44%] [G loss: 1.112665]\n",
      "59 [D loss: 0.074567, acc.: 92.19%] [G loss: 1.077482]\n",
      "60 [D loss: 0.097120, acc.: 89.06%] [G loss: 1.066804]\n",
      "61 [D loss: 0.102196, acc.: 95.31%] [G loss: 0.969309]\n",
      "62 [D loss: 0.083230, acc.: 93.75%] [G loss: 0.998586]\n",
      "63 [D loss: 0.123920, acc.: 90.62%] [G loss: 0.918855]\n",
      "64 [D loss: 0.119418, acc.: 89.06%] [G loss: 1.029974]\n",
      "65 [D loss: 0.088325, acc.: 100.00%] [G loss: 0.686973]\n",
      "66 [D loss: 0.104372, acc.: 89.06%] [G loss: 0.744665]\n",
      "67 [D loss: 0.099889, acc.: 90.62%] [G loss: 0.704563]\n",
      "68 [D loss: 0.119881, acc.: 96.88%] [G loss: 0.847336]\n",
      "69 [D loss: 0.105474, acc.: 93.75%] [G loss: 1.027016]\n",
      "70 [D loss: 0.150214, acc.: 93.75%] [G loss: 1.101010]\n",
      "71 [D loss: 0.130984, acc.: 92.19%] [G loss: 1.049365]\n",
      "72 [D loss: 0.078794, acc.: 96.88%] [G loss: 0.898683]\n",
      "73 [D loss: 0.109519, acc.: 87.50%] [G loss: 0.943973]\n",
      "74 [D loss: 0.052172, acc.: 100.00%] [G loss: 0.945002]\n",
      "75 [D loss: 0.118415, acc.: 92.19%] [G loss: 0.944597]\n",
      "76 [D loss: 0.114669, acc.: 92.19%] [G loss: 0.830319]\n",
      "77 [D loss: 0.070765, acc.: 93.75%] [G loss: 1.026466]\n",
      "78 [D loss: 0.153932, acc.: 89.06%] [G loss: 0.907300]\n",
      "79 [D loss: 0.118901, acc.: 96.88%] [G loss: 0.623828]\n",
      "80 [D loss: 0.141778, acc.: 82.81%] [G loss: 0.925254]\n",
      "81 [D loss: 0.096634, acc.: 98.44%] [G loss: 1.115938]\n",
      "82 [D loss: 0.113364, acc.: 89.06%] [G loss: 0.961966]\n",
      "83 [D loss: 0.081976, acc.: 96.88%] [G loss: 0.818280]\n",
      "84 [D loss: 0.086393, acc.: 96.88%] [G loss: 0.995112]\n",
      "85 [D loss: 0.073642, acc.: 92.19%] [G loss: 1.143080]\n",
      "86 [D loss: 0.085565, acc.: 100.00%] [G loss: 0.875779]\n",
      "87 [D loss: 0.078162, acc.: 92.19%] [G loss: 1.132127]\n",
      "88 [D loss: 0.085730, acc.: 98.44%] [G loss: 0.793782]\n",
      "89 [D loss: 0.088870, acc.: 98.44%] [G loss: 0.709419]\n",
      "90 [D loss: 0.103191, acc.: 89.06%] [G loss: 1.130637]\n",
      "91 [D loss: 0.088948, acc.: 96.88%] [G loss: 0.922462]\n",
      "92 [D loss: 0.097586, acc.: 89.06%] [G loss: 1.217927]\n",
      "93 [D loss: 0.083842, acc.: 98.44%] [G loss: 0.893497]\n",
      "94 [D loss: 0.075526, acc.: 98.44%] [G loss: 0.947940]\n",
      "95 [D loss: 0.085268, acc.: 93.75%] [G loss: 0.964678]\n",
      "96 [D loss: 0.074899, acc.: 92.19%] [G loss: 0.974403]\n",
      "97 [D loss: 0.066448, acc.: 100.00%] [G loss: 0.864559]\n",
      "98 [D loss: 0.084774, acc.: 89.06%] [G loss: 0.705083]\n",
      "99 [D loss: 0.050661, acc.: 95.31%] [G loss: 1.112565]\n",
      "100 [D loss: 0.064283, acc.: 93.75%] [G loss: 0.981311]\n",
      "101 [D loss: 0.081452, acc.: 96.88%] [G loss: 0.806130]\n",
      "102 [D loss: 0.094631, acc.: 92.19%] [G loss: 0.891659]\n",
      "103 [D loss: 0.071794, acc.: 93.75%] [G loss: 1.034647]\n",
      "104 [D loss: 0.083191, acc.: 95.31%] [G loss: 0.860158]\n",
      "105 [D loss: 0.101141, acc.: 95.31%] [G loss: 1.010168]\n",
      "106 [D loss: 0.071524, acc.: 95.31%] [G loss: 0.861648]\n",
      "107 [D loss: 0.076802, acc.: 98.44%] [G loss: 1.077447]\n",
      "108 [D loss: 0.090475, acc.: 95.31%] [G loss: 0.636701]\n",
      "109 [D loss: 0.089423, acc.: 100.00%] [G loss: 0.989288]\n",
      "110 [D loss: 0.145954, acc.: 84.38%] [G loss: 1.103796]\n",
      "111 [D loss: 0.102803, acc.: 98.44%] [G loss: 1.212143]\n",
      "112 [D loss: 0.115063, acc.: 93.75%] [G loss: 0.776018]\n",
      "113 [D loss: 0.142356, acc.: 95.31%] [G loss: 1.119443]\n",
      "114 [D loss: 0.128238, acc.: 89.06%] [G loss: 1.088818]\n",
      "115 [D loss: 0.075345, acc.: 96.88%] [G loss: 0.800223]\n",
      "116 [D loss: 0.045087, acc.: 96.88%] [G loss: 1.037286]\n",
      "117 [D loss: 0.069842, acc.: 93.75%] [G loss: 1.056431]\n",
      "118 [D loss: 0.058948, acc.: 96.88%] [G loss: 1.117553]\n",
      "119 [D loss: 0.076338, acc.: 98.44%] [G loss: 0.691647]\n",
      "120 [D loss: 0.099605, acc.: 87.50%] [G loss: 0.896313]\n",
      "121 [D loss: 0.103433, acc.: 96.88%] [G loss: 1.058450]\n",
      "122 [D loss: 0.039590, acc.: 100.00%] [G loss: 0.926191]\n",
      "123 [D loss: 0.062882, acc.: 95.31%] [G loss: 0.890460]\n",
      "124 [D loss: 0.050205, acc.: 96.88%] [G loss: 1.040989]\n",
      "125 [D loss: 0.043282, acc.: 100.00%] [G loss: 0.967047]\n",
      "126 [D loss: 0.057986, acc.: 95.31%] [G loss: 0.853720]\n",
      "127 [D loss: 0.042339, acc.: 100.00%] [G loss: 0.994706]\n",
      "128 [D loss: 0.051413, acc.: 98.44%] [G loss: 0.929972]\n",
      "129 [D loss: 0.080790, acc.: 92.19%] [G loss: 0.955183]\n",
      "130 [D loss: 0.059144, acc.: 96.88%] [G loss: 0.971477]\n",
      "131 [D loss: 0.062821, acc.: 95.31%] [G loss: 0.849884]\n",
      "132 [D loss: 0.040046, acc.: 98.44%] [G loss: 0.899272]\n",
      "133 [D loss: 0.054937, acc.: 98.44%] [G loss: 0.898592]\n",
      "134 [D loss: 0.034965, acc.: 98.44%] [G loss: 0.910425]\n",
      "135 [D loss: 0.040449, acc.: 98.44%] [G loss: 0.853746]\n",
      "136 [D loss: 0.031910, acc.: 98.44%] [G loss: 0.912120]\n",
      "137 [D loss: 0.040806, acc.: 98.44%] [G loss: 1.023374]\n",
      "138 [D loss: 0.041382, acc.: 98.44%] [G loss: 0.988100]\n",
      "139 [D loss: 0.066927, acc.: 95.31%] [G loss: 0.942693]\n",
      "140 [D loss: 0.101283, acc.: 95.31%] [G loss: 0.999916]\n",
      "141 [D loss: 0.103190, acc.: 90.62%] [G loss: 0.953280]\n",
      "142 [D loss: 0.062358, acc.: 96.88%] [G loss: 1.092396]\n",
      "143 [D loss: 0.079082, acc.: 89.06%] [G loss: 0.884881]\n",
      "144 [D loss: 0.079248, acc.: 98.44%] [G loss: 0.851321]\n",
      "145 [D loss: 0.169756, acc.: 81.25%] [G loss: 0.785154]\n",
      "146 [D loss: 0.144826, acc.: 89.06%] [G loss: 1.293950]\n",
      "147 [D loss: 0.147660, acc.: 87.50%] [G loss: 0.737597]\n",
      "148 [D loss: 0.121415, acc.: 100.00%] [G loss: 1.036960]\n",
      "149 [D loss: 0.113274, acc.: 92.19%] [G loss: 0.911388]\n",
      "150 [D loss: 0.059794, acc.: 93.75%] [G loss: 1.083017]\n",
      "151 [D loss: 0.032160, acc.: 98.44%] [G loss: 1.009739]\n",
      "152 [D loss: 0.036612, acc.: 100.00%] [G loss: 1.084628]\n",
      "153 [D loss: 0.061650, acc.: 98.44%] [G loss: 0.940804]\n",
      "154 [D loss: 0.050613, acc.: 98.44%] [G loss: 1.092722]\n",
      "155 [D loss: 0.047964, acc.: 95.31%] [G loss: 1.016595]\n",
      "156 [D loss: 0.038205, acc.: 98.44%] [G loss: 0.951281]\n",
      "157 [D loss: 0.048087, acc.: 96.88%] [G loss: 1.006102]\n",
      "158 [D loss: 0.029155, acc.: 98.44%] [G loss: 0.867479]\n",
      "159 [D loss: 0.024185, acc.: 100.00%] [G loss: 0.871214]\n",
      "160 [D loss: 0.031039, acc.: 100.00%] [G loss: 0.991934]\n",
      "161 [D loss: 0.042797, acc.: 100.00%] [G loss: 0.663796]\n",
      "162 [D loss: 0.049235, acc.: 100.00%] [G loss: 0.935405]\n",
      "163 [D loss: 0.049251, acc.: 95.31%] [G loss: 0.966114]\n",
      "164 [D loss: 0.028828, acc.: 100.00%] [G loss: 1.029194]\n",
      "165 [D loss: 0.027163, acc.: 100.00%] [G loss: 0.987239]\n",
      "166 [D loss: 0.051314, acc.: 95.31%] [G loss: 0.946826]\n",
      "167 [D loss: 0.032479, acc.: 100.00%] [G loss: 0.976257]\n",
      "168 [D loss: 0.040894, acc.: 98.44%] [G loss: 0.969583]\n",
      "169 [D loss: 0.043514, acc.: 96.88%] [G loss: 0.951671]\n",
      "170 [D loss: 0.031667, acc.: 100.00%] [G loss: 0.692956]\n",
      "171 [D loss: 0.060382, acc.: 92.19%] [G loss: 0.842392]\n",
      "172 [D loss: 0.046564, acc.: 100.00%] [G loss: 0.874539]\n",
      "173 [D loss: 0.035055, acc.: 98.44%] [G loss: 1.118811]\n",
      "174 [D loss: 0.042924, acc.: 100.00%] [G loss: 0.932450]\n",
      "175 [D loss: 0.050690, acc.: 93.75%] [G loss: 0.932834]\n",
      "176 [D loss: 0.047974, acc.: 96.88%] [G loss: 0.985156]\n",
      "177 [D loss: 0.024971, acc.: 98.44%] [G loss: 0.996502]\n",
      "178 [D loss: 0.029122, acc.: 98.44%] [G loss: 0.880394]\n",
      "179 [D loss: 0.042884, acc.: 96.88%] [G loss: 1.003647]\n",
      "180 [D loss: 0.031707, acc.: 98.44%] [G loss: 1.072713]\n",
      "181 [D loss: 0.029031, acc.: 98.44%] [G loss: 0.934113]\n",
      "182 [D loss: 0.033393, acc.: 100.00%] [G loss: 0.865592]\n",
      "183 [D loss: 0.025282, acc.: 100.00%] [G loss: 0.871048]\n",
      "184 [D loss: 0.023273, acc.: 100.00%] [G loss: 1.227036]\n",
      "185 [D loss: 0.037873, acc.: 100.00%] [G loss: 0.989159]\n",
      "186 [D loss: 0.023523, acc.: 100.00%] [G loss: 0.947788]\n",
      "187 [D loss: 0.034351, acc.: 98.44%] [G loss: 0.943421]\n",
      "188 [D loss: 0.034358, acc.: 100.00%] [G loss: 0.943646]\n",
      "189 [D loss: 0.033362, acc.: 100.00%] [G loss: 0.879335]\n",
      "190 [D loss: 0.053844, acc.: 96.88%] [G loss: 0.828750]\n",
      "191 [D loss: 0.088436, acc.: 89.06%] [G loss: 1.019036]\n",
      "192 [D loss: 0.093905, acc.: 92.19%] [G loss: 1.571467]\n",
      "193 [D loss: 0.136156, acc.: 100.00%] [G loss: 1.046614]\n",
      "194 [D loss: 0.218326, acc.: 82.81%] [G loss: 0.916980]\n",
      "195 [D loss: 0.216852, acc.: 64.06%] [G loss: 0.913520]\n",
      "196 [D loss: 0.099694, acc.: 98.44%] [G loss: 1.410673]\n",
      "197 [D loss: 0.074763, acc.: 95.31%] [G loss: 0.961154]\n",
      "198 [D loss: 0.058098, acc.: 98.44%] [G loss: 1.052214]\n",
      "199 [D loss: 0.047345, acc.: 98.44%] [G loss: 0.895634]\n",
      "200 [D loss: 0.033937, acc.: 100.00%] [G loss: 0.994229]\n",
      "201 [D loss: 0.025234, acc.: 100.00%] [G loss: 0.899420]\n",
      "202 [D loss: 0.024779, acc.: 100.00%] [G loss: 0.901472]\n",
      "203 [D loss: 0.026173, acc.: 100.00%] [G loss: 1.013058]\n",
      "204 [D loss: 0.028569, acc.: 100.00%] [G loss: 0.916422]\n",
      "205 [D loss: 0.032173, acc.: 100.00%] [G loss: 0.958244]\n",
      "206 [D loss: 0.031445, acc.: 98.44%] [G loss: 0.878084]\n",
      "207 [D loss: 0.033633, acc.: 100.00%] [G loss: 0.965088]\n",
      "208 [D loss: 0.047438, acc.: 96.88%] [G loss: 0.862560]\n",
      "209 [D loss: 0.036992, acc.: 100.00%] [G loss: 0.964141]\n",
      "210 [D loss: 0.023310, acc.: 100.00%] [G loss: 0.965387]\n",
      "211 [D loss: 0.032460, acc.: 96.88%] [G loss: 0.995018]\n",
      "212 [D loss: 0.025112, acc.: 100.00%] [G loss: 0.969547]\n",
      "213 [D loss: 0.025319, acc.: 100.00%] [G loss: 0.959466]\n",
      "214 [D loss: 0.043685, acc.: 100.00%] [G loss: 0.925933]\n",
      "215 [D loss: 0.031889, acc.: 100.00%] [G loss: 1.146588]\n",
      "216 [D loss: 0.053879, acc.: 100.00%] [G loss: 0.837903]\n",
      "217 [D loss: 0.056014, acc.: 93.75%] [G loss: 0.898238]\n",
      "218 [D loss: 0.050622, acc.: 93.75%] [G loss: 1.018070]\n",
      "219 [D loss: 0.045947, acc.: 100.00%] [G loss: 0.947695]\n",
      "220 [D loss: 0.035804, acc.: 100.00%] [G loss: 1.008167]\n",
      "221 [D loss: 0.060843, acc.: 96.88%] [G loss: 0.907224]\n",
      "222 [D loss: 0.039028, acc.: 100.00%] [G loss: 1.056199]\n",
      "223 [D loss: 0.036395, acc.: 98.44%] [G loss: 1.016483]\n",
      "224 [D loss: 0.026465, acc.: 98.44%] [G loss: 1.171665]\n",
      "225 [D loss: 0.021536, acc.: 100.00%] [G loss: 1.140668]\n",
      "226 [D loss: 0.045202, acc.: 98.44%] [G loss: 0.761880]\n",
      "227 [D loss: 0.033863, acc.: 100.00%] [G loss: 0.930824]\n",
      "228 [D loss: 0.053548, acc.: 100.00%] [G loss: 0.966108]\n",
      "229 [D loss: 0.039536, acc.: 100.00%] [G loss: 1.079929]\n",
      "230 [D loss: 0.043060, acc.: 96.88%] [G loss: 0.940949]\n",
      "231 [D loss: 0.031639, acc.: 98.44%] [G loss: 0.818490]\n",
      "232 [D loss: 0.056818, acc.: 96.88%] [G loss: 0.915729]\n",
      "233 [D loss: 0.026427, acc.: 100.00%] [G loss: 0.955522]\n",
      "234 [D loss: 0.043192, acc.: 98.44%] [G loss: 0.953121]\n",
      "235 [D loss: 0.033153, acc.: 100.00%] [G loss: 1.064950]\n",
      "236 [D loss: 0.045795, acc.: 98.44%] [G loss: 0.894123]\n",
      "237 [D loss: 0.037359, acc.: 98.44%] [G loss: 1.037636]\n",
      "238 [D loss: 0.046820, acc.: 100.00%] [G loss: 1.005852]\n",
      "239 [D loss: 0.048363, acc.: 100.00%] [G loss: 1.054461]\n",
      "240 [D loss: 0.101263, acc.: 95.31%] [G loss: 0.745480]\n",
      "241 [D loss: 0.074262, acc.: 100.00%] [G loss: 0.942594]\n",
      "242 [D loss: 0.087783, acc.: 95.31%] [G loss: 0.948303]\n",
      "243 [D loss: 0.053354, acc.: 100.00%] [G loss: 1.357589]\n",
      "244 [D loss: 0.066150, acc.: 96.88%] [G loss: 0.828129]\n",
      "245 [D loss: 0.049956, acc.: 100.00%] [G loss: 1.026472]\n",
      "246 [D loss: 0.061434, acc.: 96.88%] [G loss: 0.874291]\n",
      "247 [D loss: 0.032822, acc.: 98.44%] [G loss: 0.986077]\n",
      "248 [D loss: 0.021407, acc.: 100.00%] [G loss: 0.851182]\n",
      "249 [D loss: 0.024435, acc.: 100.00%] [G loss: 0.888742]\n",
      "250 [D loss: 0.039727, acc.: 96.88%] [G loss: 1.023409]\n",
      "251 [D loss: 0.039431, acc.: 98.44%] [G loss: 0.873209]\n",
      "252 [D loss: 0.029192, acc.: 100.00%] [G loss: 0.852642]\n",
      "253 [D loss: 0.032462, acc.: 98.44%] [G loss: 0.951972]\n",
      "254 [D loss: 0.025097, acc.: 98.44%] [G loss: 0.874462]\n",
      "255 [D loss: 0.044825, acc.: 100.00%] [G loss: 1.017686]\n",
      "256 [D loss: 0.057856, acc.: 100.00%] [G loss: 1.082770]\n",
      "257 [D loss: 0.055985, acc.: 96.88%] [G loss: 0.832048]\n",
      "258 [D loss: 0.023202, acc.: 98.44%] [G loss: 0.951538]\n",
      "259 [D loss: 0.037189, acc.: 98.44%] [G loss: 0.763924]\n",
      "260 [D loss: 0.042691, acc.: 98.44%] [G loss: 0.988830]\n",
      "261 [D loss: 0.034109, acc.: 100.00%] [G loss: 0.929287]\n",
      "262 [D loss: 0.037102, acc.: 98.44%] [G loss: 0.895808]\n",
      "263 [D loss: 0.020645, acc.: 100.00%] [G loss: 0.821679]\n",
      "264 [D loss: 0.030323, acc.: 98.44%] [G loss: 1.030333]\n",
      "265 [D loss: 0.026714, acc.: 100.00%] [G loss: 0.876065]\n",
      "266 [D loss: 0.020498, acc.: 100.00%] [G loss: 0.958388]\n",
      "267 [D loss: 0.028040, acc.: 100.00%] [G loss: 1.078676]\n",
      "268 [D loss: 0.046859, acc.: 100.00%] [G loss: 0.927183]\n",
      "269 [D loss: 0.058620, acc.: 100.00%] [G loss: 0.906426]\n",
      "270 [D loss: 0.109462, acc.: 90.62%] [G loss: 0.965863]\n",
      "271 [D loss: 0.087924, acc.: 100.00%] [G loss: 1.264158]\n",
      "272 [D loss: 0.129778, acc.: 90.62%] [G loss: 0.868895]\n",
      "273 [D loss: 0.067523, acc.: 100.00%] [G loss: 1.038090]\n",
      "274 [D loss: 0.047786, acc.: 98.44%] [G loss: 1.066151]\n",
      "275 [D loss: 0.024314, acc.: 100.00%] [G loss: 1.141936]\n",
      "276 [D loss: 0.023736, acc.: 100.00%] [G loss: 0.834181]\n",
      "277 [D loss: 0.018764, acc.: 100.00%] [G loss: 0.782798]\n",
      "278 [D loss: 0.016087, acc.: 100.00%] [G loss: 0.863105]\n",
      "279 [D loss: 0.033128, acc.: 98.44%] [G loss: 0.888958]\n",
      "280 [D loss: 0.033845, acc.: 98.44%] [G loss: 1.027728]\n",
      "281 [D loss: 0.028899, acc.: 98.44%] [G loss: 1.001327]\n",
      "282 [D loss: 0.021839, acc.: 98.44%] [G loss: 0.948347]\n",
      "283 [D loss: 0.031529, acc.: 98.44%] [G loss: 0.799708]\n",
      "284 [D loss: 0.038031, acc.: 100.00%] [G loss: 0.993107]\n",
      "285 [D loss: 0.062235, acc.: 98.44%] [G loss: 0.891457]\n",
      "286 [D loss: 0.033834, acc.: 100.00%] [G loss: 0.856744]\n",
      "287 [D loss: 0.029906, acc.: 98.44%] [G loss: 0.931394]\n",
      "288 [D loss: 0.016144, acc.: 100.00%] [G loss: 0.972615]\n",
      "289 [D loss: 0.027454, acc.: 98.44%] [G loss: 0.939405]\n",
      "290 [D loss: 0.025113, acc.: 100.00%] [G loss: 0.991848]\n",
      "291 [D loss: 0.052105, acc.: 96.88%] [G loss: 0.896856]\n",
      "292 [D loss: 0.045945, acc.: 96.88%] [G loss: 0.888405]\n",
      "293 [D loss: 0.041688, acc.: 96.88%] [G loss: 1.018749]\n",
      "294 [D loss: 0.042836, acc.: 100.00%] [G loss: 0.999684]\n",
      "295 [D loss: 0.049064, acc.: 98.44%] [G loss: 1.219235]\n",
      "296 [D loss: 0.026514, acc.: 100.00%] [G loss: 1.031625]\n",
      "297 [D loss: 0.030367, acc.: 100.00%] [G loss: 0.834966]\n",
      "298 [D loss: 0.014047, acc.: 100.00%] [G loss: 0.785731]\n",
      "299 [D loss: 0.018507, acc.: 100.00%] [G loss: 0.913557]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = LSGAN()\n",
    "    gan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdabbb1-87c0-4782-b9d8-93c703af1477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lsgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow 2.4",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
