{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XiJHfN3Kb7Tz"
   },
   "source": [
    "Note : This module requires GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxJ0w952b-E5"
   },
   "source": [
    "## CellStrat Hub Pack - Reinforcement Learning\n",
    "### RL5 - Deep Q Networks\n",
    "\n",
    "#### Instructions to be followed:\n",
    "* This kernel must require **GPU** and with CPU training will take  longer time\n",
    "* Change the kernel to **PyTorch 1.9**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# If any library needs to be installed, install with following command :-\n",
    "# pip install <library-name>\n",
    "# This pip command should be in an independent cell with no other code or comments in this cell.\n",
    "#==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39856,
     "status": "ok",
     "timestamp": 1588843813950,
     "user": {
      "displayName": "Shubha Manikarnike",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiAnyUNmcogADneayWJ1ibklIZxzbn0cqYpxvkzaQ=s64",
      "userId": "11006834529155798872"
     },
     "user_tz": -330
    },
    "id": "kHlyvaaQHIB5",
    "outputId": "4935e092-1e5a-4fa3-adee-1089b3e1c938"
   },
   "outputs": [],
   "source": [
    "## This could take few minutes to run\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#==============================================================================\n",
    "# Install packages\n",
    "#==============================================================================\n",
    "!pip install gym > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install tensorboardX > /dev/null 2>&1\n",
    "!python -m atari_py.import_roms '/home/ec2-user/SageMaker/RL_Pack/RL5 - Deep Q Newtworks/Roms/Roms' > /dev/null 2>&1\n",
    "!pip install torch > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym[atari] > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to /tmp/pip-req-build-8uwthymt\n",
      "  Running command git clone -q https://github.com/Kojoley/atari-py.git /tmp/pip-req-build-8uwthymt\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from atari-py==1.2.2) (1.20.1)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for atari-py: filename=atari_py-1.2.2-cp38-cp38-linux_x86_64.whl size=4080204 sha256=48608e47cf36452618ad360bebb5f8c7b55252c9880c53ac3129075e5372b4ad\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vboigsiu/wheels/25/11/0c/d37ea19ecec588ab95c4199a485d3a4de5284e9a08b89c8f3f\n",
      "Successfully built atari-py\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/Kojoley/atari-py.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Using cached tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from tensorboardX) (1.20.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from tensorboardX) (3.17.3)\n",
      "Requirement already satisfied: six>=1.9 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m atari_py.import_roms '/home/ec2-user/SageMaker/RL_Pack/RL5 - Deep Q Newtworks/Roms/Roms' > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQa_mHElHJ5d"
   },
   "outputs": [],
   "source": [
    "\n",
    "#==============================================================================\n",
    "# Import packages\n",
    "#==============================================================================\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from lib import wrappers\n",
    "from lib import dqn_model\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4X1HkpEHx33"
   },
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 10 # We will train the model until this reward is achieved\n",
    "#MEAN_REWARD_BOUND = 19.5\n",
    "GAMMA = 0.99  # Gamma value\n",
    "BATCH_SIZE = 32 \n",
    "REPLAY_SIZE = 10000 #Maximum Capacity of the buffer\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000 # How frequently we update target model weights with actual model weights\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QKEbrDvH6x5"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjQTFiyoIBBT"
   },
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DIDRzi65IM46"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "      # Play a step using epsilon greedy Policy and return the reward.\n",
    "        done_reward = None\n",
    "        # Epsilon Greesy Policy\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC65HaPsIVu-"
   },
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    # apply the target network to our next state observations and calculate the maximum Q-value along the same action dimension 1.\n",
    "    # Function max() returns both maximum values and indices of those values (so it calculates both max and argmax),\n",
    "    # which is very convenient. However, in this case, we're interested only in values, so we take the first entry of the result.\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    # detach the value from its computation graph to prevent gradients from flowing into the neural network used to calculate \n",
    "    # Q approximation for next states.\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHRo8Np1LVt1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wY80Ybked2AE"
   },
   "outputs": [],
   "source": [
    "env = wrappers.make_env(DEFAULT_ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1588843884625,
     "user": {
      "displayName": "Shubha Manikarnike",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiAnyUNmcogADneayWJ1ibklIZxzbn0cqYpxvkzaQ=s64",
      "userId": "11006834529155798872"
     },
     "user_tz": -330
    },
    "id": "Sewc38-leWnt",
    "outputId": "3b9424c2-da2a-42c4-f071-5b57d803f687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1588843906889,
     "user": {
      "displayName": "Shubha Manikarnike",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiAnyUNmcogADneayWJ1ibklIZxzbn0cqYpxvkzaQ=s64",
      "userId": "11006834529155798872"
     },
     "user_tz": -330
    },
    "id": "bbYNafN1d3Zj",
    "outputId": "5a0c945d-1d88-4185-c6ec-5c21188e7390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dXh5zi1JIsO4",
    "outputId": "be48429c-bbd7-49d1-8472-5a99c2d28a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "882: done 1 games, mean reward -21.000, eps 0.99, speed 610.70 f/s\n",
      "1944: done 2 games, mean reward -21.000, eps 0.98, speed 517.12 f/s\n",
      "3045: done 3 games, mean reward -20.667, eps 0.97, speed 811.87 f/s\n",
      "Best mean reward updated -21.000 -> -20.667, model saved\n",
      "3897: done 4 games, mean reward -20.750, eps 0.96, speed 790.37 f/s\n",
      "4808: done 5 games, mean reward -20.800, eps 0.95, speed 788.55 f/s\n",
      "5764: done 6 games, mean reward -20.667, eps 0.94, speed 778.46 f/s\n",
      "6526: done 7 games, mean reward -20.714, eps 0.93, speed 757.78 f/s\n",
      "7487: done 8 games, mean reward -20.625, eps 0.93, speed 755.38 f/s\n",
      "Best mean reward updated -20.667 -> -20.625, model saved\n",
      "8414: done 9 games, mean reward -20.556, eps 0.92, speed 743.21 f/s\n",
      "Best mean reward updated -20.625 -> -20.556, model saved\n",
      "9236: done 10 games, mean reward -20.600, eps 0.91, speed 727.76 f/s\n",
      "10079: done 11 games, mean reward -20.636, eps 0.90, speed 143.92 f/s\n",
      "10869: done 12 games, mean reward -20.667, eps 0.89, speed 17.68 f/s\n",
      "11835: done 13 games, mean reward -20.538, eps 0.88, speed 16.46 f/s\n",
      "Best mean reward updated -20.556 -> -20.538, model saved\n",
      "12625: done 14 games, mean reward -20.571, eps 0.87, speed 16.08 f/s\n",
      "13601: done 15 games, mean reward -20.533, eps 0.86, speed 15.51 f/s\n",
      "Best mean reward updated -20.538 -> -20.533, model saved\n",
      "14451: done 16 games, mean reward -20.562, eps 0.86, speed 14.58 f/s\n",
      "15361: done 17 games, mean reward -20.588, eps 0.85, speed 14.62 f/s\n",
      "16123: done 18 games, mean reward -20.611, eps 0.84, speed 14.53 f/s\n",
      "17008: done 19 games, mean reward -20.632, eps 0.83, speed 14.56 f/s\n",
      "17956: done 20 games, mean reward -20.600, eps 0.82, speed 14.56 f/s\n",
      "18718: done 21 games, mean reward -20.619, eps 0.81, speed 14.26 f/s\n",
      "19573: done 22 games, mean reward -20.636, eps 0.80, speed 14.23 f/s\n",
      "20363: done 23 games, mean reward -20.652, eps 0.80, speed 14.36 f/s\n",
      "21243: done 24 games, mean reward -20.667, eps 0.79, speed 14.49 f/s\n",
      "22108: done 25 games, mean reward -20.680, eps 0.78, speed 14.41 f/s\n",
      "23131: done 26 games, mean reward -20.654, eps 0.77, speed 14.11 f/s\n",
      "24120: done 27 games, mean reward -20.630, eps 0.76, speed 14.15 f/s\n",
      "25302: done 28 games, mean reward -20.536, eps 0.75, speed 14.92 f/s\n",
      "26263: done 29 games, mean reward -20.517, eps 0.74, speed 14.94 f/s\n",
      "Best mean reward updated -20.533 -> -20.517, model saved\n",
      "27199: done 30 games, mean reward -20.500, eps 0.73, speed 14.67 f/s\n",
      "Best mean reward updated -20.517 -> -20.500, model saved\n",
      "28137: done 31 games, mean reward -20.516, eps 0.72, speed 14.76 f/s\n",
      "29336: done 32 games, mean reward -20.438, eps 0.71, speed 14.93 f/s\n",
      "Best mean reward updated -20.500 -> -20.438, model saved\n",
      "30207: done 33 games, mean reward -20.455, eps 0.70, speed 14.96 f/s\n",
      "31235: done 34 games, mean reward -20.412, eps 0.69, speed 14.82 f/s\n",
      "Best mean reward updated -20.438 -> -20.412, model saved\n",
      "32118: done 35 games, mean reward -20.400, eps 0.68, speed 14.34 f/s\n",
      "Best mean reward updated -20.412 -> -20.400, model saved\n",
      "33062: done 36 games, mean reward -20.417, eps 0.67, speed 14.78 f/s\n",
      "33974: done 37 games, mean reward -20.432, eps 0.66, speed 14.73 f/s\n",
      "34929: done 38 games, mean reward -20.421, eps 0.65, speed 14.81 f/s\n",
      "36081: done 39 games, mean reward -20.410, eps 0.64, speed 14.77 f/s\n",
      "37060: done 40 games, mean reward -20.400, eps 0.63, speed 14.71 f/s\n",
      "38009: done 41 games, mean reward -20.390, eps 0.62, speed 14.83 f/s\n",
      "Best mean reward updated -20.400 -> -20.390, model saved\n",
      "39498: done 42 games, mean reward -20.310, eps 0.61, speed 14.87 f/s\n",
      "Best mean reward updated -20.390 -> -20.310, model saved\n",
      "40504: done 43 games, mean reward -20.302, eps 0.59, speed 13.92 f/s\n",
      "Best mean reward updated -20.310 -> -20.302, model saved\n",
      "41671: done 44 games, mean reward -20.295, eps 0.58, speed 14.72 f/s\n",
      "Best mean reward updated -20.302 -> -20.295, model saved\n",
      "42839: done 45 games, mean reward -20.267, eps 0.57, speed 14.75 f/s\n",
      "Best mean reward updated -20.295 -> -20.267, model saved\n",
      "43999: done 46 games, mean reward -20.261, eps 0.56, speed 14.80 f/s\n",
      "Best mean reward updated -20.267 -> -20.261, model saved\n",
      "45327: done 47 games, mean reward -20.191, eps 0.55, speed 14.47 f/s\n",
      "Best mean reward updated -20.261 -> -20.191, model saved\n",
      "46282: done 48 games, mean reward -20.188, eps 0.54, speed 14.78 f/s\n",
      "Best mean reward updated -20.191 -> -20.188, model saved\n",
      "47470: done 49 games, mean reward -20.184, eps 0.53, speed 14.90 f/s\n",
      "Best mean reward updated -20.188 -> -20.184, model saved\n",
      "49013: done 50 games, mean reward -20.080, eps 0.51, speed 14.51 f/s\n",
      "Best mean reward updated -20.184 -> -20.080, model saved\n",
      "50391: done 51 games, mean reward -20.059, eps 0.50, speed 14.23 f/s\n",
      "Best mean reward updated -20.080 -> -20.059, model saved\n",
      "51551: done 52 games, mean reward -20.038, eps 0.48, speed 14.40 f/s\n",
      "Best mean reward updated -20.059 -> -20.038, model saved\n",
      "53087: done 53 games, mean reward -19.962, eps 0.47, speed 14.30 f/s\n",
      "Best mean reward updated -20.038 -> -19.962, model saved\n",
      "54776: done 54 games, mean reward -19.926, eps 0.45, speed 14.15 f/s\n",
      "Best mean reward updated -19.962 -> -19.926, model saved\n",
      "56192: done 55 games, mean reward -19.909, eps 0.44, speed 14.35 f/s\n",
      "Best mean reward updated -19.926 -> -19.909, model saved\n",
      "57738: done 56 games, mean reward -19.857, eps 0.42, speed 14.26 f/s\n",
      "Best mean reward updated -19.909 -> -19.857, model saved\n",
      "58930: done 57 games, mean reward -19.825, eps 0.41, speed 14.20 f/s\n",
      "Best mean reward updated -19.857 -> -19.825, model saved\n",
      "60408: done 58 games, mean reward -19.759, eps 0.40, speed 14.54 f/s\n",
      "Best mean reward updated -19.825 -> -19.759, model saved\n",
      "61573: done 59 games, mean reward -19.763, eps 0.38, speed 14.53 f/s\n",
      "62871: done 60 games, mean reward -19.750, eps 0.37, speed 14.31 f/s\n",
      "Best mean reward updated -19.759 -> -19.750, model saved\n",
      "64449: done 61 games, mean reward -19.689, eps 0.36, speed 14.57 f/s\n",
      "Best mean reward updated -19.750 -> -19.689, model saved\n",
      "66589: done 62 games, mean reward -19.565, eps 0.33, speed 14.31 f/s\n",
      "Best mean reward updated -19.689 -> -19.565, model saved\n",
      "67922: done 63 games, mean reward -19.556, eps 0.32, speed 14.02 f/s\n",
      "Best mean reward updated -19.565 -> -19.556, model saved\n",
      "69847: done 64 games, mean reward -19.469, eps 0.30, speed 14.07 f/s\n",
      "Best mean reward updated -19.556 -> -19.469, model saved\n",
      "71628: done 65 games, mean reward -19.446, eps 0.28, speed 13.78 f/s\n",
      "Best mean reward updated -19.469 -> -19.446, model saved\n",
      "73058: done 66 games, mean reward -19.409, eps 0.27, speed 13.80 f/s\n",
      "Best mean reward updated -19.446 -> -19.409, model saved\n",
      "74795: done 67 games, mean reward -19.388, eps 0.25, speed 13.37 f/s\n",
      "Best mean reward updated -19.409 -> -19.388, model saved\n",
      "76565: done 68 games, mean reward -19.368, eps 0.23, speed 13.11 f/s\n",
      "Best mean reward updated -19.388 -> -19.368, model saved\n",
      "78139: done 69 games, mean reward -19.333, eps 0.22, speed 12.96 f/s\n",
      "Best mean reward updated -19.368 -> -19.333, model saved\n",
      "79837: done 70 games, mean reward -19.314, eps 0.20, speed 12.86 f/s\n",
      "Best mean reward updated -19.333 -> -19.314, model saved\n",
      "81686: done 71 games, mean reward -19.268, eps 0.18, speed 13.03 f/s\n",
      "Best mean reward updated -19.314 -> -19.268, model saved\n",
      "83723: done 72 games, mean reward -19.264, eps 0.16, speed 12.81 f/s\n",
      "Best mean reward updated -19.268 -> -19.264, model saved\n",
      "85725: done 73 games, mean reward -19.233, eps 0.14, speed 12.85 f/s\n",
      "Best mean reward updated -19.264 -> -19.233, model saved\n",
      "87492: done 74 games, mean reward -19.203, eps 0.13, speed 12.59 f/s\n",
      "Best mean reward updated -19.233 -> -19.203, model saved\n",
      "89548: done 75 games, mean reward -19.187, eps 0.10, speed 12.72 f/s\n",
      "Best mean reward updated -19.203 -> -19.187, model saved\n",
      "91456: done 76 games, mean reward -19.171, eps 0.09, speed 12.51 f/s\n",
      "Best mean reward updated -19.187 -> -19.171, model saved\n",
      "94437: done 77 games, mean reward -19.078, eps 0.06, speed 12.49 f/s\n",
      "Best mean reward updated -19.171 -> -19.078, model saved\n",
      "97995: done 78 games, mean reward -18.936, eps 0.02, speed 10.97 f/s\n",
      "Best mean reward updated -19.078 -> -18.936, model saved\n",
      "101336: done 79 games, mean reward -18.861, eps 0.02, speed 12.12 f/s\n",
      "Best mean reward updated -18.936 -> -18.861, model saved\n",
      "104996: done 80 games, mean reward -18.788, eps 0.02, speed 12.14 f/s\n",
      "Best mean reward updated -18.861 -> -18.788, model saved\n",
      "108776: done 81 games, mean reward -18.691, eps 0.02, speed 12.12 f/s\n",
      "Best mean reward updated -18.788 -> -18.691, model saved\n",
      "112281: done 82 games, mean reward -18.561, eps 0.02, speed 12.14 f/s\n",
      "Best mean reward updated -18.691 -> -18.561, model saved\n",
      "114853: done 83 games, mean reward -18.494, eps 0.02, speed 12.20 f/s\n",
      "Best mean reward updated -18.561 -> -18.494, model saved\n",
      "118015: done 84 games, mean reward -18.417, eps 0.02, speed 12.20 f/s\n",
      "Best mean reward updated -18.494 -> -18.417, model saved\n",
      "121085: done 85 games, mean reward -18.318, eps 0.02, speed 12.13 f/s\n",
      "Best mean reward updated -18.417 -> -18.318, model saved\n",
      "124034: done 86 games, mean reward -18.267, eps 0.02, speed 12.12 f/s\n",
      "Best mean reward updated -18.318 -> -18.267, model saved\n",
      "127268: done 87 games, mean reward -18.218, eps 0.02, speed 12.01 f/s\n",
      "Best mean reward updated -18.267 -> -18.218, model saved\n",
      "130611: done 88 games, mean reward -18.125, eps 0.02, speed 12.06 f/s\n",
      "Best mean reward updated -18.218 -> -18.125, model saved\n",
      "132912: done 89 games, mean reward -18.101, eps 0.02, speed 12.11 f/s\n",
      "Best mean reward updated -18.125 -> -18.101, model saved\n",
      "135980: done 90 games, mean reward -18.056, eps 0.02, speed 12.07 f/s\n",
      "Best mean reward updated -18.101 -> -18.056, model saved\n"
     ]
    }
   ],
   "source": [
    "# Training take 1 - 1.5 hours. This Step can be skipped and you can directly go to the next step. We will use a pre trained model.\n",
    "env = wrappers.make_env(DEFAULT_ENV_NAME)\n",
    "# Create two networks - Training network (net) and target network (tgt_net)\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    # Reduce the value of Epsilon for every timestep\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "    # Agent plays a Step and Collects the reward\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), mean_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"mean_score10.dat\")\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "    # Fixed Q Targets : For every 1000 steps update the Target weights with the weigths from training network\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZQjJzXyLajn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from lib import wrappers\n",
    "from lib import dqn_model\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUwhe3z9O19x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym pyvirtualdisplay > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xvfbwrapper\n",
      "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
      "Building wheels for collected packages: xvfbwrapper\n",
      "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5010 sha256=b52604245fd14882db195a2d5696313e711e633842b89f611636e223b4c7fd06\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/e1/da/b4/57ac130c024104997ae76f389fe0e7e43922ec3cfdffaf1b1e\n",
      "Successfully built xvfbwrapper\n",
      "Installing collected packages: xvfbwrapper\n",
      "Successfully installed xvfbwrapper-0.2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xvfbwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install gym pyvirtualdisplay > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: yum: command not found\n"
     ]
    }
   ],
   "source": [
    "!sudo yum install -y Xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ga2opnfIO40W"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) #error only\n",
    "\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNB341pqPBrS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7faa957c9b50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UorYqHHPFB7"
   },
   "outputs": [],
   "source": [
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBpTI2MrPM7B"
   },
   "outputs": [],
   "source": [
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sG8zRMc3PPpA"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(wrappers.make_env(DEFAULT_ENV_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyjiLviDPRrV"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10242/561050434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_ENV_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-best.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    136\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.load_state_dict(torch.load(DEFAULT_ENV_NAME + \"-best.dat\"))\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "c = collections.Counter()\n",
    "\n",
    "while True:\n",
    "  start_ts = time.time()  \n",
    "  state_v = torch.tensor(np.array([state], copy=False))\n",
    "  q_vals = net(state_v).data.numpy()[0]\n",
    "  env.render()\n",
    "  action = np.argmax(q_vals)\n",
    "  c[action] += 1\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  total_reward += reward\n",
    "  if done:\n",
    "    break\n",
    "env.close()\n",
    "show_video()  \n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "print(\"Action counts:\", c)\n",
    "if True:\n",
    "  env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMWKmHb4O28cP58bUV8r14I",
   "collapsed_sections": [],
   "name": "Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch 1.9",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
